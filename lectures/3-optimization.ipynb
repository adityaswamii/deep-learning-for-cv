{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e942d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CS231n: Optimization\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, os, random\n",
    "from collections import Counter\n",
    "from ipywidgets import interact, IntSlider, Dropdown\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d483ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 Loading Utilities\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f, encoding='bytes')\n",
    "        X = datadict[b'data']\n",
    "        Y = datadict[b'labels']\n",
    "        # reshape to (N, C, H, W) then transpose to (N, H, W, C) for channel-last format\n",
    "        X = X.reshape(10000, 3, 32, 32).astype(\"float\").transpose(0, 2, 3, 1)\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(folder_path):\n",
    "    xs, ys = [], []\n",
    "    for b in range(1, 6):\n",
    "        f = os.path.join(folder_path, f'data_batch_{b}')\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(folder_path, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10('../cifar-10-batches-py')\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30385122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 32, 32, 3)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "063d97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (49000, 3072)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Test data shape:  (1000, 3072)\n",
      "dev data shape:  (500, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7067523c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the image mean for each class\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_image_means = {}\n",
    "for y in range(10):\n",
    "    X_class = X_train[y_train == y]\n",
    "    class_image_means[y] = np.mean(X_class, axis=0)\n",
    "\n",
    "# show class means\n",
    "plt.figure(figsize=(10,5))\n",
    "for y in range(10):\n",
    "    plt.subplot(2, 5, y+1)\n",
    "    plt.imshow(class_image_means[y].reshape(32,32,3).astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{classes[y]}')\n",
    "plt.suptitle('CIFAR-10 Class Mean Images', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca6b2c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130.64189796 135.98173469 132.47391837 130.05569388 135.34804082\n",
      " 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean for the training data\n",
    "\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
    "plt.show()\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb79d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute the image mean for each class (after subtracting overall mean)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "class_image_means = {}\n",
    "for y in range(10):\n",
    "    X_class = X_train[y_train == y]\n",
    "    class_image_means[y] = np.mean(X_class, axis=0)\n",
    "\n",
    "# show class means\n",
    "plt.figure(figsize=(10,5))\n",
    "for y in range(10):\n",
    "    plt.subplot(2, 5, y+1)\n",
    "    plt.imshow(class_image_means[y].reshape(32,32,3).astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{classes[y]}')\n",
    "plt.suptitle('CIFAR-10 Class Mean Images (After Mean Subtraction)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d36c573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)\n"
     ]
    }
   ],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our classifier\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb471a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function Definitions\n",
    "\n",
    "def scores(X, W):\n",
    "    \"\"\"\n",
    "    Compute the scores given the data and weights.\n",
    "    - X is the data matrix where each row is a data point.\n",
    "    - W is the weight matrix.\n",
    "    Returns the scores matrix.\n",
    "    \"\"\"\n",
    "    return W @ X.T\n",
    "\n",
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return scores, loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return scores, loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "  \"\"\"\n",
    "  fully-vectorized implementation :\n",
    "  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "  - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "  - W are weights (e.g. 10 x 3073)\n",
    "  \"\"\"\n",
    "  # evaluate loss over all examples in X without using any for loops\n",
    "  # left as exercise to reader in the assignment\n",
    "  delta = 1.0\n",
    "  score = scores(X, W)  # shape (C, N)\n",
    "  correct_class_scores = score[y, np.arange(X.shape[0])]  # shape (N,)\n",
    "  margins = np.maximum(0, correct_class_scores - score + delta)  # shape (C, N)\n",
    "  margins[y, np.arange(X.shape[0])] = 0  # zero out correct class margins\n",
    "  loss = np.sum(margins)\n",
    "  return score, loss\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "475bc747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 0 the loss was 4681.532979836529 (best loss: 4681.53)\n",
      "in attempt 1 the loss was 4423.4393474019125 (best loss: 4423.44)\n",
      "in attempt 2 the loss was 4926.340176217874 (best loss: 4423.44)\n",
      "in attempt 3 the loss was 4610.212165986841 (best loss: 4423.44)\n",
      "in attempt 4 the loss was 4669.954664307294 (best loss: 4423.44)\n",
      "in attempt 5 the loss was 4737.784805173961 (best loss: 4423.44)\n",
      "in attempt 6 the loss was 4563.867120316652 (best loss: 4423.44)\n",
      "in attempt 7 the loss was 4467.142361998925 (best loss: 4423.44)\n",
      "in attempt 8 the loss was 4296.267450089592 (best loss: 4296.27)\n",
      "in attempt 9 the loss was 4788.016664987423 (best loss: 4296.27)\n",
      "in attempt 10 the loss was 4456.475598073631 (best loss: 4296.27)\n",
      "in attempt 11 the loss was 4373.862577390425 (best loss: 4296.27)\n",
      "in attempt 12 the loss was 4502.899455462164 (best loss: 4296.27)\n",
      "in attempt 13 the loss was 4685.592931151178 (best loss: 4296.27)\n",
      "in attempt 14 the loss was 4561.799726604686 (best loss: 4296.27)\n",
      "in attempt 15 the loss was 4299.039840564598 (best loss: 4296.27)\n",
      "in attempt 16 the loss was 4636.891120458663 (best loss: 4296.27)\n",
      "in attempt 17 the loss was 4435.289212833867 (best loss: 4296.27)\n",
      "in attempt 18 the loss was 4515.07069909275 (best loss: 4296.27)\n",
      "in attempt 19 the loss was 4406.07361507641 (best loss: 4296.27)\n",
      "in attempt 20 the loss was 4606.5322565679835 (best loss: 4296.27)\n",
      "in attempt 21 the loss was 4244.326252698498 (best loss: 4244.33)\n",
      "in attempt 22 the loss was 4517.951493543187 (best loss: 4244.33)\n",
      "in attempt 23 the loss was 4455.617710360383 (best loss: 4244.33)\n",
      "in attempt 24 the loss was 4380.573244658759 (best loss: 4244.33)\n",
      "in attempt 25 the loss was 4670.670749888842 (best loss: 4244.33)\n",
      "in attempt 26 the loss was 4523.189011427744 (best loss: 4244.33)\n",
      "in attempt 27 the loss was 4460.542490571951 (best loss: 4244.33)\n",
      "in attempt 28 the loss was 4321.90188729474 (best loss: 4244.33)\n",
      "in attempt 29 the loss was 4647.027031058215 (best loss: 4244.33)\n",
      "in attempt 30 the loss was 4886.7745820498385 (best loss: 4244.33)\n",
      "in attempt 31 the loss was 4565.7563627649015 (best loss: 4244.33)\n",
      "in attempt 32 the loss was 4552.035125970664 (best loss: 4244.33)\n",
      "in attempt 33 the loss was 4521.3660015078985 (best loss: 4244.33)\n",
      "in attempt 34 the loss was 4633.307464292713 (best loss: 4244.33)\n",
      "in attempt 35 the loss was 4614.870729940851 (best loss: 4244.33)\n",
      "in attempt 36 the loss was 4315.2070661921225 (best loss: 4244.33)\n",
      "in attempt 37 the loss was 4734.748716715418 (best loss: 4244.33)\n",
      "in attempt 38 the loss was 4481.406241169758 (best loss: 4244.33)\n",
      "in attempt 39 the loss was 4682.1765549143165 (best loss: 4244.33)\n",
      "in attempt 40 the loss was 4289.450038439492 (best loss: 4244.33)\n",
      "in attempt 41 the loss was 4641.21744011209 (best loss: 4244.33)\n",
      "in attempt 42 the loss was 4600.212906765846 (best loss: 4244.33)\n",
      "in attempt 43 the loss was 4564.866713854352 (best loss: 4244.33)\n",
      "in attempt 44 the loss was 4452.174510580711 (best loss: 4244.33)\n",
      "in attempt 45 the loss was 4494.0535682021655 (best loss: 4244.33)\n",
      "in attempt 46 the loss was 4558.067152382982 (best loss: 4244.33)\n",
      "in attempt 47 the loss was 4660.025279352459 (best loss: 4244.33)\n",
      "in attempt 48 the loss was 4713.873993339268 (best loss: 4244.33)\n",
      "in attempt 49 the loss was 4702.7984699623275 (best loss: 4244.33)\n",
      "in attempt 50 the loss was 4435.2820739456865 (best loss: 4244.33)\n",
      "in attempt 51 the loss was 4427.331611470495 (best loss: 4244.33)\n",
      "in attempt 52 the loss was 4495.689203474958 (best loss: 4244.33)\n",
      "in attempt 53 the loss was 4542.29728012819 (best loss: 4244.33)\n",
      "in attempt 54 the loss was 4632.403396278733 (best loss: 4244.33)\n",
      "in attempt 55 the loss was 4651.1104297290885 (best loss: 4244.33)\n",
      "in attempt 56 the loss was 4695.918366943346 (best loss: 4244.33)\n",
      "in attempt 57 the loss was 4636.249304758719 (best loss: 4244.33)\n",
      "in attempt 58 the loss was 4413.215379893836 (best loss: 4244.33)\n",
      "in attempt 59 the loss was 4396.935127949497 (best loss: 4244.33)\n",
      "in attempt 60 the loss was 4595.2292861682 (best loss: 4244.33)\n",
      "in attempt 61 the loss was 4498.188806143379 (best loss: 4244.33)\n",
      "in attempt 62 the loss was 4591.6139357552565 (best loss: 4244.33)\n",
      "in attempt 63 the loss was 4581.483768005085 (best loss: 4244.33)\n",
      "in attempt 64 the loss was 4360.124654694635 (best loss: 4244.33)\n",
      "in attempt 65 the loss was 4502.155549791416 (best loss: 4244.33)\n",
      "in attempt 66 the loss was 4677.931442732703 (best loss: 4244.33)\n",
      "in attempt 67 the loss was 4323.289475590727 (best loss: 4244.33)\n",
      "in attempt 68 the loss was 4479.537619855135 (best loss: 4244.33)\n",
      "in attempt 69 the loss was 4640.079269200625 (best loss: 4244.33)\n",
      "in attempt 70 the loss was 4188.005446271456 (best loss: 4188.01)\n",
      "in attempt 71 the loss was 4311.943131392151 (best loss: 4188.01)\n",
      "in attempt 72 the loss was 4357.475313156647 (best loss: 4188.01)\n",
      "in attempt 73 the loss was 4806.472928120706 (best loss: 4188.01)\n",
      "in attempt 74 the loss was 4762.5552096848705 (best loss: 4188.01)\n",
      "in attempt 75 the loss was 4374.343348059678 (best loss: 4188.01)\n",
      "in attempt 76 the loss was 4452.021263904509 (best loss: 4188.01)\n",
      "in attempt 77 the loss was 4842.301616352772 (best loss: 4188.01)\n",
      "in attempt 78 the loss was 4590.288372390343 (best loss: 4188.01)\n",
      "in attempt 79 the loss was 4469.77581370541 (best loss: 4188.01)\n",
      "in attempt 80 the loss was 4624.565894260029 (best loss: 4188.01)\n",
      "in attempt 81 the loss was 4700.438085344199 (best loss: 4188.01)\n",
      "in attempt 82 the loss was 4517.718655436051 (best loss: 4188.01)\n",
      "in attempt 83 the loss was 4649.669648842149 (best loss: 4188.01)\n",
      "in attempt 84 the loss was 4398.497329979593 (best loss: 4188.01)\n",
      "in attempt 85 the loss was 4600.994029686772 (best loss: 4188.01)\n",
      "in attempt 86 the loss was 4772.330031715837 (best loss: 4188.01)\n",
      "in attempt 87 the loss was 4687.917917109109 (best loss: 4188.01)\n",
      "in attempt 88 the loss was 4479.477231731269 (best loss: 4188.01)\n",
      "in attempt 89 the loss was 4526.827298998482 (best loss: 4188.01)\n",
      "in attempt 90 the loss was 4668.143633404616 (best loss: 4188.01)\n",
      "in attempt 91 the loss was 4153.42497731179 (best loss: 4153.42)\n",
      "in attempt 92 the loss was 4584.352835319012 (best loss: 4153.42)\n",
      "in attempt 93 the loss was 4303.888276170233 (best loss: 4153.42)\n",
      "in attempt 94 the loss was 4759.0623123830155 (best loss: 4153.42)\n",
      "in attempt 95 the loss was 4391.054066272374 (best loss: 4153.42)\n",
      "in attempt 96 the loss was 4807.046202290235 (best loss: 4153.42)\n",
      "in attempt 97 the loss was 4389.85204682902 (best loss: 4153.42)\n",
      "in attempt 98 the loss was 4435.228745949883 (best loss: 4153.42)\n",
      "in attempt 99 the loss was 4585.5759138271405 (best loss: 4153.42)\n",
      "in attempt 100 the loss was 4777.525208561741 (best loss: 4153.42)\n",
      "in attempt 101 the loss was 4455.2310756011775 (best loss: 4153.42)\n",
      "in attempt 102 the loss was 4278.456092272982 (best loss: 4153.42)\n",
      "in attempt 103 the loss was 4740.303117719503 (best loss: 4153.42)\n",
      "in attempt 104 the loss was 4538.766873412982 (best loss: 4153.42)\n",
      "in attempt 105 the loss was 4575.056147407184 (best loss: 4153.42)\n",
      "in attempt 106 the loss was 4317.92896896108 (best loss: 4153.42)\n",
      "in attempt 107 the loss was 4784.345324436028 (best loss: 4153.42)\n",
      "in attempt 108 the loss was 4612.629551082712 (best loss: 4153.42)\n",
      "in attempt 109 the loss was 4462.4888165229895 (best loss: 4153.42)\n",
      "in attempt 110 the loss was 4395.4810686766505 (best loss: 4153.42)\n",
      "in attempt 111 the loss was 4617.690056823623 (best loss: 4153.42)\n",
      "in attempt 112 the loss was 4585.637997117991 (best loss: 4153.42)\n",
      "in attempt 113 the loss was 4451.582079636104 (best loss: 4153.42)\n",
      "in attempt 114 the loss was 4618.116251266514 (best loss: 4153.42)\n",
      "in attempt 115 the loss was 4330.981705082268 (best loss: 4153.42)\n",
      "in attempt 116 the loss was 4498.9771797924805 (best loss: 4153.42)\n",
      "in attempt 117 the loss was 4445.64451331374 (best loss: 4153.42)\n",
      "in attempt 118 the loss was 4582.635339257271 (best loss: 4153.42)\n",
      "in attempt 119 the loss was 4491.476449969663 (best loss: 4153.42)\n",
      "in attempt 120 the loss was 4369.292862213151 (best loss: 4153.42)\n",
      "in attempt 121 the loss was 4599.027740685104 (best loss: 4153.42)\n",
      "in attempt 122 the loss was 4482.126117039827 (best loss: 4153.42)\n",
      "in attempt 123 the loss was 4532.48130489164 (best loss: 4153.42)\n",
      "in attempt 124 the loss was 4422.698322306393 (best loss: 4153.42)\n",
      "in attempt 125 the loss was 4663.6701763458295 (best loss: 4153.42)\n",
      "in attempt 126 the loss was 4703.998159919889 (best loss: 4153.42)\n",
      "in attempt 127 the loss was 4770.023149061575 (best loss: 4153.42)\n",
      "in attempt 128 the loss was 4396.67859641111 (best loss: 4153.42)\n",
      "in attempt 129 the loss was 4264.229290297826 (best loss: 4153.42)\n",
      "in attempt 130 the loss was 4438.458313155845 (best loss: 4153.42)\n",
      "in attempt 131 the loss was 4728.050769175015 (best loss: 4153.42)\n",
      "in attempt 132 the loss was 4708.623766309776 (best loss: 4153.42)\n",
      "in attempt 133 the loss was 4640.022878600685 (best loss: 4153.42)\n",
      "in attempt 134 the loss was 4476.639424219634 (best loss: 4153.42)\n",
      "in attempt 135 the loss was 4571.504954665074 (best loss: 4153.42)\n",
      "in attempt 136 the loss was 4522.034332884985 (best loss: 4153.42)\n",
      "in attempt 137 the loss was 4721.473748405628 (best loss: 4153.42)\n",
      "in attempt 138 the loss was 4875.076784982932 (best loss: 4153.42)\n",
      "in attempt 139 the loss was 4751.347619372329 (best loss: 4153.42)\n",
      "in attempt 140 the loss was 4825.780435666545 (best loss: 4153.42)\n",
      "in attempt 141 the loss was 4277.540367717498 (best loss: 4153.42)\n",
      "in attempt 142 the loss was 4374.465672535249 (best loss: 4153.42)\n",
      "in attempt 143 the loss was 4602.142186942607 (best loss: 4153.42)\n",
      "in attempt 144 the loss was 4731.1107412763395 (best loss: 4153.42)\n",
      "in attempt 145 the loss was 4219.155369667587 (best loss: 4153.42)\n",
      "in attempt 146 the loss was 4769.074105763109 (best loss: 4153.42)\n",
      "in attempt 147 the loss was 4718.284766342114 (best loss: 4153.42)\n",
      "in attempt 148 the loss was 4711.063385819251 (best loss: 4153.42)\n",
      "in attempt 149 the loss was 4576.5008246377365 (best loss: 4153.42)\n",
      "in attempt 150 the loss was 4191.132778585491 (best loss: 4153.42)\n",
      "in attempt 151 the loss was 4405.939682298429 (best loss: 4153.42)\n",
      "in attempt 152 the loss was 4514.744697969867 (best loss: 4153.42)\n",
      "in attempt 153 the loss was 4402.226668103195 (best loss: 4153.42)\n",
      "in attempt 154 the loss was 4814.884238951567 (best loss: 4153.42)\n",
      "in attempt 155 the loss was 4654.012482517229 (best loss: 4153.42)\n",
      "in attempt 156 the loss was 4599.683975401542 (best loss: 4153.42)\n",
      "in attempt 157 the loss was 4485.824368573412 (best loss: 4153.42)\n",
      "in attempt 158 the loss was 4672.723694342386 (best loss: 4153.42)\n",
      "in attempt 159 the loss was 4769.79367717993 (best loss: 4153.42)\n",
      "in attempt 160 the loss was 4365.869924740934 (best loss: 4153.42)\n",
      "in attempt 161 the loss was 4609.192075357039 (best loss: 4153.42)\n",
      "in attempt 162 the loss was 4451.339336253623 (best loss: 4153.42)\n",
      "in attempt 163 the loss was 4511.758114706569 (best loss: 4153.42)\n",
      "in attempt 164 the loss was 4678.075132718518 (best loss: 4153.42)\n",
      "in attempt 165 the loss was 4228.816664999341 (best loss: 4153.42)\n",
      "in attempt 166 the loss was 4510.1367873970175 (best loss: 4153.42)\n",
      "in attempt 167 the loss was 4697.616504266292 (best loss: 4153.42)\n",
      "in attempt 168 the loss was 4431.846845561795 (best loss: 4153.42)\n",
      "in attempt 169 the loss was 4386.665645836836 (best loss: 4153.42)\n",
      "in attempt 170 the loss was 4632.264025103333 (best loss: 4153.42)\n",
      "in attempt 171 the loss was 4519.590868486524 (best loss: 4153.42)\n",
      "in attempt 172 the loss was 4658.848011615637 (best loss: 4153.42)\n",
      "in attempt 173 the loss was 4300.495701587215 (best loss: 4153.42)\n",
      "in attempt 174 the loss was 4613.695186356473 (best loss: 4153.42)\n",
      "in attempt 175 the loss was 4974.331283229306 (best loss: 4153.42)\n",
      "in attempt 176 the loss was 4617.367943680112 (best loss: 4153.42)\n",
      "in attempt 177 the loss was 4307.68918794383 (best loss: 4153.42)\n",
      "in attempt 178 the loss was 4734.565924110464 (best loss: 4153.42)\n",
      "in attempt 179 the loss was 4880.565451997636 (best loss: 4153.42)\n",
      "in attempt 180 the loss was 4494.9457375466745 (best loss: 4153.42)\n",
      "in attempt 181 the loss was 4681.154683843304 (best loss: 4153.42)\n",
      "in attempt 182 the loss was 4524.6300420331245 (best loss: 4153.42)\n",
      "in attempt 183 the loss was 4392.065666056784 (best loss: 4153.42)\n",
      "in attempt 184 the loss was 4743.800660726395 (best loss: 4153.42)\n",
      "in attempt 185 the loss was 4630.649959448216 (best loss: 4153.42)\n",
      "in attempt 186 the loss was 4439.094058292164 (best loss: 4153.42)\n",
      "in attempt 187 the loss was 4506.046266216856 (best loss: 4153.42)\n",
      "in attempt 188 the loss was 4795.845622968496 (best loss: 4153.42)\n",
      "in attempt 189 the loss was 4663.704190038206 (best loss: 4153.42)\n",
      "in attempt 190 the loss was 4685.142180392624 (best loss: 4153.42)\n",
      "in attempt 191 the loss was 4423.5069847451605 (best loss: 4153.42)\n",
      "in attempt 192 the loss was 4597.405779672456 (best loss: 4153.42)\n",
      "in attempt 193 the loss was 4617.257874634695 (best loss: 4153.42)\n",
      "in attempt 194 the loss was 4275.144489591115 (best loss: 4153.42)\n",
      "in attempt 195 the loss was 4682.336112074707 (best loss: 4153.42)\n",
      "in attempt 196 the loss was 4345.39452424601 (best loss: 4153.42)\n",
      "in attempt 197 the loss was 4535.203358024275 (best loss: 4153.42)\n",
      "in attempt 198 the loss was 4105.28683023344 (best loss: 4105.29)\n",
      "in attempt 199 the loss was 4533.414232950656 (best loss: 4105.29)\n",
      "in attempt 200 the loss was 4370.49170265009 (best loss: 4105.29)\n",
      "in attempt 201 the loss was 4834.63999266079 (best loss: 4105.29)\n",
      "in attempt 202 the loss was 4655.301533438001 (best loss: 4105.29)\n",
      "in attempt 203 the loss was 4611.25398393198 (best loss: 4105.29)\n",
      "in attempt 204 the loss was 4475.300851374561 (best loss: 4105.29)\n",
      "in attempt 205 the loss was 4595.154720103425 (best loss: 4105.29)\n",
      "in attempt 206 the loss was 4382.097196446306 (best loss: 4105.29)\n",
      "in attempt 207 the loss was 4461.384360667397 (best loss: 4105.29)\n",
      "in attempt 208 the loss was 4453.250843887273 (best loss: 4105.29)\n",
      "in attempt 209 the loss was 4554.466188971544 (best loss: 4105.29)\n",
      "in attempt 210 the loss was 4568.075836179903 (best loss: 4105.29)\n",
      "in attempt 211 the loss was 4454.10789902687 (best loss: 4105.29)\n",
      "in attempt 212 the loss was 4374.698421080175 (best loss: 4105.29)\n",
      "in attempt 213 the loss was 4438.507948620264 (best loss: 4105.29)\n",
      "in attempt 214 the loss was 4640.209397488943 (best loss: 4105.29)\n",
      "in attempt 215 the loss was 4470.895254234974 (best loss: 4105.29)\n",
      "in attempt 216 the loss was 4596.4647362229425 (best loss: 4105.29)\n",
      "in attempt 217 the loss was 4584.488308361695 (best loss: 4105.29)\n",
      "in attempt 218 the loss was 4264.906836947754 (best loss: 4105.29)\n",
      "in attempt 219 the loss was 4492.518667635945 (best loss: 4105.29)\n",
      "in attempt 220 the loss was 4376.86105246206 (best loss: 4105.29)\n",
      "in attempt 221 the loss was 4648.292174486952 (best loss: 4105.29)\n",
      "in attempt 222 the loss was 4430.68972187758 (best loss: 4105.29)\n",
      "in attempt 223 the loss was 4533.6561205653115 (best loss: 4105.29)\n",
      "in attempt 224 the loss was 4398.554191893139 (best loss: 4105.29)\n",
      "in attempt 225 the loss was 4738.311906856706 (best loss: 4105.29)\n",
      "in attempt 226 the loss was 4364.624023670127 (best loss: 4105.29)\n",
      "in attempt 227 the loss was 4504.895000240953 (best loss: 4105.29)\n",
      "in attempt 228 the loss was 4599.283199892081 (best loss: 4105.29)\n",
      "in attempt 229 the loss was 4392.880083230271 (best loss: 4105.29)\n",
      "in attempt 230 the loss was 4416.266643531757 (best loss: 4105.29)\n",
      "in attempt 231 the loss was 4608.206388293072 (best loss: 4105.29)\n",
      "in attempt 232 the loss was 4952.233081501899 (best loss: 4105.29)\n",
      "in attempt 233 the loss was 4300.394210630659 (best loss: 4105.29)\n",
      "in attempt 234 the loss was 4452.4651951293035 (best loss: 4105.29)\n",
      "in attempt 235 the loss was 4570.182294600502 (best loss: 4105.29)\n",
      "in attempt 236 the loss was 4580.744135533147 (best loss: 4105.29)\n",
      "in attempt 237 the loss was 4383.102164899034 (best loss: 4105.29)\n",
      "in attempt 238 the loss was 4764.082334887555 (best loss: 4105.29)\n",
      "in attempt 239 the loss was 4479.475028088022 (best loss: 4105.29)\n",
      "in attempt 240 the loss was 4321.034178349636 (best loss: 4105.29)\n",
      "in attempt 241 the loss was 4598.713090149382 (best loss: 4105.29)\n",
      "in attempt 242 the loss was 4464.426054067166 (best loss: 4105.29)\n",
      "in attempt 243 the loss was 4383.621337585761 (best loss: 4105.29)\n",
      "in attempt 244 the loss was 4391.90190855983 (best loss: 4105.29)\n",
      "in attempt 245 the loss was 4721.221798905198 (best loss: 4105.29)\n",
      "in attempt 246 the loss was 4757.27336442226 (best loss: 4105.29)\n",
      "in attempt 247 the loss was 4624.734654870702 (best loss: 4105.29)\n",
      "in attempt 248 the loss was 4179.076553929093 (best loss: 4105.29)\n",
      "in attempt 249 the loss was 4365.6673595817065 (best loss: 4105.29)\n",
      "in attempt 250 the loss was 4730.4351235447475 (best loss: 4105.29)\n",
      "in attempt 251 the loss was 4357.40124498456 (best loss: 4105.29)\n",
      "in attempt 252 the loss was 4482.47919096052 (best loss: 4105.29)\n",
      "in attempt 253 the loss was 4453.6643551794505 (best loss: 4105.29)\n",
      "in attempt 254 the loss was 4590.076705940262 (best loss: 4105.29)\n",
      "in attempt 255 the loss was 4518.819309198059 (best loss: 4105.29)\n",
      "in attempt 256 the loss was 4434.110725722709 (best loss: 4105.29)\n",
      "in attempt 257 the loss was 4549.516672598471 (best loss: 4105.29)\n",
      "in attempt 258 the loss was 4266.4249547665095 (best loss: 4105.29)\n",
      "in attempt 259 the loss was 4442.282140972829 (best loss: 4105.29)\n",
      "in attempt 260 the loss was 4521.235640859858 (best loss: 4105.29)\n",
      "in attempt 261 the loss was 4527.755348367013 (best loss: 4105.29)\n",
      "in attempt 262 the loss was 4677.730774979675 (best loss: 4105.29)\n",
      "in attempt 263 the loss was 4292.323753163859 (best loss: 4105.29)\n",
      "in attempt 264 the loss was 4354.3690192958165 (best loss: 4105.29)\n",
      "in attempt 265 the loss was 4670.85283589264 (best loss: 4105.29)\n",
      "in attempt 266 the loss was 4752.574715797613 (best loss: 4105.29)\n",
      "in attempt 267 the loss was 4495.071649919246 (best loss: 4105.29)\n",
      "in attempt 268 the loss was 4897.0284089948145 (best loss: 4105.29)\n",
      "in attempt 269 the loss was 4361.148619339112 (best loss: 4105.29)\n",
      "in attempt 270 the loss was 4422.160943844746 (best loss: 4105.29)\n",
      "in attempt 271 the loss was 4471.435881034078 (best loss: 4105.29)\n",
      "in attempt 272 the loss was 4451.30194604381 (best loss: 4105.29)\n",
      "in attempt 273 the loss was 4752.301723816874 (best loss: 4105.29)\n",
      "in attempt 274 the loss was 4220.741690178705 (best loss: 4105.29)\n",
      "in attempt 275 the loss was 4450.6878569427445 (best loss: 4105.29)\n",
      "in attempt 276 the loss was 4387.248963790434 (best loss: 4105.29)\n",
      "in attempt 277 the loss was 4540.8946714576605 (best loss: 4105.29)\n",
      "in attempt 278 the loss was 4685.068980988806 (best loss: 4105.29)\n",
      "in attempt 279 the loss was 4739.8286809660995 (best loss: 4105.29)\n",
      "in attempt 280 the loss was 4544.991559050493 (best loss: 4105.29)\n",
      "in attempt 281 the loss was 4603.527265277964 (best loss: 4105.29)\n",
      "in attempt 282 the loss was 4720.252031120799 (best loss: 4105.29)\n",
      "in attempt 283 the loss was 4782.623112216861 (best loss: 4105.29)\n",
      "in attempt 284 the loss was 4461.0048068268425 (best loss: 4105.29)\n",
      "in attempt 285 the loss was 4265.057716628219 (best loss: 4105.29)\n",
      "in attempt 286 the loss was 4497.209089398296 (best loss: 4105.29)\n",
      "in attempt 287 the loss was 4540.603266110291 (best loss: 4105.29)\n",
      "in attempt 288 the loss was 4401.216042495176 (best loss: 4105.29)\n",
      "in attempt 289 the loss was 4408.586342598866 (best loss: 4105.29)\n",
      "in attempt 290 the loss was 4551.1315748280085 (best loss: 4105.29)\n",
      "in attempt 291 the loss was 4248.057324930049 (best loss: 4105.29)\n",
      "in attempt 292 the loss was 4402.449701960922 (best loss: 4105.29)\n",
      "in attempt 293 the loss was 4447.138205809004 (best loss: 4105.29)\n",
      "in attempt 294 the loss was 4571.558328422867 (best loss: 4105.29)\n",
      "in attempt 295 the loss was 4716.365049452855 (best loss: 4105.29)\n",
      "in attempt 296 the loss was 4358.0010145411325 (best loss: 4105.29)\n",
      "in attempt 297 the loss was 4332.9134322729 (best loss: 4105.29)\n",
      "in attempt 298 the loss was 4536.081118919528 (best loss: 4105.29)\n",
      "in attempt 299 the loss was 4611.470951762629 (best loss: 4105.29)\n",
      "in attempt 300 the loss was 4391.643678552515 (best loss: 4105.29)\n",
      "in attempt 301 the loss was 4451.028293010961 (best loss: 4105.29)\n",
      "in attempt 302 the loss was 4614.724093543153 (best loss: 4105.29)\n",
      "in attempt 303 the loss was 4726.1941578530195 (best loss: 4105.29)\n",
      "in attempt 304 the loss was 4535.304841358344 (best loss: 4105.29)\n",
      "in attempt 305 the loss was 4751.983559711636 (best loss: 4105.29)\n",
      "in attempt 306 the loss was 4562.5770707876945 (best loss: 4105.29)\n",
      "in attempt 307 the loss was 4806.236467958288 (best loss: 4105.29)\n",
      "in attempt 308 the loss was 4551.044443833085 (best loss: 4105.29)\n",
      "in attempt 309 the loss was 4465.994888648906 (best loss: 4105.29)\n",
      "in attempt 310 the loss was 4484.881403954673 (best loss: 4105.29)\n",
      "in attempt 311 the loss was 4205.797151633006 (best loss: 4105.29)\n",
      "in attempt 312 the loss was 4630.019651516979 (best loss: 4105.29)\n",
      "in attempt 313 the loss was 4669.413385656184 (best loss: 4105.29)\n",
      "in attempt 314 the loss was 4814.239546955621 (best loss: 4105.29)\n",
      "in attempt 315 the loss was 4982.691026893073 (best loss: 4105.29)\n",
      "in attempt 316 the loss was 4469.193599370725 (best loss: 4105.29)\n",
      "in attempt 317 the loss was 4418.975030261744 (best loss: 4105.29)\n",
      "in attempt 318 the loss was 4268.940632093336 (best loss: 4105.29)\n",
      "in attempt 319 the loss was 4485.501707799838 (best loss: 4105.29)\n",
      "in attempt 320 the loss was 4500.368089296733 (best loss: 4105.29)\n",
      "in attempt 321 the loss was 4456.532447051693 (best loss: 4105.29)\n",
      "in attempt 322 the loss was 4390.071464355702 (best loss: 4105.29)\n",
      "in attempt 323 the loss was 4516.496970958526 (best loss: 4105.29)\n",
      "in attempt 324 the loss was 4554.538672861131 (best loss: 4105.29)\n",
      "in attempt 325 the loss was 4547.262409662055 (best loss: 4105.29)\n",
      "in attempt 326 the loss was 4646.611773890123 (best loss: 4105.29)\n",
      "in attempt 327 the loss was 4302.119040843207 (best loss: 4105.29)\n",
      "in attempt 328 the loss was 4488.231413974782 (best loss: 4105.29)\n",
      "in attempt 329 the loss was 4498.97881293676 (best loss: 4105.29)\n",
      "in attempt 330 the loss was 4646.249486927149 (best loss: 4105.29)\n",
      "in attempt 331 the loss was 4479.430121745962 (best loss: 4105.29)\n",
      "in attempt 332 the loss was 4493.52979133474 (best loss: 4105.29)\n",
      "in attempt 333 the loss was 4331.303292330056 (best loss: 4105.29)\n",
      "in attempt 334 the loss was 4442.798537849803 (best loss: 4105.29)\n",
      "in attempt 335 the loss was 4654.9034023505 (best loss: 4105.29)\n",
      "in attempt 336 the loss was 4624.40952155197 (best loss: 4105.29)\n",
      "in attempt 337 the loss was 4771.590725254394 (best loss: 4105.29)\n",
      "in attempt 338 the loss was 4552.9981284478035 (best loss: 4105.29)\n",
      "in attempt 339 the loss was 4459.67811042723 (best loss: 4105.29)\n",
      "in attempt 340 the loss was 4412.6811137330815 (best loss: 4105.29)\n",
      "in attempt 341 the loss was 4361.585345404922 (best loss: 4105.29)\n",
      "in attempt 342 the loss was 4556.6847489570255 (best loss: 4105.29)\n",
      "in attempt 343 the loss was 4345.787014756816 (best loss: 4105.29)\n",
      "in attempt 344 the loss was 4473.737552542036 (best loss: 4105.29)\n",
      "in attempt 345 the loss was 4457.085656605237 (best loss: 4105.29)\n",
      "in attempt 346 the loss was 4611.864597499476 (best loss: 4105.29)\n",
      "in attempt 347 the loss was 4328.74914380267 (best loss: 4105.29)\n",
      "in attempt 348 the loss was 4819.228391692372 (best loss: 4105.29)\n",
      "in attempt 349 the loss was 4317.967011307317 (best loss: 4105.29)\n",
      "in attempt 350 the loss was 4348.004972356903 (best loss: 4105.29)\n",
      "in attempt 351 the loss was 4674.4172979875875 (best loss: 4105.29)\n",
      "in attempt 352 the loss was 4433.793518764507 (best loss: 4105.29)\n",
      "in attempt 353 the loss was 4238.842250635356 (best loss: 4105.29)\n",
      "in attempt 354 the loss was 4599.609056818601 (best loss: 4105.29)\n",
      "in attempt 355 the loss was 4406.257450980273 (best loss: 4105.29)\n",
      "in attempt 356 the loss was 4447.399571229711 (best loss: 4105.29)\n",
      "in attempt 357 the loss was 4829.139199318282 (best loss: 4105.29)\n",
      "in attempt 358 the loss was 4568.805447895265 (best loss: 4105.29)\n",
      "in attempt 359 the loss was 4506.310831762653 (best loss: 4105.29)\n",
      "in attempt 360 the loss was 4421.170904726645 (best loss: 4105.29)\n",
      "in attempt 361 the loss was 4725.191520172152 (best loss: 4105.29)\n",
      "in attempt 362 the loss was 4407.696765744618 (best loss: 4105.29)\n",
      "in attempt 363 the loss was 4468.001951552516 (best loss: 4105.29)\n",
      "in attempt 364 the loss was 4314.62285524647 (best loss: 4105.29)\n",
      "in attempt 365 the loss was 4634.519049907162 (best loss: 4105.29)\n",
      "in attempt 366 the loss was 4454.1023478954985 (best loss: 4105.29)\n",
      "in attempt 367 the loss was 4635.702059075926 (best loss: 4105.29)\n",
      "in attempt 368 the loss was 4433.735566930358 (best loss: 4105.29)\n",
      "in attempt 369 the loss was 4580.610107798571 (best loss: 4105.29)\n",
      "in attempt 370 the loss was 4738.113775078515 (best loss: 4105.29)\n",
      "in attempt 371 the loss was 4465.383847207317 (best loss: 4105.29)\n",
      "in attempt 372 the loss was 4422.558542550525 (best loss: 4105.29)\n",
      "in attempt 373 the loss was 4732.5847683631 (best loss: 4105.29)\n",
      "in attempt 374 the loss was 4292.204450554045 (best loss: 4105.29)\n",
      "in attempt 375 the loss was 4610.635387352779 (best loss: 4105.29)\n",
      "in attempt 376 the loss was 4376.086097786812 (best loss: 4105.29)\n",
      "in attempt 377 the loss was 4463.235048892189 (best loss: 4105.29)\n",
      "in attempt 378 the loss was 4410.2999895260145 (best loss: 4105.29)\n",
      "in attempt 379 the loss was 4256.141344622664 (best loss: 4105.29)\n",
      "in attempt 380 the loss was 4357.254205299092 (best loss: 4105.29)\n",
      "in attempt 381 the loss was 4707.189183476408 (best loss: 4105.29)\n",
      "in attempt 382 the loss was 4563.899505196454 (best loss: 4105.29)\n",
      "in attempt 383 the loss was 4651.3219460064065 (best loss: 4105.29)\n",
      "in attempt 384 the loss was 4296.561517178832 (best loss: 4105.29)\n",
      "in attempt 385 the loss was 4678.59696856303 (best loss: 4105.29)\n",
      "in attempt 386 the loss was 4831.699292761086 (best loss: 4105.29)\n",
      "in attempt 387 the loss was 4418.227670891727 (best loss: 4105.29)\n",
      "in attempt 388 the loss was 4638.1110335663125 (best loss: 4105.29)\n",
      "in attempt 389 the loss was 4325.9179074986205 (best loss: 4105.29)\n",
      "in attempt 390 the loss was 4695.9225949070515 (best loss: 4105.29)\n",
      "in attempt 391 the loss was 4649.9381630147855 (best loss: 4105.29)\n",
      "in attempt 392 the loss was 4426.588481272705 (best loss: 4105.29)\n",
      "in attempt 393 the loss was 4548.918654454623 (best loss: 4105.29)\n",
      "in attempt 394 the loss was 4502.033631508232 (best loss: 4105.29)\n",
      "in attempt 395 the loss was 4132.137159790656 (best loss: 4105.29)\n",
      "in attempt 396 the loss was 4528.340530113408 (best loss: 4105.29)\n",
      "in attempt 397 the loss was 4475.360058166463 (best loss: 4105.29)\n",
      "in attempt 398 the loss was 4705.313154126462 (best loss: 4105.29)\n",
      "in attempt 399 the loss was 4643.360449952175 (best loss: 4105.29)\n",
      "in attempt 400 the loss was 4493.0550272698 (best loss: 4105.29)\n",
      "in attempt 401 the loss was 4464.205750051551 (best loss: 4105.29)\n",
      "in attempt 402 the loss was 4244.997215331404 (best loss: 4105.29)\n",
      "in attempt 403 the loss was 4647.347544094244 (best loss: 4105.29)\n",
      "in attempt 404 the loss was 4496.92636770782 (best loss: 4105.29)\n",
      "in attempt 405 the loss was 4652.6788375230435 (best loss: 4105.29)\n",
      "in attempt 406 the loss was 4391.733432536084 (best loss: 4105.29)\n",
      "in attempt 407 the loss was 4688.204217790626 (best loss: 4105.29)\n",
      "in attempt 408 the loss was 4126.627769253026 (best loss: 4105.29)\n",
      "in attempt 409 the loss was 4667.720422714052 (best loss: 4105.29)\n",
      "in attempt 410 the loss was 4492.841295455299 (best loss: 4105.29)\n",
      "in attempt 411 the loss was 4310.9775240735635 (best loss: 4105.29)\n",
      "in attempt 412 the loss was 4457.832883067178 (best loss: 4105.29)\n",
      "in attempt 413 the loss was 4451.44955214339 (best loss: 4105.29)\n",
      "in attempt 414 the loss was 4361.602699323307 (best loss: 4105.29)\n",
      "in attempt 415 the loss was 4579.797319589474 (best loss: 4105.29)\n",
      "in attempt 416 the loss was 4595.646093378591 (best loss: 4105.29)\n",
      "in attempt 417 the loss was 4310.099912069453 (best loss: 4105.29)\n",
      "in attempt 418 the loss was 4729.897286834205 (best loss: 4105.29)\n",
      "in attempt 419 the loss was 4631.087011487872 (best loss: 4105.29)\n",
      "in attempt 420 the loss was 4486.929968488554 (best loss: 4105.29)\n",
      "in attempt 421 the loss was 4759.311429967356 (best loss: 4105.29)\n",
      "in attempt 422 the loss was 4383.482612713154 (best loss: 4105.29)\n",
      "in attempt 423 the loss was 4687.395306546086 (best loss: 4105.29)\n",
      "in attempt 424 the loss was 4635.464791321534 (best loss: 4105.29)\n",
      "in attempt 425 the loss was 4498.02568356267 (best loss: 4105.29)\n",
      "in attempt 426 the loss was 4305.990954056563 (best loss: 4105.29)\n",
      "in attempt 427 the loss was 4546.486317089327 (best loss: 4105.29)\n",
      "in attempt 428 the loss was 4456.29678939303 (best loss: 4105.29)\n",
      "in attempt 429 the loss was 4484.655888571204 (best loss: 4105.29)\n",
      "in attempt 430 the loss was 4974.736589416965 (best loss: 4105.29)\n",
      "in attempt 431 the loss was 4588.860993159478 (best loss: 4105.29)\n",
      "in attempt 432 the loss was 4368.592489482376 (best loss: 4105.29)\n",
      "in attempt 433 the loss was 4617.118294990476 (best loss: 4105.29)\n",
      "in attempt 434 the loss was 4619.560716216698 (best loss: 4105.29)\n",
      "in attempt 435 the loss was 4208.795176796722 (best loss: 4105.29)\n",
      "in attempt 436 the loss was 4499.56351143881 (best loss: 4105.29)\n",
      "in attempt 437 the loss was 4703.08759216303 (best loss: 4105.29)\n",
      "in attempt 438 the loss was 4575.092369566544 (best loss: 4105.29)\n",
      "in attempt 439 the loss was 4439.590498266859 (best loss: 4105.29)\n",
      "in attempt 440 the loss was 4326.971709331496 (best loss: 4105.29)\n",
      "in attempt 441 the loss was 4410.058366128564 (best loss: 4105.29)\n",
      "in attempt 442 the loss was 4603.363842958328 (best loss: 4105.29)\n",
      "in attempt 443 the loss was 4420.418018639394 (best loss: 4105.29)\n",
      "in attempt 444 the loss was 4645.575073337988 (best loss: 4105.29)\n",
      "in attempt 445 the loss was 4556.555241774562 (best loss: 4105.29)\n",
      "in attempt 446 the loss was 4358.508594792243 (best loss: 4105.29)\n",
      "in attempt 447 the loss was 4376.412121251406 (best loss: 4105.29)\n",
      "in attempt 448 the loss was 4659.616460238525 (best loss: 4105.29)\n",
      "in attempt 449 the loss was 4650.1023443332215 (best loss: 4105.29)\n",
      "in attempt 450 the loss was 4287.440954909121 (best loss: 4105.29)\n",
      "in attempt 451 the loss was 4493.043476302537 (best loss: 4105.29)\n",
      "in attempt 452 the loss was 4557.11064130912 (best loss: 4105.29)\n",
      "in attempt 453 the loss was 4759.890346879014 (best loss: 4105.29)\n",
      "in attempt 454 the loss was 4767.401652938046 (best loss: 4105.29)\n",
      "in attempt 455 the loss was 4538.299016897497 (best loss: 4105.29)\n",
      "in attempt 456 the loss was 4464.380369500752 (best loss: 4105.29)\n",
      "in attempt 457 the loss was 4792.8998091660505 (best loss: 4105.29)\n",
      "in attempt 458 the loss was 4631.197129505497 (best loss: 4105.29)\n",
      "in attempt 459 the loss was 4495.623001113916 (best loss: 4105.29)\n",
      "in attempt 460 the loss was 4600.201275984591 (best loss: 4105.29)\n",
      "in attempt 461 the loss was 4570.272770264394 (best loss: 4105.29)\n",
      "in attempt 462 the loss was 4248.42458518722 (best loss: 4105.29)\n",
      "in attempt 463 the loss was 4508.56206830858 (best loss: 4105.29)\n",
      "in attempt 464 the loss was 4346.201752748063 (best loss: 4105.29)\n",
      "in attempt 465 the loss was 4651.331466448201 (best loss: 4105.29)\n",
      "in attempt 466 the loss was 4767.043944370777 (best loss: 4105.29)\n",
      "in attempt 467 the loss was 4419.746126820033 (best loss: 4105.29)\n",
      "in attempt 468 the loss was 4623.205684042741 (best loss: 4105.29)\n",
      "in attempt 469 the loss was 4692.211266349336 (best loss: 4105.29)\n",
      "in attempt 470 the loss was 4519.008210757914 (best loss: 4105.29)\n",
      "in attempt 471 the loss was 4705.978791725496 (best loss: 4105.29)\n",
      "in attempt 472 the loss was 4354.717810412527 (best loss: 4105.29)\n",
      "in attempt 473 the loss was 4519.476664799399 (best loss: 4105.29)\n",
      "in attempt 474 the loss was 4533.67617298014 (best loss: 4105.29)\n",
      "in attempt 475 the loss was 4864.080398097062 (best loss: 4105.29)\n",
      "in attempt 476 the loss was 4650.274312561794 (best loss: 4105.29)\n",
      "in attempt 477 the loss was 4477.947797756802 (best loss: 4105.29)\n",
      "in attempt 478 the loss was 4467.262255314358 (best loss: 4105.29)\n",
      "in attempt 479 the loss was 4573.481794076048 (best loss: 4105.29)\n",
      "in attempt 480 the loss was 4589.271821568238 (best loss: 4105.29)\n",
      "in attempt 481 the loss was 4328.1792251346005 (best loss: 4105.29)\n",
      "in attempt 482 the loss was 4727.394957160825 (best loss: 4105.29)\n",
      "in attempt 483 the loss was 4503.283664542659 (best loss: 4105.29)\n",
      "in attempt 484 the loss was 4435.596201851215 (best loss: 4105.29)\n",
      "in attempt 485 the loss was 4501.400572700923 (best loss: 4105.29)\n",
      "in attempt 486 the loss was 4499.957092488552 (best loss: 4105.29)\n",
      "in attempt 487 the loss was 4575.97851667539 (best loss: 4105.29)\n",
      "in attempt 488 the loss was 4718.165404429644 (best loss: 4105.29)\n",
      "in attempt 489 the loss was 4368.863627471872 (best loss: 4105.29)\n",
      "in attempt 490 the loss was 4461.956532302016 (best loss: 4105.29)\n",
      "in attempt 491 the loss was 4303.730138159489 (best loss: 4105.29)\n",
      "in attempt 492 the loss was 4679.773072646443 (best loss: 4105.29)\n",
      "in attempt 493 the loss was 4285.294587275679 (best loss: 4105.29)\n",
      "in attempt 494 the loss was 4839.058685991225 (best loss: 4105.29)\n",
      "in attempt 495 the loss was 4694.185721736878 (best loss: 4105.29)\n",
      "in attempt 496 the loss was 4398.07516307544 (best loss: 4105.29)\n",
      "in attempt 497 the loss was 4427.2403585842 (best loss: 4105.29)\n",
      "in attempt 498 the loss was 4560.184991715829 (best loss: 4105.29)\n",
      "in attempt 499 the loss was 4519.764983230193 (best loss: 4105.29)\n",
      "in attempt 500 the loss was 4637.169405604387 (best loss: 4105.29)\n",
      "in attempt 501 the loss was 4771.194942798775 (best loss: 4105.29)\n",
      "in attempt 502 the loss was 4559.250441620612 (best loss: 4105.29)\n",
      "in attempt 503 the loss was 4552.505845339187 (best loss: 4105.29)\n",
      "in attempt 504 the loss was 4561.389504940149 (best loss: 4105.29)\n",
      "in attempt 505 the loss was 4479.2141979964335 (best loss: 4105.29)\n",
      "in attempt 506 the loss was 4416.8783478104015 (best loss: 4105.29)\n",
      "in attempt 507 the loss was 4622.9783251324825 (best loss: 4105.29)\n",
      "in attempt 508 the loss was 4468.679232605784 (best loss: 4105.29)\n",
      "in attempt 509 the loss was 4489.601942153768 (best loss: 4105.29)\n",
      "in attempt 510 the loss was 4850.6828792284505 (best loss: 4105.29)\n",
      "in attempt 511 the loss was 4523.945439252465 (best loss: 4105.29)\n",
      "in attempt 512 the loss was 4422.249556644088 (best loss: 4105.29)\n",
      "in attempt 513 the loss was 4456.508527081291 (best loss: 4105.29)\n",
      "in attempt 514 the loss was 4674.172881533337 (best loss: 4105.29)\n",
      "in attempt 515 the loss was 4530.244731008488 (best loss: 4105.29)\n",
      "in attempt 516 the loss was 4682.764959750423 (best loss: 4105.29)\n",
      "in attempt 517 the loss was 4087.757863510087 (best loss: 4087.76)\n",
      "in attempt 518 the loss was 4261.537970041938 (best loss: 4087.76)\n",
      "in attempt 519 the loss was 4455.251102608607 (best loss: 4087.76)\n",
      "in attempt 520 the loss was 4618.881630039361 (best loss: 4087.76)\n",
      "in attempt 521 the loss was 4514.3856570000935 (best loss: 4087.76)\n",
      "in attempt 522 the loss was 4809.035326469526 (best loss: 4087.76)\n",
      "in attempt 523 the loss was 4408.649220853736 (best loss: 4087.76)\n",
      "in attempt 524 the loss was 4366.063776694646 (best loss: 4087.76)\n",
      "in attempt 525 the loss was 4557.0968917426235 (best loss: 4087.76)\n",
      "in attempt 526 the loss was 4589.7185171808815 (best loss: 4087.76)\n",
      "in attempt 527 the loss was 4862.583823912214 (best loss: 4087.76)\n",
      "in attempt 528 the loss was 4506.15732890097 (best loss: 4087.76)\n",
      "in attempt 529 the loss was 4716.80110146852 (best loss: 4087.76)\n",
      "in attempt 530 the loss was 4506.465987630547 (best loss: 4087.76)\n",
      "in attempt 531 the loss was 4237.631559419157 (best loss: 4087.76)\n",
      "in attempt 532 the loss was 4379.8658729777835 (best loss: 4087.76)\n",
      "in attempt 533 the loss was 4649.71731629472 (best loss: 4087.76)\n",
      "in attempt 534 the loss was 4460.703531489213 (best loss: 4087.76)\n",
      "in attempt 535 the loss was 4637.063210538971 (best loss: 4087.76)\n",
      "in attempt 536 the loss was 4670.379756159749 (best loss: 4087.76)\n",
      "in attempt 537 the loss was 4281.766541531719 (best loss: 4087.76)\n",
      "in attempt 538 the loss was 4756.7691880167795 (best loss: 4087.76)\n",
      "in attempt 539 the loss was 4364.048889236678 (best loss: 4087.76)\n",
      "in attempt 540 the loss was 4576.210892730171 (best loss: 4087.76)\n",
      "in attempt 541 the loss was 4638.2580806879205 (best loss: 4087.76)\n",
      "in attempt 542 the loss was 4645.777142339812 (best loss: 4087.76)\n",
      "in attempt 543 the loss was 4870.612575659761 (best loss: 4087.76)\n",
      "in attempt 544 the loss was 4319.591329498249 (best loss: 4087.76)\n",
      "in attempt 545 the loss was 4579.746251296121 (best loss: 4087.76)\n",
      "in attempt 546 the loss was 4403.280132352325 (best loss: 4087.76)\n",
      "in attempt 547 the loss was 4773.1074242246705 (best loss: 4087.76)\n",
      "in attempt 548 the loss was 4501.7700524876645 (best loss: 4087.76)\n",
      "in attempt 549 the loss was 4637.694755437782 (best loss: 4087.76)\n",
      "in attempt 550 the loss was 4361.218737989638 (best loss: 4087.76)\n",
      "in attempt 551 the loss was 4851.926108811379 (best loss: 4087.76)\n",
      "in attempt 552 the loss was 4463.683633437389 (best loss: 4087.76)\n",
      "in attempt 553 the loss was 4728.199419795005 (best loss: 4087.76)\n",
      "in attempt 554 the loss was 4791.9072657528895 (best loss: 4087.76)\n",
      "in attempt 555 the loss was 4407.785610925956 (best loss: 4087.76)\n",
      "in attempt 556 the loss was 4686.670599689125 (best loss: 4087.76)\n",
      "in attempt 557 the loss was 4554.216720848022 (best loss: 4087.76)\n",
      "in attempt 558 the loss was 4484.597566765375 (best loss: 4087.76)\n",
      "in attempt 559 the loss was 4485.521897582126 (best loss: 4087.76)\n",
      "in attempt 560 the loss was 4722.341503137197 (best loss: 4087.76)\n",
      "in attempt 561 the loss was 4710.650720726392 (best loss: 4087.76)\n",
      "in attempt 562 the loss was 4422.518367318897 (best loss: 4087.76)\n",
      "in attempt 563 the loss was 4419.091576486257 (best loss: 4087.76)\n",
      "in attempt 564 the loss was 4816.722728032179 (best loss: 4087.76)\n",
      "in attempt 565 the loss was 4219.584515646642 (best loss: 4087.76)\n",
      "in attempt 566 the loss was 4544.074547382514 (best loss: 4087.76)\n",
      "in attempt 567 the loss was 4433.517235228571 (best loss: 4087.76)\n",
      "in attempt 568 the loss was 4364.8678860065775 (best loss: 4087.76)\n",
      "in attempt 569 the loss was 4713.657208264606 (best loss: 4087.76)\n",
      "in attempt 570 the loss was 4528.400153217113 (best loss: 4087.76)\n",
      "in attempt 571 the loss was 4517.631311838698 (best loss: 4087.76)\n",
      "in attempt 572 the loss was 4663.641202499711 (best loss: 4087.76)\n",
      "in attempt 573 the loss was 4398.090772861298 (best loss: 4087.76)\n",
      "in attempt 574 the loss was 4409.0517076484075 (best loss: 4087.76)\n",
      "in attempt 575 the loss was 4459.2232986623385 (best loss: 4087.76)\n",
      "in attempt 576 the loss was 4537.775991092349 (best loss: 4087.76)\n",
      "in attempt 577 the loss was 4649.068117591574 (best loss: 4087.76)\n",
      "in attempt 578 the loss was 4618.173211593263 (best loss: 4087.76)\n",
      "in attempt 579 the loss was 4220.789627670522 (best loss: 4087.76)\n",
      "in attempt 580 the loss was 4321.4648564036315 (best loss: 4087.76)\n",
      "in attempt 581 the loss was 4422.3990261887975 (best loss: 4087.76)\n",
      "in attempt 582 the loss was 4500.757364378709 (best loss: 4087.76)\n",
      "in attempt 583 the loss was 4579.424842395988 (best loss: 4087.76)\n",
      "in attempt 584 the loss was 4673.9858951978695 (best loss: 4087.76)\n",
      "in attempt 585 the loss was 4724.825005535586 (best loss: 4087.76)\n",
      "in attempt 586 the loss was 4697.001477970844 (best loss: 4087.76)\n",
      "in attempt 587 the loss was 4926.480722505754 (best loss: 4087.76)\n",
      "in attempt 588 the loss was 4665.959221946748 (best loss: 4087.76)\n",
      "in attempt 589 the loss was 4653.0566187977 (best loss: 4087.76)\n",
      "in attempt 590 the loss was 4535.922492876378 (best loss: 4087.76)\n",
      "in attempt 591 the loss was 4329.037178974711 (best loss: 4087.76)\n",
      "in attempt 592 the loss was 4464.058779542069 (best loss: 4087.76)\n",
      "in attempt 593 the loss was 4618.143959441625 (best loss: 4087.76)\n",
      "in attempt 594 the loss was 4537.998530002855 (best loss: 4087.76)\n",
      "in attempt 595 the loss was 4694.276399703473 (best loss: 4087.76)\n",
      "in attempt 596 the loss was 4384.8858593384375 (best loss: 4087.76)\n",
      "in attempt 597 the loss was 4254.422083971643 (best loss: 4087.76)\n",
      "in attempt 598 the loss was 4155.318070327659 (best loss: 4087.76)\n",
      "in attempt 599 the loss was 4681.018645850223 (best loss: 4087.76)\n",
      "in attempt 600 the loss was 4652.436378126503 (best loss: 4087.76)\n",
      "in attempt 601 the loss was 4729.362858962286 (best loss: 4087.76)\n",
      "in attempt 602 the loss was 4423.795048179246 (best loss: 4087.76)\n",
      "in attempt 603 the loss was 4664.538645356194 (best loss: 4087.76)\n",
      "in attempt 604 the loss was 4279.627407251295 (best loss: 4087.76)\n",
      "in attempt 605 the loss was 4471.881222755048 (best loss: 4087.76)\n",
      "in attempt 606 the loss was 4576.623795706195 (best loss: 4087.76)\n",
      "in attempt 607 the loss was 4603.9395549716555 (best loss: 4087.76)\n",
      "in attempt 608 the loss was 4585.672481095553 (best loss: 4087.76)\n",
      "in attempt 609 the loss was 4359.496686513679 (best loss: 4087.76)\n",
      "in attempt 610 the loss was 4876.804025863637 (best loss: 4087.76)\n",
      "in attempt 611 the loss was 4700.25179659107 (best loss: 4087.76)\n",
      "in attempt 612 the loss was 4383.094200901222 (best loss: 4087.76)\n",
      "in attempt 613 the loss was 4587.360992613861 (best loss: 4087.76)\n",
      "in attempt 614 the loss was 4539.633368638621 (best loss: 4087.76)\n",
      "in attempt 615 the loss was 4431.437223790068 (best loss: 4087.76)\n",
      "in attempt 616 the loss was 4571.251719053701 (best loss: 4087.76)\n",
      "in attempt 617 the loss was 4513.21594670925 (best loss: 4087.76)\n",
      "in attempt 618 the loss was 4325.408953364689 (best loss: 4087.76)\n",
      "in attempt 619 the loss was 4249.065095658354 (best loss: 4087.76)\n",
      "in attempt 620 the loss was 4650.585564390223 (best loss: 4087.76)\n",
      "in attempt 621 the loss was 4570.923285547131 (best loss: 4087.76)\n",
      "in attempt 622 the loss was 4312.122297473946 (best loss: 4087.76)\n",
      "in attempt 623 the loss was 4459.598077487876 (best loss: 4087.76)\n",
      "in attempt 624 the loss was 4465.235756459986 (best loss: 4087.76)\n",
      "in attempt 625 the loss was 4516.435663660698 (best loss: 4087.76)\n",
      "in attempt 626 the loss was 4525.349562315218 (best loss: 4087.76)\n",
      "in attempt 627 the loss was 4665.7240704791475 (best loss: 4087.76)\n",
      "in attempt 628 the loss was 4405.423005625405 (best loss: 4087.76)\n",
      "in attempt 629 the loss was 4537.565662541743 (best loss: 4087.76)\n",
      "in attempt 630 the loss was 4882.731637119372 (best loss: 4087.76)\n",
      "in attempt 631 the loss was 4649.852611524397 (best loss: 4087.76)\n",
      "in attempt 632 the loss was 4607.887169606995 (best loss: 4087.76)\n",
      "in attempt 633 the loss was 4683.766398921142 (best loss: 4087.76)\n",
      "in attempt 634 the loss was 4484.279073148293 (best loss: 4087.76)\n",
      "in attempt 635 the loss was 4481.36414956364 (best loss: 4087.76)\n",
      "in attempt 636 the loss was 4584.670354155098 (best loss: 4087.76)\n",
      "in attempt 637 the loss was 4512.303064057485 (best loss: 4087.76)\n",
      "in attempt 638 the loss was 4628.404575709259 (best loss: 4087.76)\n",
      "in attempt 639 the loss was 4464.108088229375 (best loss: 4087.76)\n",
      "in attempt 640 the loss was 4261.811072545885 (best loss: 4087.76)\n",
      "in attempt 641 the loss was 4802.053153760591 (best loss: 4087.76)\n",
      "in attempt 642 the loss was 4459.45195291084 (best loss: 4087.76)\n",
      "in attempt 643 the loss was 4560.610807543547 (best loss: 4087.76)\n",
      "in attempt 644 the loss was 4605.659716647355 (best loss: 4087.76)\n",
      "in attempt 645 the loss was 4731.034239248636 (best loss: 4087.76)\n",
      "in attempt 646 the loss was 4458.167882505374 (best loss: 4087.76)\n",
      "in attempt 647 the loss was 4759.396609442613 (best loss: 4087.76)\n",
      "in attempt 648 the loss was 4566.0937820447225 (best loss: 4087.76)\n",
      "in attempt 649 the loss was 4561.681379564921 (best loss: 4087.76)\n",
      "in attempt 650 the loss was 4654.7860157062305 (best loss: 4087.76)\n",
      "in attempt 651 the loss was 4531.8521215218 (best loss: 4087.76)\n",
      "in attempt 652 the loss was 4274.958737444572 (best loss: 4087.76)\n",
      "in attempt 653 the loss was 4395.906747691257 (best loss: 4087.76)\n",
      "in attempt 654 the loss was 4766.462400628168 (best loss: 4087.76)\n",
      "in attempt 655 the loss was 4470.706939279877 (best loss: 4087.76)\n",
      "in attempt 656 the loss was 4515.356582489103 (best loss: 4087.76)\n",
      "in attempt 657 the loss was 4305.2418294114095 (best loss: 4087.76)\n",
      "in attempt 658 the loss was 4520.478555152698 (best loss: 4087.76)\n",
      "in attempt 659 the loss was 4432.143414969904 (best loss: 4087.76)\n",
      "in attempt 660 the loss was 4549.961913960828 (best loss: 4087.76)\n",
      "in attempt 661 the loss was 4410.079573052699 (best loss: 4087.76)\n",
      "in attempt 662 the loss was 4662.168926056038 (best loss: 4087.76)\n",
      "in attempt 663 the loss was 4442.442320993534 (best loss: 4087.76)\n",
      "in attempt 664 the loss was 4582.9451121854145 (best loss: 4087.76)\n",
      "in attempt 665 the loss was 4644.275740245502 (best loss: 4087.76)\n",
      "in attempt 666 the loss was 4706.029695160609 (best loss: 4087.76)\n",
      "in attempt 667 the loss was 4685.148834419859 (best loss: 4087.76)\n",
      "in attempt 668 the loss was 4469.034762035013 (best loss: 4087.76)\n",
      "in attempt 669 the loss was 4682.448439222247 (best loss: 4087.76)\n",
      "in attempt 670 the loss was 4541.34318634448 (best loss: 4087.76)\n",
      "in attempt 671 the loss was 4499.501794901873 (best loss: 4087.76)\n",
      "in attempt 672 the loss was 4844.455360154555 (best loss: 4087.76)\n",
      "in attempt 673 the loss was 4449.585305133727 (best loss: 4087.76)\n",
      "in attempt 674 the loss was 4556.167745979441 (best loss: 4087.76)\n",
      "in attempt 675 the loss was 4725.320183371336 (best loss: 4087.76)\n",
      "in attempt 676 the loss was 4344.784136907245 (best loss: 4087.76)\n",
      "in attempt 677 the loss was 4537.462844408694 (best loss: 4087.76)\n",
      "in attempt 678 the loss was 4639.759913780517 (best loss: 4087.76)\n",
      "in attempt 679 the loss was 4453.382990307566 (best loss: 4087.76)\n",
      "in attempt 680 the loss was 4414.602462496561 (best loss: 4087.76)\n",
      "in attempt 681 the loss was 4480.5253814749585 (best loss: 4087.76)\n",
      "in attempt 682 the loss was 4505.287774133704 (best loss: 4087.76)\n",
      "in attempt 683 the loss was 4464.619914685097 (best loss: 4087.76)\n",
      "in attempt 684 the loss was 4590.22345889579 (best loss: 4087.76)\n",
      "in attempt 685 the loss was 4349.555952228859 (best loss: 4087.76)\n",
      "in attempt 686 the loss was 4734.080840154645 (best loss: 4087.76)\n",
      "in attempt 687 the loss was 4737.954930117201 (best loss: 4087.76)\n",
      "in attempt 688 the loss was 4636.6409841687855 (best loss: 4087.76)\n",
      "in attempt 689 the loss was 4578.456769440805 (best loss: 4087.76)\n",
      "in attempt 690 the loss was 4228.085516243742 (best loss: 4087.76)\n",
      "in attempt 691 the loss was 4243.12284190879 (best loss: 4087.76)\n",
      "in attempt 692 the loss was 4563.990133771271 (best loss: 4087.76)\n",
      "in attempt 693 the loss was 4651.696392514626 (best loss: 4087.76)\n",
      "in attempt 694 the loss was 4545.895180747599 (best loss: 4087.76)\n",
      "in attempt 695 the loss was 4912.013325429855 (best loss: 4087.76)\n",
      "in attempt 696 the loss was 4761.292129185961 (best loss: 4087.76)\n",
      "in attempt 697 the loss was 4652.5005040496135 (best loss: 4087.76)\n",
      "in attempt 698 the loss was 4300.987855092612 (best loss: 4087.76)\n",
      "in attempt 699 the loss was 4577.861027410997 (best loss: 4087.76)\n",
      "in attempt 700 the loss was 4659.283481004035 (best loss: 4087.76)\n",
      "in attempt 701 the loss was 4249.070229161496 (best loss: 4087.76)\n",
      "in attempt 702 the loss was 4461.298616682065 (best loss: 4087.76)\n",
      "in attempt 703 the loss was 4583.432025931247 (best loss: 4087.76)\n",
      "in attempt 704 the loss was 4240.821507134908 (best loss: 4087.76)\n",
      "in attempt 705 the loss was 4644.130956620948 (best loss: 4087.76)\n",
      "in attempt 706 the loss was 4805.608660851934 (best loss: 4087.76)\n",
      "in attempt 707 the loss was 4545.20545502311 (best loss: 4087.76)\n",
      "in attempt 708 the loss was 4438.619889293288 (best loss: 4087.76)\n",
      "in attempt 709 the loss was 4324.508180801095 (best loss: 4087.76)\n",
      "in attempt 710 the loss was 4454.149919928069 (best loss: 4087.76)\n",
      "in attempt 711 the loss was 4617.302820281934 (best loss: 4087.76)\n",
      "in attempt 712 the loss was 4759.1510458522225 (best loss: 4087.76)\n",
      "in attempt 713 the loss was 4416.856198625216 (best loss: 4087.76)\n",
      "in attempt 714 the loss was 4433.759441235401 (best loss: 4087.76)\n",
      "in attempt 715 the loss was 4631.841090181315 (best loss: 4087.76)\n",
      "in attempt 716 the loss was 4524.574985181098 (best loss: 4087.76)\n",
      "in attempt 717 the loss was 4462.290092635783 (best loss: 4087.76)\n",
      "in attempt 718 the loss was 4619.643431199388 (best loss: 4087.76)\n",
      "in attempt 719 the loss was 4546.286949574797 (best loss: 4087.76)\n",
      "in attempt 720 the loss was 4389.455352235691 (best loss: 4087.76)\n",
      "in attempt 721 the loss was 4432.151956275155 (best loss: 4087.76)\n",
      "in attempt 722 the loss was 4855.577253294397 (best loss: 4087.76)\n",
      "in attempt 723 the loss was 4266.038327420354 (best loss: 4087.76)\n",
      "in attempt 724 the loss was 4720.3614894789935 (best loss: 4087.76)\n",
      "in attempt 725 the loss was 4499.926607070611 (best loss: 4087.76)\n",
      "in attempt 726 the loss was 4658.149141029757 (best loss: 4087.76)\n",
      "in attempt 727 the loss was 4490.209855443361 (best loss: 4087.76)\n",
      "in attempt 728 the loss was 4676.728342246124 (best loss: 4087.76)\n",
      "in attempt 729 the loss was 4719.123254874284 (best loss: 4087.76)\n",
      "in attempt 730 the loss was 4677.4642349269125 (best loss: 4087.76)\n",
      "in attempt 731 the loss was 4640.417335676761 (best loss: 4087.76)\n",
      "in attempt 732 the loss was 4699.677069217066 (best loss: 4087.76)\n",
      "in attempt 733 the loss was 4509.300593571872 (best loss: 4087.76)\n",
      "in attempt 734 the loss was 4629.6300127836885 (best loss: 4087.76)\n",
      "in attempt 735 the loss was 4765.470523499991 (best loss: 4087.76)\n",
      "in attempt 736 the loss was 4663.430086675371 (best loss: 4087.76)\n",
      "in attempt 737 the loss was 4523.534969840609 (best loss: 4087.76)\n",
      "in attempt 738 the loss was 4347.503214583363 (best loss: 4087.76)\n",
      "in attempt 739 the loss was 4932.2213294424855 (best loss: 4087.76)\n",
      "in attempt 740 the loss was 4377.569838594155 (best loss: 4087.76)\n",
      "in attempt 741 the loss was 4418.96390138089 (best loss: 4087.76)\n",
      "in attempt 742 the loss was 4513.047203022356 (best loss: 4087.76)\n",
      "in attempt 743 the loss was 4315.224301330087 (best loss: 4087.76)\n",
      "in attempt 744 the loss was 4608.772160488409 (best loss: 4087.76)\n",
      "in attempt 745 the loss was 4456.616380196818 (best loss: 4087.76)\n",
      "in attempt 746 the loss was 4527.216571808134 (best loss: 4087.76)\n",
      "in attempt 747 the loss was 4545.7852990270885 (best loss: 4087.76)\n",
      "in attempt 748 the loss was 4704.738746124732 (best loss: 4087.76)\n",
      "in attempt 749 the loss was 4551.020637693192 (best loss: 4087.76)\n",
      "in attempt 750 the loss was 4557.5411080211115 (best loss: 4087.76)\n",
      "in attempt 751 the loss was 4602.611528378453 (best loss: 4087.76)\n",
      "in attempt 752 the loss was 4507.9319810436355 (best loss: 4087.76)\n",
      "in attempt 753 the loss was 4621.390391588658 (best loss: 4087.76)\n",
      "in attempt 754 the loss was 4518.268824452314 (best loss: 4087.76)\n",
      "in attempt 755 the loss was 4553.461424659022 (best loss: 4087.76)\n",
      "in attempt 756 the loss was 4875.787504359198 (best loss: 4087.76)\n",
      "in attempt 757 the loss was 4409.003526380813 (best loss: 4087.76)\n",
      "in attempt 758 the loss was 4286.8258422414165 (best loss: 4087.76)\n",
      "in attempt 759 the loss was 4647.9834576729345 (best loss: 4087.76)\n",
      "in attempt 760 the loss was 4366.179528887838 (best loss: 4087.76)\n",
      "in attempt 761 the loss was 4441.595832543735 (best loss: 4087.76)\n",
      "in attempt 762 the loss was 4594.599072670695 (best loss: 4087.76)\n",
      "in attempt 763 the loss was 4507.203735199739 (best loss: 4087.76)\n",
      "in attempt 764 the loss was 4322.393561483903 (best loss: 4087.76)\n",
      "in attempt 765 the loss was 4481.468721479665 (best loss: 4087.76)\n",
      "in attempt 766 the loss was 4549.001407202297 (best loss: 4087.76)\n",
      "in attempt 767 the loss was 4487.190331296524 (best loss: 4087.76)\n",
      "in attempt 768 the loss was 4486.53081526855 (best loss: 4087.76)\n",
      "in attempt 769 the loss was 4501.185002704867 (best loss: 4087.76)\n",
      "in attempt 770 the loss was 4327.170822067198 (best loss: 4087.76)\n",
      "in attempt 771 the loss was 4370.2235107260785 (best loss: 4087.76)\n",
      "in attempt 772 the loss was 4730.329635821117 (best loss: 4087.76)\n",
      "in attempt 773 the loss was 4291.620122647038 (best loss: 4087.76)\n",
      "in attempt 774 the loss was 4540.599614284487 (best loss: 4087.76)\n",
      "in attempt 775 the loss was 4881.314877129636 (best loss: 4087.76)\n",
      "in attempt 776 the loss was 4400.576201333233 (best loss: 4087.76)\n",
      "in attempt 777 the loss was 4460.630330199977 (best loss: 4087.76)\n",
      "in attempt 778 the loss was 4483.015236372029 (best loss: 4087.76)\n",
      "in attempt 779 the loss was 4352.189571312141 (best loss: 4087.76)\n",
      "in attempt 780 the loss was 4593.397372613341 (best loss: 4087.76)\n",
      "in attempt 781 the loss was 4534.870753772691 (best loss: 4087.76)\n",
      "in attempt 782 the loss was 4645.21265567965 (best loss: 4087.76)\n",
      "in attempt 783 the loss was 4444.982637207149 (best loss: 4087.76)\n",
      "in attempt 784 the loss was 4439.519868519239 (best loss: 4087.76)\n",
      "in attempt 785 the loss was 4406.884013331911 (best loss: 4087.76)\n",
      "in attempt 786 the loss was 4399.732193603813 (best loss: 4087.76)\n",
      "in attempt 787 the loss was 4376.205802537447 (best loss: 4087.76)\n",
      "in attempt 788 the loss was 4684.061401417637 (best loss: 4087.76)\n",
      "in attempt 789 the loss was 4518.284450862775 (best loss: 4087.76)\n",
      "in attempt 790 the loss was 4560.356403472441 (best loss: 4087.76)\n",
      "in attempt 791 the loss was 4800.3249483744385 (best loss: 4087.76)\n",
      "in attempt 792 the loss was 4481.134192749476 (best loss: 4087.76)\n",
      "in attempt 793 the loss was 4516.523211637802 (best loss: 4087.76)\n",
      "in attempt 794 the loss was 4583.017063269327 (best loss: 4087.76)\n",
      "in attempt 795 the loss was 4485.753535435229 (best loss: 4087.76)\n",
      "in attempt 796 the loss was 4562.317541589363 (best loss: 4087.76)\n",
      "in attempt 797 the loss was 4516.9103491058795 (best loss: 4087.76)\n",
      "in attempt 798 the loss was 4289.401550509014 (best loss: 4087.76)\n",
      "in attempt 799 the loss was 4552.861192375238 (best loss: 4087.76)\n",
      "in attempt 800 the loss was 4523.952425471678 (best loss: 4087.76)\n",
      "in attempt 801 the loss was 4619.063428706051 (best loss: 4087.76)\n",
      "in attempt 802 the loss was 4279.667225760063 (best loss: 4087.76)\n",
      "in attempt 803 the loss was 4271.035000357502 (best loss: 4087.76)\n",
      "in attempt 804 the loss was 4553.774132004055 (best loss: 4087.76)\n",
      "in attempt 805 the loss was 4619.630809748276 (best loss: 4087.76)\n",
      "in attempt 806 the loss was 4990.041235509667 (best loss: 4087.76)\n",
      "in attempt 807 the loss was 4570.727926304211 (best loss: 4087.76)\n",
      "in attempt 808 the loss was 4389.954893980281 (best loss: 4087.76)\n",
      "in attempt 809 the loss was 4656.536343293154 (best loss: 4087.76)\n",
      "in attempt 810 the loss was 4179.9675901274895 (best loss: 4087.76)\n",
      "in attempt 811 the loss was 4512.71078759666 (best loss: 4087.76)\n",
      "in attempt 812 the loss was 4249.303148717225 (best loss: 4087.76)\n",
      "in attempt 813 the loss was 4450.942556829145 (best loss: 4087.76)\n",
      "in attempt 814 the loss was 4515.489748550663 (best loss: 4087.76)\n",
      "in attempt 815 the loss was 4432.738009762679 (best loss: 4087.76)\n",
      "in attempt 816 the loss was 4222.756121338936 (best loss: 4087.76)\n",
      "in attempt 817 the loss was 4460.308357529557 (best loss: 4087.76)\n",
      "in attempt 818 the loss was 4637.065254075197 (best loss: 4087.76)\n",
      "in attempt 819 the loss was 4375.376073922278 (best loss: 4087.76)\n",
      "in attempt 820 the loss was 4504.619030358851 (best loss: 4087.76)\n",
      "in attempt 821 the loss was 4504.779102527863 (best loss: 4087.76)\n",
      "in attempt 822 the loss was 4414.53541373486 (best loss: 4087.76)\n",
      "in attempt 823 the loss was 4747.087344127211 (best loss: 4087.76)\n",
      "in attempt 824 the loss was 4562.8598527859685 (best loss: 4087.76)\n",
      "in attempt 825 the loss was 4432.755395567981 (best loss: 4087.76)\n",
      "in attempt 826 the loss was 4877.181711230163 (best loss: 4087.76)\n",
      "in attempt 827 the loss was 4609.197978119025 (best loss: 4087.76)\n",
      "in attempt 828 the loss was 4522.662404310053 (best loss: 4087.76)\n",
      "in attempt 829 the loss was 4609.725768009071 (best loss: 4087.76)\n",
      "in attempt 830 the loss was 4367.931884657693 (best loss: 4087.76)\n",
      "in attempt 831 the loss was 4303.249261804869 (best loss: 4087.76)\n",
      "in attempt 832 the loss was 4695.611848821749 (best loss: 4087.76)\n",
      "in attempt 833 the loss was 4680.077121789907 (best loss: 4087.76)\n",
      "in attempt 834 the loss was 4340.008390290344 (best loss: 4087.76)\n",
      "in attempt 835 the loss was 4403.137111680579 (best loss: 4087.76)\n",
      "in attempt 836 the loss was 4674.506047918476 (best loss: 4087.76)\n",
      "in attempt 837 the loss was 4521.556189579903 (best loss: 4087.76)\n",
      "in attempt 838 the loss was 4631.197460612051 (best loss: 4087.76)\n",
      "in attempt 839 the loss was 4616.0942754174175 (best loss: 4087.76)\n",
      "in attempt 840 the loss was 4599.26996523219 (best loss: 4087.76)\n",
      "in attempt 841 the loss was 4485.340815955571 (best loss: 4087.76)\n",
      "in attempt 842 the loss was 4742.951079264289 (best loss: 4087.76)\n",
      "in attempt 843 the loss was 4875.840585482192 (best loss: 4087.76)\n",
      "in attempt 844 the loss was 4348.790982660789 (best loss: 4087.76)\n",
      "in attempt 845 the loss was 4542.160146080045 (best loss: 4087.76)\n",
      "in attempt 846 the loss was 4360.093456114619 (best loss: 4087.76)\n",
      "in attempt 847 the loss was 4571.831433044174 (best loss: 4087.76)\n",
      "in attempt 848 the loss was 4759.512586552568 (best loss: 4087.76)\n",
      "in attempt 849 the loss was 4489.445161350484 (best loss: 4087.76)\n",
      "in attempt 850 the loss was 4453.331596137992 (best loss: 4087.76)\n",
      "in attempt 851 the loss was 4590.81561878798 (best loss: 4087.76)\n",
      "in attempt 852 the loss was 4430.96990029301 (best loss: 4087.76)\n",
      "in attempt 853 the loss was 4231.75665637858 (best loss: 4087.76)\n",
      "in attempt 854 the loss was 4525.326205144681 (best loss: 4087.76)\n",
      "in attempt 855 the loss was 4476.179609178709 (best loss: 4087.76)\n",
      "in attempt 856 the loss was 4397.328445147552 (best loss: 4087.76)\n",
      "in attempt 857 the loss was 4502.271165978845 (best loss: 4087.76)\n",
      "in attempt 858 the loss was 4781.464395785887 (best loss: 4087.76)\n",
      "in attempt 859 the loss was 4388.337907861715 (best loss: 4087.76)\n",
      "in attempt 860 the loss was 4480.632813657192 (best loss: 4087.76)\n",
      "in attempt 861 the loss was 4456.64387736973 (best loss: 4087.76)\n",
      "in attempt 862 the loss was 4367.461098856407 (best loss: 4087.76)\n",
      "in attempt 863 the loss was 4524.602243311674 (best loss: 4087.76)\n",
      "in attempt 864 the loss was 4551.894063095705 (best loss: 4087.76)\n",
      "in attempt 865 the loss was 4542.625355174578 (best loss: 4087.76)\n",
      "in attempt 866 the loss was 4462.50934812032 (best loss: 4087.76)\n",
      "in attempt 867 the loss was 4611.032348317278 (best loss: 4087.76)\n",
      "in attempt 868 the loss was 4314.903593520741 (best loss: 4087.76)\n",
      "in attempt 869 the loss was 4562.914136707907 (best loss: 4087.76)\n",
      "in attempt 870 the loss was 4400.29241496915 (best loss: 4087.76)\n",
      "in attempt 871 the loss was 4722.528566430663 (best loss: 4087.76)\n",
      "in attempt 872 the loss was 4537.233555375682 (best loss: 4087.76)\n",
      "in attempt 873 the loss was 4673.006140299563 (best loss: 4087.76)\n",
      "in attempt 874 the loss was 4514.544693176551 (best loss: 4087.76)\n",
      "in attempt 875 the loss was 4493.744789865224 (best loss: 4087.76)\n",
      "in attempt 876 the loss was 4841.711854937429 (best loss: 4087.76)\n",
      "in attempt 877 the loss was 4372.7700665308475 (best loss: 4087.76)\n",
      "in attempt 878 the loss was 4602.529545002476 (best loss: 4087.76)\n",
      "in attempt 879 the loss was 4392.174656919837 (best loss: 4087.76)\n",
      "in attempt 880 the loss was 4678.49250174927 (best loss: 4087.76)\n",
      "in attempt 881 the loss was 4563.889916110534 (best loss: 4087.76)\n",
      "in attempt 882 the loss was 4650.630063059398 (best loss: 4087.76)\n",
      "in attempt 883 the loss was 4472.386420728964 (best loss: 4087.76)\n",
      "in attempt 884 the loss was 4770.808357802248 (best loss: 4087.76)\n",
      "in attempt 885 the loss was 4378.175326155662 (best loss: 4087.76)\n",
      "in attempt 886 the loss was 4643.852304664843 (best loss: 4087.76)\n",
      "in attempt 887 the loss was 4461.071071822629 (best loss: 4087.76)\n",
      "in attempt 888 the loss was 4466.56735521158 (best loss: 4087.76)\n",
      "in attempt 889 the loss was 4571.158245641262 (best loss: 4087.76)\n",
      "in attempt 890 the loss was 4490.092671464739 (best loss: 4087.76)\n",
      "in attempt 891 the loss was 4583.355972503054 (best loss: 4087.76)\n",
      "in attempt 892 the loss was 4457.882518912788 (best loss: 4087.76)\n",
      "in attempt 893 the loss was 4479.603658934717 (best loss: 4087.76)\n",
      "in attempt 894 the loss was 4690.809239737171 (best loss: 4087.76)\n",
      "in attempt 895 the loss was 4501.082859366108 (best loss: 4087.76)\n",
      "in attempt 896 the loss was 4579.8577936355105 (best loss: 4087.76)\n",
      "in attempt 897 the loss was 4487.6576762206905 (best loss: 4087.76)\n",
      "in attempt 898 the loss was 4312.288086048386 (best loss: 4087.76)\n",
      "in attempt 899 the loss was 4593.789949555526 (best loss: 4087.76)\n",
      "in attempt 900 the loss was 4579.405110872061 (best loss: 4087.76)\n",
      "in attempt 901 the loss was 4508.645504621909 (best loss: 4087.76)\n",
      "in attempt 902 the loss was 4508.50929535305 (best loss: 4087.76)\n",
      "in attempt 903 the loss was 4171.141529259722 (best loss: 4087.76)\n",
      "in attempt 904 the loss was 4899.399383832643 (best loss: 4087.76)\n",
      "in attempt 905 the loss was 4436.475929485159 (best loss: 4087.76)\n",
      "in attempt 906 the loss was 4470.075147323207 (best loss: 4087.76)\n",
      "in attempt 907 the loss was 4619.4234410807785 (best loss: 4087.76)\n",
      "in attempt 908 the loss was 4279.211003614671 (best loss: 4087.76)\n",
      "in attempt 909 the loss was 4697.396862115184 (best loss: 4087.76)\n",
      "in attempt 910 the loss was 4605.229250662831 (best loss: 4087.76)\n",
      "in attempt 911 the loss was 4465.581721833996 (best loss: 4087.76)\n",
      "in attempt 912 the loss was 4505.339809095749 (best loss: 4087.76)\n",
      "in attempt 913 the loss was 4645.127655760911 (best loss: 4087.76)\n",
      "in attempt 914 the loss was 4639.305087684631 (best loss: 4087.76)\n",
      "in attempt 915 the loss was 4584.6511943099995 (best loss: 4087.76)\n",
      "in attempt 916 the loss was 4451.744335528045 (best loss: 4087.76)\n",
      "in attempt 917 the loss was 4473.573544319157 (best loss: 4087.76)\n",
      "in attempt 918 the loss was 4598.017017505179 (best loss: 4087.76)\n",
      "in attempt 919 the loss was 4372.366532876912 (best loss: 4087.76)\n",
      "in attempt 920 the loss was 4543.539582111672 (best loss: 4087.76)\n",
      "in attempt 921 the loss was 4577.326822554032 (best loss: 4087.76)\n",
      "in attempt 922 the loss was 4466.9458295657605 (best loss: 4087.76)\n",
      "in attempt 923 the loss was 4558.958064755592 (best loss: 4087.76)\n",
      "in attempt 924 the loss was 4532.163603829623 (best loss: 4087.76)\n",
      "in attempt 925 the loss was 4433.901172793192 (best loss: 4087.76)\n",
      "in attempt 926 the loss was 4480.082152680745 (best loss: 4087.76)\n",
      "in attempt 927 the loss was 4763.700219373646 (best loss: 4087.76)\n",
      "in attempt 928 the loss was 4510.132270573146 (best loss: 4087.76)\n",
      "in attempt 929 the loss was 4422.462747418567 (best loss: 4087.76)\n",
      "in attempt 930 the loss was 4664.463489989233 (best loss: 4087.76)\n",
      "in attempt 931 the loss was 4768.897381286467 (best loss: 4087.76)\n",
      "in attempt 932 the loss was 4357.222689276436 (best loss: 4087.76)\n",
      "in attempt 933 the loss was 4531.346115502318 (best loss: 4087.76)\n",
      "in attempt 934 the loss was 4359.152857666545 (best loss: 4087.76)\n",
      "in attempt 935 the loss was 4240.614614297513 (best loss: 4087.76)\n",
      "in attempt 936 the loss was 4658.679672552721 (best loss: 4087.76)\n",
      "in attempt 937 the loss was 4398.7362729758315 (best loss: 4087.76)\n",
      "in attempt 938 the loss was 4725.670153813739 (best loss: 4087.76)\n",
      "in attempt 939 the loss was 4556.710199823399 (best loss: 4087.76)\n",
      "in attempt 940 the loss was 4687.571001179397 (best loss: 4087.76)\n",
      "in attempt 941 the loss was 4577.059976620574 (best loss: 4087.76)\n",
      "in attempt 942 the loss was 4546.646050400543 (best loss: 4087.76)\n",
      "in attempt 943 the loss was 4325.241875827008 (best loss: 4087.76)\n",
      "in attempt 944 the loss was 4316.245942885208 (best loss: 4087.76)\n",
      "in attempt 945 the loss was 4297.270713071991 (best loss: 4087.76)\n",
      "in attempt 946 the loss was 4749.890209616838 (best loss: 4087.76)\n",
      "in attempt 947 the loss was 4463.80473434703 (best loss: 4087.76)\n",
      "in attempt 948 the loss was 4582.010537117894 (best loss: 4087.76)\n",
      "in attempt 949 the loss was 4610.468136466806 (best loss: 4087.76)\n",
      "in attempt 950 the loss was 4328.984486137747 (best loss: 4087.76)\n",
      "in attempt 951 the loss was 4599.047995030012 (best loss: 4087.76)\n",
      "in attempt 952 the loss was 4581.476229774059 (best loss: 4087.76)\n",
      "in attempt 953 the loss was 4648.897424919422 (best loss: 4087.76)\n",
      "in attempt 954 the loss was 4598.115663692627 (best loss: 4087.76)\n",
      "in attempt 955 the loss was 4631.54182686031 (best loss: 4087.76)\n",
      "in attempt 956 the loss was 4735.192035815951 (best loss: 4087.76)\n",
      "in attempt 957 the loss was 4523.912789141693 (best loss: 4087.76)\n",
      "in attempt 958 the loss was 4535.957269383808 (best loss: 4087.76)\n",
      "in attempt 959 the loss was 4357.0197811807275 (best loss: 4087.76)\n",
      "in attempt 960 the loss was 4517.400196638648 (best loss: 4087.76)\n",
      "in attempt 961 the loss was 4664.040092506432 (best loss: 4087.76)\n",
      "in attempt 962 the loss was 4571.340400693616 (best loss: 4087.76)\n",
      "in attempt 963 the loss was 4604.465589761963 (best loss: 4087.76)\n",
      "in attempt 964 the loss was 4264.850796853983 (best loss: 4087.76)\n",
      "in attempt 965 the loss was 4497.306126602203 (best loss: 4087.76)\n",
      "in attempt 966 the loss was 4463.020103104011 (best loss: 4087.76)\n",
      "in attempt 967 the loss was 4819.493726492913 (best loss: 4087.76)\n",
      "in attempt 968 the loss was 4356.757666437681 (best loss: 4087.76)\n",
      "in attempt 969 the loss was 4261.441169526863 (best loss: 4087.76)\n",
      "in attempt 970 the loss was 4894.101996183046 (best loss: 4087.76)\n",
      "in attempt 971 the loss was 4471.569370903942 (best loss: 4087.76)\n",
      "in attempt 972 the loss was 4621.921519512907 (best loss: 4087.76)\n",
      "in attempt 973 the loss was 4612.648808690488 (best loss: 4087.76)\n",
      "in attempt 974 the loss was 4543.467630646821 (best loss: 4087.76)\n",
      "in attempt 975 the loss was 4489.343747954711 (best loss: 4087.76)\n",
      "in attempt 976 the loss was 4913.992971959207 (best loss: 4087.76)\n",
      "in attempt 977 the loss was 4446.517569249381 (best loss: 4087.76)\n",
      "in attempt 978 the loss was 4606.623354434389 (best loss: 4087.76)\n",
      "in attempt 979 the loss was 4515.160092349241 (best loss: 4087.76)\n",
      "in attempt 980 the loss was 4764.082566048592 (best loss: 4087.76)\n",
      "in attempt 981 the loss was 4364.77823398566 (best loss: 4087.76)\n",
      "in attempt 982 the loss was 4414.528786306471 (best loss: 4087.76)\n",
      "in attempt 983 the loss was 4315.618662730112 (best loss: 4087.76)\n",
      "in attempt 984 the loss was 4290.861103547214 (best loss: 4087.76)\n",
      "in attempt 985 the loss was 4472.1641874452225 (best loss: 4087.76)\n",
      "in attempt 986 the loss was 4526.817731283922 (best loss: 4087.76)\n",
      "in attempt 987 the loss was 4837.017954694813 (best loss: 4087.76)\n",
      "in attempt 988 the loss was 4678.676099600699 (best loss: 4087.76)\n",
      "in attempt 989 the loss was 4565.2760480069555 (best loss: 4087.76)\n",
      "in attempt 990 the loss was 4737.795136944199 (best loss: 4087.76)\n",
      "in attempt 991 the loss was 4619.874150930812 (best loss: 4087.76)\n",
      "in attempt 992 the loss was 4542.662845962364 (best loss: 4087.76)\n",
      "in attempt 993 the loss was 4374.502961643842 (best loss: 4087.76)\n",
      "in attempt 994 the loss was 4397.869389973765 (best loss: 4087.76)\n",
      "in attempt 995 the loss was 4482.490324307306 (best loss: 4087.76)\n",
      "in attempt 996 the loss was 4395.405639385792 (best loss: 4087.76)\n",
      "in attempt 997 the loss was 4726.113729240433 (best loss: 4087.76)\n",
      "in attempt 998 the loss was 4379.560196210456 (best loss: 4087.76)\n",
      "in attempt 999 the loss was 4386.842744738293 (best loss: 4087.76)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Random Search\n",
    "\n",
    "np.random.seed(42)\n",
    "# seed 0 best loss = 3966.41\n",
    "# seed 3 best loss = 3912.75\n",
    "# seed 20 best loss = 3968.10\n",
    "# seed 42 best loss = 3893.36\n",
    "bestloss = float('inf')\n",
    "for num in range(1000):\n",
    "    W = np.random.randn(10, 3073) * 0.0001 # generate random weights\n",
    "    s, loss = L(X_dev, y_dev, W)\n",
    "    if loss < bestloss:\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print(f'in attempt {num} the loss was {loss} (best loss: {bestloss:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19948cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random search accuracy on test set: 8.80%\n"
     ]
    }
   ],
   "source": [
    "# To see how well it works on the test set\n",
    "\n",
    "score = scores(X_test, bestW)\n",
    "Y_pred = np.argmax(score, axis=0)\n",
    "accuracy = np.mean(Y_pred == y_test)\n",
    "print(f'Random search accuracy on test set: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c9e4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 0 the loss was 9504.953000221085 (best loss: 9504.95)\n",
      "in attempt 1 the loss was 9673.037594295056 (best loss: 9504.95)\n",
      "in attempt 2 the loss was 9572.373207494731 (best loss: 9504.95)\n",
      "in attempt 3 the loss was 9446.641050532042 (best loss: 9446.64)\n",
      "in attempt 4 the loss was 9594.740694847162 (best loss: 9446.64)\n",
      "in attempt 5 the loss was 9470.582520542499 (best loss: 9446.64)\n",
      "in attempt 6 the loss was 9613.354345676142 (best loss: 9446.64)\n",
      "in attempt 7 the loss was 9398.600413986536 (best loss: 9398.60)\n",
      "in attempt 8 the loss was 9476.206924176964 (best loss: 9398.60)\n",
      "in attempt 9 the loss was 9490.244740708564 (best loss: 9398.60)\n",
      "in attempt 10 the loss was 9491.040387080207 (best loss: 9398.60)\n",
      "in attempt 11 the loss was 9289.363719904282 (best loss: 9289.36)\n",
      "in attempt 12 the loss was 9270.032134691282 (best loss: 9270.03)\n",
      "in attempt 13 the loss was 9389.034414583293 (best loss: 9270.03)\n",
      "in attempt 14 the loss was 9307.265337006493 (best loss: 9270.03)\n",
      "in attempt 15 the loss was 9320.094540206524 (best loss: 9270.03)\n",
      "in attempt 16 the loss was 9237.34479593035 (best loss: 9237.34)\n",
      "in attempt 17 the loss was 9281.267424985175 (best loss: 9237.34)\n",
      "in attempt 18 the loss was 9246.862051295895 (best loss: 9237.34)\n",
      "in attempt 19 the loss was 9253.473259240058 (best loss: 9237.34)\n",
      "in attempt 20 the loss was 9225.67755925775 (best loss: 9225.68)\n",
      "in attempt 21 the loss was 9286.164096173588 (best loss: 9225.68)\n",
      "in attempt 22 the loss was 9339.081543977016 (best loss: 9225.68)\n",
      "in attempt 23 the loss was 9378.771217228152 (best loss: 9225.68)\n",
      "in attempt 24 the loss was 9221.67543820847 (best loss: 9221.68)\n",
      "in attempt 25 the loss was 9306.632928992325 (best loss: 9221.68)\n",
      "in attempt 26 the loss was 9195.07706394787 (best loss: 9195.08)\n",
      "in attempt 27 the loss was 9223.52462241872 (best loss: 9195.08)\n",
      "in attempt 28 the loss was 9069.522742922754 (best loss: 9069.52)\n",
      "in attempt 29 the loss was 9121.362658630434 (best loss: 9069.52)\n",
      "in attempt 30 the loss was 8964.994936012401 (best loss: 8964.99)\n",
      "in attempt 31 the loss was 9054.621770542028 (best loss: 8964.99)\n",
      "in attempt 32 the loss was 8851.966202383566 (best loss: 8851.97)\n",
      "in attempt 33 the loss was 8861.022392559134 (best loss: 8851.97)\n",
      "in attempt 34 the loss was 8843.84035781278 (best loss: 8843.84)\n",
      "in attempt 35 the loss was 8840.04848311836 (best loss: 8840.05)\n",
      "in attempt 36 the loss was 8909.194331022998 (best loss: 8840.05)\n",
      "in attempt 37 the loss was 8803.518352986797 (best loss: 8803.52)\n",
      "in attempt 38 the loss was 8907.240911495142 (best loss: 8803.52)\n",
      "in attempt 39 the loss was 8829.916183434909 (best loss: 8803.52)\n",
      "in attempt 40 the loss was 8796.126149503692 (best loss: 8796.13)\n",
      "in attempt 41 the loss was 8811.0173327195 (best loss: 8796.13)\n",
      "in attempt 42 the loss was 8790.0823890173 (best loss: 8790.08)\n",
      "in attempt 43 the loss was 8686.944853219371 (best loss: 8686.94)\n",
      "in attempt 44 the loss was 8761.299780340632 (best loss: 8686.94)\n",
      "in attempt 45 the loss was 8849.307636361833 (best loss: 8686.94)\n",
      "in attempt 46 the loss was 8687.320730885553 (best loss: 8686.94)\n",
      "in attempt 47 the loss was 8829.861570410996 (best loss: 8686.94)\n",
      "in attempt 48 the loss was 8642.831883679375 (best loss: 8642.83)\n",
      "in attempt 49 the loss was 8665.516289003772 (best loss: 8642.83)\n",
      "in attempt 50 the loss was 8725.558902799297 (best loss: 8642.83)\n",
      "in attempt 51 the loss was 8641.003046226042 (best loss: 8641.00)\n",
      "in attempt 52 the loss was 8614.616681826812 (best loss: 8614.62)\n",
      "in attempt 53 the loss was 8656.125718656902 (best loss: 8614.62)\n",
      "in attempt 54 the loss was 8618.267502773215 (best loss: 8614.62)\n",
      "in attempt 55 the loss was 8631.80904825001 (best loss: 8614.62)\n",
      "in attempt 56 the loss was 8738.475279286475 (best loss: 8614.62)\n",
      "in attempt 57 the loss was 8663.51090709942 (best loss: 8614.62)\n",
      "in attempt 58 the loss was 8848.972441704364 (best loss: 8614.62)\n",
      "in attempt 59 the loss was 8635.92092463121 (best loss: 8614.62)\n",
      "in attempt 60 the loss was 8695.069390675186 (best loss: 8614.62)\n",
      "in attempt 61 the loss was 8601.303804116684 (best loss: 8601.30)\n",
      "in attempt 62 the loss was 8656.584001102561 (best loss: 8601.30)\n",
      "in attempt 63 the loss was 8636.083193246628 (best loss: 8601.30)\n",
      "in attempt 64 the loss was 8637.524133582638 (best loss: 8601.30)\n",
      "in attempt 65 the loss was 8761.21492189785 (best loss: 8601.30)\n",
      "in attempt 66 the loss was 8646.350479043504 (best loss: 8601.30)\n",
      "in attempt 67 the loss was 8517.464389152594 (best loss: 8517.46)\n",
      "in attempt 68 the loss was 8492.253043047955 (best loss: 8492.25)\n",
      "in attempt 69 the loss was 8667.678419913664 (best loss: 8492.25)\n",
      "in attempt 70 the loss was 8512.411836742145 (best loss: 8492.25)\n",
      "in attempt 71 the loss was 8606.696377885875 (best loss: 8492.25)\n",
      "in attempt 72 the loss was 8565.40572975281 (best loss: 8492.25)\n",
      "in attempt 73 the loss was 8563.598481515173 (best loss: 8492.25)\n",
      "in attempt 74 the loss was 8612.081444554407 (best loss: 8492.25)\n",
      "in attempt 75 the loss was 8618.922151945007 (best loss: 8492.25)\n",
      "in attempt 76 the loss was 8577.137262082048 (best loss: 8492.25)\n",
      "in attempt 77 the loss was 8611.797343337432 (best loss: 8492.25)\n",
      "in attempt 78 the loss was 8584.336691126158 (best loss: 8492.25)\n",
      "in attempt 79 the loss was 8609.482636565754 (best loss: 8492.25)\n",
      "in attempt 80 the loss was 8722.420616178639 (best loss: 8492.25)\n",
      "in attempt 81 the loss was 8614.143132237916 (best loss: 8492.25)\n",
      "in attempt 82 the loss was 8446.72062883043 (best loss: 8446.72)\n",
      "in attempt 83 the loss was 8499.505094445049 (best loss: 8446.72)\n",
      "in attempt 84 the loss was 8459.849689097355 (best loss: 8446.72)\n",
      "in attempt 85 the loss was 8579.229100074937 (best loss: 8446.72)\n",
      "in attempt 86 the loss was 8457.570509200945 (best loss: 8446.72)\n",
      "in attempt 87 the loss was 8475.374009776153 (best loss: 8446.72)\n",
      "in attempt 88 the loss was 8510.925525728539 (best loss: 8446.72)\n",
      "in attempt 89 the loss was 8597.143817591608 (best loss: 8446.72)\n",
      "in attempt 90 the loss was 8582.462020050909 (best loss: 8446.72)\n",
      "in attempt 91 the loss was 8434.9302755629 (best loss: 8434.93)\n",
      "in attempt 92 the loss was 8466.444757115387 (best loss: 8434.93)\n",
      "in attempt 93 the loss was 8453.091432961563 (best loss: 8434.93)\n",
      "in attempt 94 the loss was 8559.776664652778 (best loss: 8434.93)\n",
      "in attempt 95 the loss was 8402.83527435784 (best loss: 8402.84)\n",
      "in attempt 96 the loss was 8500.107001328382 (best loss: 8402.84)\n",
      "in attempt 97 the loss was 8456.177121579178 (best loss: 8402.84)\n",
      "in attempt 98 the loss was 8556.242079288091 (best loss: 8402.84)\n",
      "in attempt 99 the loss was 8381.813240311341 (best loss: 8381.81)\n",
      "in attempt 100 the loss was 8551.595899072427 (best loss: 8381.81)\n",
      "in attempt 101 the loss was 8339.703324347349 (best loss: 8339.70)\n",
      "in attempt 102 the loss was 8281.23235387472 (best loss: 8281.23)\n",
      "in attempt 103 the loss was 8446.231230890124 (best loss: 8281.23)\n",
      "in attempt 104 the loss was 8343.552189748023 (best loss: 8281.23)\n",
      "in attempt 105 the loss was 8370.641167752137 (best loss: 8281.23)\n",
      "in attempt 106 the loss was 8335.084385294273 (best loss: 8281.23)\n",
      "in attempt 107 the loss was 8445.745519448927 (best loss: 8281.23)\n",
      "in attempt 108 the loss was 8143.320028685235 (best loss: 8143.32)\n",
      "in attempt 109 the loss was 8180.154156506073 (best loss: 8143.32)\n",
      "in attempt 110 the loss was 8242.145443739548 (best loss: 8143.32)\n",
      "in attempt 111 the loss was 8135.246387432213 (best loss: 8135.25)\n",
      "in attempt 112 the loss was 8060.557912845163 (best loss: 8060.56)\n",
      "in attempt 113 the loss was 8179.362850042392 (best loss: 8060.56)\n",
      "in attempt 114 the loss was 8024.55049595484 (best loss: 8024.55)\n",
      "in attempt 115 the loss was 8054.991278166315 (best loss: 8024.55)\n",
      "in attempt 116 the loss was 7982.896007894806 (best loss: 7982.90)\n",
      "in attempt 117 the loss was 8024.9062811147305 (best loss: 7982.90)\n",
      "in attempt 118 the loss was 8006.139543829366 (best loss: 7982.90)\n",
      "in attempt 119 the loss was 8059.91964417997 (best loss: 7982.90)\n",
      "in attempt 120 the loss was 8034.576150463319 (best loss: 7982.90)\n",
      "in attempt 121 the loss was 8006.639993135748 (best loss: 7982.90)\n",
      "in attempt 122 the loss was 7996.941433451773 (best loss: 7982.90)\n",
      "in attempt 123 the loss was 8039.89087940881 (best loss: 7982.90)\n",
      "in attempt 124 the loss was 8010.211482177091 (best loss: 7982.90)\n",
      "in attempt 125 the loss was 8040.008711107844 (best loss: 7982.90)\n",
      "in attempt 126 the loss was 7920.0939191678945 (best loss: 7920.09)\n",
      "in attempt 127 the loss was 7908.221601432717 (best loss: 7908.22)\n",
      "in attempt 128 the loss was 7964.730287385151 (best loss: 7908.22)\n",
      "in attempt 129 the loss was 7960.087269803729 (best loss: 7908.22)\n",
      "in attempt 130 the loss was 7903.965240471868 (best loss: 7903.97)\n",
      "in attempt 131 the loss was 7944.0686455588075 (best loss: 7903.97)\n",
      "in attempt 132 the loss was 8022.5488644008 (best loss: 7903.97)\n",
      "in attempt 133 the loss was 7946.02640187837 (best loss: 7903.97)\n",
      "in attempt 134 the loss was 7940.409988753459 (best loss: 7903.97)\n",
      "in attempt 135 the loss was 7841.060907433552 (best loss: 7841.06)\n",
      "in attempt 136 the loss was 7988.451434522255 (best loss: 7841.06)\n",
      "in attempt 137 the loss was 7865.885253444364 (best loss: 7841.06)\n",
      "in attempt 138 the loss was 7843.87286455538 (best loss: 7841.06)\n",
      "in attempt 139 the loss was 7882.5986191784505 (best loss: 7841.06)\n",
      "in attempt 140 the loss was 7792.763870884764 (best loss: 7792.76)\n",
      "in attempt 141 the loss was 7847.371852785132 (best loss: 7792.76)\n",
      "in attempt 142 the loss was 7772.511297081499 (best loss: 7772.51)\n",
      "in attempt 143 the loss was 7858.494741217704 (best loss: 7772.51)\n",
      "in attempt 144 the loss was 7691.9591880163025 (best loss: 7691.96)\n",
      "in attempt 145 the loss was 7648.132814908371 (best loss: 7648.13)\n",
      "in attempt 146 the loss was 7820.916434181288 (best loss: 7648.13)\n",
      "in attempt 147 the loss was 7667.294947212791 (best loss: 7648.13)\n",
      "in attempt 148 the loss was 7700.646156657036 (best loss: 7648.13)\n",
      "in attempt 149 the loss was 7692.617948064408 (best loss: 7648.13)\n",
      "in attempt 150 the loss was 7702.654152316417 (best loss: 7648.13)\n",
      "in attempt 151 the loss was 7660.958220616041 (best loss: 7648.13)\n",
      "in attempt 152 the loss was 7704.066690336243 (best loss: 7648.13)\n",
      "in attempt 153 the loss was 7602.091625376052 (best loss: 7602.09)\n",
      "in attempt 154 the loss was 7544.377320917685 (best loss: 7544.38)\n",
      "in attempt 155 the loss was 7580.352746252651 (best loss: 7544.38)\n",
      "in attempt 156 the loss was 7585.149716548535 (best loss: 7544.38)\n",
      "in attempt 157 the loss was 7680.177158565695 (best loss: 7544.38)\n",
      "in attempt 158 the loss was 7560.702943832144 (best loss: 7544.38)\n",
      "in attempt 159 the loss was 7661.680688671329 (best loss: 7544.38)\n",
      "in attempt 160 the loss was 7611.8460922656095 (best loss: 7544.38)\n",
      "in attempt 161 the loss was 7643.840049142242 (best loss: 7544.38)\n",
      "in attempt 162 the loss was 7550.718268446226 (best loss: 7544.38)\n",
      "in attempt 163 the loss was 7580.304606548068 (best loss: 7544.38)\n",
      "in attempt 164 the loss was 7569.659090448778 (best loss: 7544.38)\n",
      "in attempt 165 the loss was 7679.572971140352 (best loss: 7544.38)\n",
      "in attempt 166 the loss was 7706.710787153206 (best loss: 7544.38)\n",
      "in attempt 167 the loss was 7512.036617228753 (best loss: 7512.04)\n",
      "in attempt 168 the loss was 7592.4107823699 (best loss: 7512.04)\n",
      "in attempt 169 the loss was 7626.1331674055655 (best loss: 7512.04)\n",
      "in attempt 170 the loss was 7561.398532117959 (best loss: 7512.04)\n",
      "in attempt 171 the loss was 7469.766547957904 (best loss: 7469.77)\n",
      "in attempt 172 the loss was 7543.308852649423 (best loss: 7469.77)\n",
      "in attempt 173 the loss was 7548.192592214715 (best loss: 7469.77)\n",
      "in attempt 174 the loss was 7558.096383677308 (best loss: 7469.77)\n",
      "in attempt 175 the loss was 7435.178694301523 (best loss: 7435.18)\n",
      "in attempt 176 the loss was 7487.132174084245 (best loss: 7435.18)\n",
      "in attempt 177 the loss was 7358.411719072897 (best loss: 7358.41)\n",
      "in attempt 178 the loss was 7325.579637648916 (best loss: 7325.58)\n",
      "in attempt 179 the loss was 7260.630725240455 (best loss: 7260.63)\n",
      "in attempt 180 the loss was 7320.105392827216 (best loss: 7260.63)\n",
      "in attempt 181 the loss was 7253.949282211419 (best loss: 7253.95)\n",
      "in attempt 182 the loss was 7317.8360368203175 (best loss: 7253.95)\n",
      "in attempt 183 the loss was 7246.782622002957 (best loss: 7246.78)\n",
      "in attempt 184 the loss was 7386.203897012491 (best loss: 7246.78)\n",
      "in attempt 185 the loss was 7310.0516255661605 (best loss: 7246.78)\n",
      "in attempt 186 the loss was 7381.290032368719 (best loss: 7246.78)\n",
      "in attempt 187 the loss was 7318.593724690648 (best loss: 7246.78)\n",
      "in attempt 188 the loss was 7290.095195047476 (best loss: 7246.78)\n",
      "in attempt 189 the loss was 7360.755153246511 (best loss: 7246.78)\n",
      "in attempt 190 the loss was 7335.6481708199135 (best loss: 7246.78)\n",
      "in attempt 191 the loss was 7214.017741342859 (best loss: 7214.02)\n",
      "in attempt 192 the loss was 7311.875151240734 (best loss: 7214.02)\n",
      "in attempt 193 the loss was 7126.136998435613 (best loss: 7126.14)\n",
      "in attempt 194 the loss was 7127.189327506623 (best loss: 7126.14)\n",
      "in attempt 195 the loss was 7216.719647971672 (best loss: 7126.14)\n",
      "in attempt 196 the loss was 7155.408062164679 (best loss: 7126.14)\n",
      "in attempt 197 the loss was 7164.83856826071 (best loss: 7126.14)\n",
      "in attempt 198 the loss was 7202.6679121456 (best loss: 7126.14)\n",
      "in attempt 199 the loss was 7169.729732386427 (best loss: 7126.14)\n",
      "in attempt 200 the loss was 7125.153507292027 (best loss: 7125.15)\n",
      "in attempt 201 the loss was 7173.695540972827 (best loss: 7125.15)\n",
      "in attempt 202 the loss was 7234.616001049526 (best loss: 7125.15)\n",
      "in attempt 203 the loss was 7215.970681558503 (best loss: 7125.15)\n",
      "in attempt 204 the loss was 7159.669829698292 (best loss: 7125.15)\n",
      "in attempt 205 the loss was 7174.353682449923 (best loss: 7125.15)\n",
      "in attempt 206 the loss was 7093.5354453975015 (best loss: 7093.54)\n",
      "in attempt 207 the loss was 7171.78756845498 (best loss: 7093.54)\n",
      "in attempt 208 the loss was 7151.746036837949 (best loss: 7093.54)\n",
      "in attempt 209 the loss was 7203.083971686099 (best loss: 7093.54)\n",
      "in attempt 210 the loss was 7111.198941677411 (best loss: 7093.54)\n",
      "in attempt 211 the loss was 7187.848213027744 (best loss: 7093.54)\n",
      "in attempt 212 the loss was 7226.480793368919 (best loss: 7093.54)\n",
      "in attempt 213 the loss was 7086.09348566855 (best loss: 7086.09)\n",
      "in attempt 214 the loss was 7201.549540487707 (best loss: 7086.09)\n",
      "in attempt 215 the loss was 7157.0270646569525 (best loss: 7086.09)\n",
      "in attempt 216 the loss was 7165.98038243942 (best loss: 7086.09)\n",
      "in attempt 217 the loss was 7171.350070346309 (best loss: 7086.09)\n",
      "in attempt 218 the loss was 7140.05023717214 (best loss: 7086.09)\n",
      "in attempt 219 the loss was 7188.025792672946 (best loss: 7086.09)\n",
      "in attempt 220 the loss was 7099.762123555279 (best loss: 7086.09)\n",
      "in attempt 221 the loss was 7052.45600065665 (best loss: 7052.46)\n",
      "in attempt 222 the loss was 7125.443609300484 (best loss: 7052.46)\n",
      "in attempt 223 the loss was 7136.503495205454 (best loss: 7052.46)\n",
      "in attempt 224 the loss was 7118.448974337467 (best loss: 7052.46)\n",
      "in attempt 225 the loss was 7065.269556190516 (best loss: 7052.46)\n",
      "in attempt 226 the loss was 7123.080993503497 (best loss: 7052.46)\n",
      "in attempt 227 the loss was 7081.782272284515 (best loss: 7052.46)\n",
      "in attempt 228 the loss was 7168.3156212934155 (best loss: 7052.46)\n",
      "in attempt 229 the loss was 7157.974564478649 (best loss: 7052.46)\n",
      "in attempt 230 the loss was 7099.003680466985 (best loss: 7052.46)\n",
      "in attempt 231 the loss was 7074.273184601956 (best loss: 7052.46)\n",
      "in attempt 232 the loss was 7024.425075855167 (best loss: 7024.43)\n",
      "in attempt 233 the loss was 7168.75889220067 (best loss: 7024.43)\n",
      "in attempt 234 the loss was 6990.711226383944 (best loss: 6990.71)\n",
      "in attempt 235 the loss was 7079.837857883211 (best loss: 6990.71)\n",
      "in attempt 236 the loss was 6955.43509826439 (best loss: 6955.44)\n",
      "in attempt 237 the loss was 7047.8862371630985 (best loss: 6955.44)\n",
      "in attempt 238 the loss was 6990.767639127815 (best loss: 6955.44)\n",
      "in attempt 239 the loss was 6941.134570017257 (best loss: 6941.13)\n",
      "in attempt 240 the loss was 7031.1554982638545 (best loss: 6941.13)\n",
      "in attempt 241 the loss was 6954.043548631533 (best loss: 6941.13)\n",
      "in attempt 242 the loss was 6984.0313143049 (best loss: 6941.13)\n",
      "in attempt 243 the loss was 6931.528326490682 (best loss: 6931.53)\n",
      "in attempt 244 the loss was 6965.586268009149 (best loss: 6931.53)\n",
      "in attempt 245 the loss was 7010.655545469612 (best loss: 6931.53)\n",
      "in attempt 246 the loss was 6968.582901330197 (best loss: 6931.53)\n",
      "in attempt 247 the loss was 6932.03708130785 (best loss: 6931.53)\n",
      "in attempt 248 the loss was 6967.76052376408 (best loss: 6931.53)\n",
      "in attempt 249 the loss was 6958.160808587316 (best loss: 6931.53)\n",
      "in attempt 250 the loss was 6982.781752476641 (best loss: 6931.53)\n",
      "in attempt 251 the loss was 6927.914883413857 (best loss: 6927.91)\n",
      "in attempt 252 the loss was 6899.942598912208 (best loss: 6899.94)\n",
      "in attempt 253 the loss was 7019.783919550666 (best loss: 6899.94)\n",
      "in attempt 254 the loss was 6856.532204072649 (best loss: 6856.53)\n",
      "in attempt 255 the loss was 6842.2688036708205 (best loss: 6842.27)\n",
      "in attempt 256 the loss was 6882.139842403254 (best loss: 6842.27)\n",
      "in attempt 257 the loss was 6909.803692772262 (best loss: 6842.27)\n",
      "in attempt 258 the loss was 6954.11472264228 (best loss: 6842.27)\n",
      "in attempt 259 the loss was 6923.993965863259 (best loss: 6842.27)\n",
      "in attempt 260 the loss was 6805.541766220742 (best loss: 6805.54)\n",
      "in attempt 261 the loss was 6805.62990919527 (best loss: 6805.54)\n",
      "in attempt 262 the loss was 6836.838587680093 (best loss: 6805.54)\n",
      "in attempt 263 the loss was 6828.056497427375 (best loss: 6805.54)\n",
      "in attempt 264 the loss was 6834.921307615284 (best loss: 6805.54)\n",
      "in attempt 265 the loss was 6941.144415203675 (best loss: 6805.54)\n",
      "in attempt 266 the loss was 6836.094537217946 (best loss: 6805.54)\n",
      "in attempt 267 the loss was 6846.239002123264 (best loss: 6805.54)\n",
      "in attempt 268 the loss was 6848.221423701843 (best loss: 6805.54)\n",
      "in attempt 269 the loss was 6867.862955078766 (best loss: 6805.54)\n",
      "in attempt 270 the loss was 6812.22713418088 (best loss: 6805.54)\n",
      "in attempt 271 the loss was 6936.947764032537 (best loss: 6805.54)\n",
      "in attempt 272 the loss was 6858.639342407711 (best loss: 6805.54)\n",
      "in attempt 273 the loss was 6867.686782561086 (best loss: 6805.54)\n",
      "in attempt 274 the loss was 6809.670373137001 (best loss: 6805.54)\n",
      "in attempt 275 the loss was 6834.34516970215 (best loss: 6805.54)\n",
      "in attempt 276 the loss was 6855.324745148989 (best loss: 6805.54)\n",
      "in attempt 277 the loss was 6880.222478576578 (best loss: 6805.54)\n",
      "in attempt 278 the loss was 6861.028215478145 (best loss: 6805.54)\n",
      "in attempt 279 the loss was 6831.652065329803 (best loss: 6805.54)\n",
      "in attempt 280 the loss was 6892.825202909335 (best loss: 6805.54)\n",
      "in attempt 281 the loss was 6893.6174292041 (best loss: 6805.54)\n",
      "in attempt 282 the loss was 6854.58336087598 (best loss: 6805.54)\n",
      "in attempt 283 the loss was 6870.355525471262 (best loss: 6805.54)\n",
      "in attempt 284 the loss was 6803.75077079372 (best loss: 6803.75)\n",
      "in attempt 285 the loss was 6899.398855277244 (best loss: 6803.75)\n",
      "in attempt 286 the loss was 6908.05780083883 (best loss: 6803.75)\n",
      "in attempt 287 the loss was 6935.311047331457 (best loss: 6803.75)\n",
      "in attempt 288 the loss was 6812.853853522566 (best loss: 6803.75)\n",
      "in attempt 289 the loss was 6800.236794550116 (best loss: 6800.24)\n",
      "in attempt 290 the loss was 6850.997482463449 (best loss: 6800.24)\n",
      "in attempt 291 the loss was 6851.544090919732 (best loss: 6800.24)\n",
      "in attempt 292 the loss was 6777.720127395351 (best loss: 6777.72)\n",
      "in attempt 293 the loss was 6800.93254241336 (best loss: 6777.72)\n",
      "in attempt 294 the loss was 6812.189613403443 (best loss: 6777.72)\n",
      "in attempt 295 the loss was 6740.355125316599 (best loss: 6740.36)\n",
      "in attempt 296 the loss was 6766.66990202551 (best loss: 6740.36)\n",
      "in attempt 297 the loss was 6802.619787758093 (best loss: 6740.36)\n",
      "in attempt 298 the loss was 6811.365140790511 (best loss: 6740.36)\n",
      "in attempt 299 the loss was 6744.178416284867 (best loss: 6740.36)\n",
      "in attempt 300 the loss was 6764.509757881917 (best loss: 6740.36)\n",
      "in attempt 301 the loss was 6775.657439335488 (best loss: 6740.36)\n",
      "in attempt 302 the loss was 6772.1449343514105 (best loss: 6740.36)\n",
      "in attempt 303 the loss was 6874.8637684498835 (best loss: 6740.36)\n",
      "in attempt 304 the loss was 6857.81274878235 (best loss: 6740.36)\n",
      "in attempt 305 the loss was 6656.167365852182 (best loss: 6656.17)\n",
      "in attempt 306 the loss was 6693.419979416719 (best loss: 6656.17)\n",
      "in attempt 307 the loss was 6692.624102070069 (best loss: 6656.17)\n",
      "in attempt 308 the loss was 6714.943035957664 (best loss: 6656.17)\n",
      "in attempt 309 the loss was 6716.586852113629 (best loss: 6656.17)\n",
      "in attempt 310 the loss was 6714.964855141274 (best loss: 6656.17)\n",
      "in attempt 311 the loss was 6687.580575282322 (best loss: 6656.17)\n",
      "in attempt 312 the loss was 6704.174508876982 (best loss: 6656.17)\n",
      "in attempt 313 the loss was 6704.373529103357 (best loss: 6656.17)\n",
      "in attempt 314 the loss was 6709.970496937276 (best loss: 6656.17)\n",
      "in attempt 315 the loss was 6633.656187645303 (best loss: 6633.66)\n",
      "in attempt 316 the loss was 6689.745114817559 (best loss: 6633.66)\n",
      "in attempt 317 the loss was 6656.604303275499 (best loss: 6633.66)\n",
      "in attempt 318 the loss was 6612.5373679561535 (best loss: 6612.54)\n",
      "in attempt 319 the loss was 6531.579029513481 (best loss: 6531.58)\n",
      "in attempt 320 the loss was 6608.271062847296 (best loss: 6531.58)\n",
      "in attempt 321 the loss was 6624.383061656907 (best loss: 6531.58)\n",
      "in attempt 322 the loss was 6552.073998289783 (best loss: 6531.58)\n",
      "in attempt 323 the loss was 6627.364116910556 (best loss: 6531.58)\n",
      "in attempt 324 the loss was 6590.177249828364 (best loss: 6531.58)\n",
      "in attempt 325 the loss was 6538.295988154318 (best loss: 6531.58)\n",
      "in attempt 326 the loss was 6585.228543497072 (best loss: 6531.58)\n",
      "in attempt 327 the loss was 6571.7100590150585 (best loss: 6531.58)\n",
      "in attempt 328 the loss was 6600.583402405648 (best loss: 6531.58)\n",
      "in attempt 329 the loss was 6585.785514460339 (best loss: 6531.58)\n",
      "in attempt 330 the loss was 6570.4726118255385 (best loss: 6531.58)\n",
      "in attempt 331 the loss was 6574.616232212203 (best loss: 6531.58)\n",
      "in attempt 332 the loss was 6617.74467959265 (best loss: 6531.58)\n",
      "in attempt 333 the loss was 6524.370152739337 (best loss: 6524.37)\n",
      "in attempt 334 the loss was 6564.475638979871 (best loss: 6524.37)\n",
      "in attempt 335 the loss was 6547.9412920441755 (best loss: 6524.37)\n",
      "in attempt 336 the loss was 6565.154524364799 (best loss: 6524.37)\n",
      "in attempt 337 the loss was 6632.18021156527 (best loss: 6524.37)\n",
      "in attempt 338 the loss was 6591.733296828159 (best loss: 6524.37)\n",
      "in attempt 339 the loss was 6584.1965280331715 (best loss: 6524.37)\n",
      "in attempt 340 the loss was 6546.154761547901 (best loss: 6524.37)\n",
      "in attempt 341 the loss was 6564.368312357232 (best loss: 6524.37)\n",
      "in attempt 342 the loss was 6639.078065599453 (best loss: 6524.37)\n",
      "in attempt 343 the loss was 6579.068325348764 (best loss: 6524.37)\n",
      "in attempt 344 the loss was 6589.430131174513 (best loss: 6524.37)\n",
      "in attempt 345 the loss was 6580.16929018516 (best loss: 6524.37)\n",
      "in attempt 346 the loss was 6556.011525419606 (best loss: 6524.37)\n",
      "in attempt 347 the loss was 6615.443501840784 (best loss: 6524.37)\n",
      "in attempt 348 the loss was 6587.94803752927 (best loss: 6524.37)\n",
      "in attempt 349 the loss was 6535.843522927759 (best loss: 6524.37)\n",
      "in attempt 350 the loss was 6565.370412373637 (best loss: 6524.37)\n",
      "in attempt 351 the loss was 6587.712928570018 (best loss: 6524.37)\n",
      "in attempt 352 the loss was 6603.310326727319 (best loss: 6524.37)\n",
      "in attempt 353 the loss was 6559.460089193068 (best loss: 6524.37)\n",
      "in attempt 354 the loss was 6548.559621048223 (best loss: 6524.37)\n",
      "in attempt 355 the loss was 6596.729656686872 (best loss: 6524.37)\n",
      "in attempt 356 the loss was 6599.6701267577955 (best loss: 6524.37)\n",
      "in attempt 357 the loss was 6502.230856900738 (best loss: 6502.23)\n",
      "in attempt 358 the loss was 6578.966417748554 (best loss: 6502.23)\n",
      "in attempt 359 the loss was 6489.56702471255 (best loss: 6489.57)\n",
      "in attempt 360 the loss was 6566.543158010876 (best loss: 6489.57)\n",
      "in attempt 361 the loss was 6570.286930990641 (best loss: 6489.57)\n",
      "in attempt 362 the loss was 6523.941625986786 (best loss: 6489.57)\n",
      "in attempt 363 the loss was 6516.035790624687 (best loss: 6489.57)\n",
      "in attempt 364 the loss was 6598.592066359435 (best loss: 6489.57)\n",
      "in attempt 365 the loss was 6526.252093865949 (best loss: 6489.57)\n",
      "in attempt 366 the loss was 6562.726143980644 (best loss: 6489.57)\n",
      "in attempt 367 the loss was 6504.046286686909 (best loss: 6489.57)\n",
      "in attempt 368 the loss was 6533.890391537731 (best loss: 6489.57)\n",
      "in attempt 369 the loss was 6480.672327159911 (best loss: 6480.67)\n",
      "in attempt 370 the loss was 6570.084873337619 (best loss: 6480.67)\n",
      "in attempt 371 the loss was 6478.323352438225 (best loss: 6478.32)\n",
      "in attempt 372 the loss was 6602.465153644211 (best loss: 6478.32)\n",
      "in attempt 373 the loss was 6541.554830069989 (best loss: 6478.32)\n",
      "in attempt 374 the loss was 6498.588081535493 (best loss: 6478.32)\n",
      "in attempt 375 the loss was 6537.554657543431 (best loss: 6478.32)\n",
      "in attempt 376 the loss was 6511.046183463901 (best loss: 6478.32)\n",
      "in attempt 377 the loss was 6520.227448832525 (best loss: 6478.32)\n",
      "in attempt 378 the loss was 6496.09693667029 (best loss: 6478.32)\n",
      "in attempt 379 the loss was 6542.825430227453 (best loss: 6478.32)\n",
      "in attempt 380 the loss was 6531.989175714888 (best loss: 6478.32)\n",
      "in attempt 381 the loss was 6505.444870183675 (best loss: 6478.32)\n",
      "in attempt 382 the loss was 6508.713614029379 (best loss: 6478.32)\n",
      "in attempt 383 the loss was 6484.380815496753 (best loss: 6478.32)\n",
      "in attempt 384 the loss was 6474.558652275467 (best loss: 6474.56)\n",
      "in attempt 385 the loss was 6565.7512751051645 (best loss: 6474.56)\n",
      "in attempt 386 the loss was 6544.434550377904 (best loss: 6474.56)\n",
      "in attempt 387 the loss was 6491.841132356509 (best loss: 6474.56)\n",
      "in attempt 388 the loss was 6493.333593565416 (best loss: 6474.56)\n",
      "in attempt 389 the loss was 6591.887068657412 (best loss: 6474.56)\n",
      "in attempt 390 the loss was 6469.980875621144 (best loss: 6469.98)\n",
      "in attempt 391 the loss was 6470.159887832728 (best loss: 6469.98)\n",
      "in attempt 392 the loss was 6486.684870773397 (best loss: 6469.98)\n",
      "in attempt 393 the loss was 6542.343794916156 (best loss: 6469.98)\n",
      "in attempt 394 the loss was 6494.490632405956 (best loss: 6469.98)\n",
      "in attempt 395 the loss was 6512.084139264538 (best loss: 6469.98)\n",
      "in attempt 396 the loss was 6506.631531793573 (best loss: 6469.98)\n",
      "in attempt 397 the loss was 6584.560827008885 (best loss: 6469.98)\n",
      "in attempt 398 the loss was 6529.155178959039 (best loss: 6469.98)\n",
      "in attempt 399 the loss was 6530.234559575814 (best loss: 6469.98)\n",
      "in attempt 400 the loss was 6553.114941773094 (best loss: 6469.98)\n",
      "in attempt 401 the loss was 6520.2690591032315 (best loss: 6469.98)\n",
      "in attempt 402 the loss was 6485.4760886567165 (best loss: 6469.98)\n",
      "in attempt 403 the loss was 6569.739719695741 (best loss: 6469.98)\n",
      "in attempt 404 the loss was 6494.781232535049 (best loss: 6469.98)\n",
      "in attempt 405 the loss was 6560.793984763712 (best loss: 6469.98)\n",
      "in attempt 406 the loss was 6557.97484958191 (best loss: 6469.98)\n",
      "in attempt 407 the loss was 6484.188801432909 (best loss: 6469.98)\n",
      "in attempt 408 the loss was 6504.643634478922 (best loss: 6469.98)\n",
      "in attempt 409 the loss was 6451.056891756258 (best loss: 6451.06)\n",
      "in attempt 410 the loss was 6513.087505503833 (best loss: 6451.06)\n",
      "in attempt 411 the loss was 6584.293601221345 (best loss: 6451.06)\n",
      "in attempt 412 the loss was 6533.280146454616 (best loss: 6451.06)\n",
      "in attempt 413 the loss was 6463.461782231927 (best loss: 6451.06)\n",
      "in attempt 414 the loss was 6564.487598906023 (best loss: 6451.06)\n",
      "in attempt 415 the loss was 6599.643287118309 (best loss: 6451.06)\n",
      "in attempt 416 the loss was 6457.24996770534 (best loss: 6451.06)\n",
      "in attempt 417 the loss was 6447.95484124571 (best loss: 6447.95)\n",
      "in attempt 418 the loss was 6450.808114651187 (best loss: 6447.95)\n",
      "in attempt 419 the loss was 6490.489598364227 (best loss: 6447.95)\n",
      "in attempt 420 the loss was 6507.491907046698 (best loss: 6447.95)\n",
      "in attempt 421 the loss was 6495.802100408631 (best loss: 6447.95)\n",
      "in attempt 422 the loss was 6453.475153944173 (best loss: 6447.95)\n",
      "in attempt 423 the loss was 6442.246076527811 (best loss: 6442.25)\n",
      "in attempt 424 the loss was 6493.5366047765365 (best loss: 6442.25)\n",
      "in attempt 425 the loss was 6490.869596877164 (best loss: 6442.25)\n",
      "in attempt 426 the loss was 6452.622984910047 (best loss: 6442.25)\n",
      "in attempt 427 the loss was 6368.704676128319 (best loss: 6368.70)\n",
      "in attempt 428 the loss was 6424.397712910513 (best loss: 6368.70)\n",
      "in attempt 429 the loss was 6386.955501316648 (best loss: 6368.70)\n",
      "in attempt 430 the loss was 6433.98126966265 (best loss: 6368.70)\n",
      "in attempt 431 the loss was 6432.355320132403 (best loss: 6368.70)\n",
      "in attempt 432 the loss was 6427.392787637972 (best loss: 6368.70)\n",
      "in attempt 433 the loss was 6347.431282236505 (best loss: 6347.43)\n",
      "in attempt 434 the loss was 6422.294392191541 (best loss: 6347.43)\n",
      "in attempt 435 the loss was 6359.567415705576 (best loss: 6347.43)\n",
      "in attempt 436 the loss was 6437.429879595924 (best loss: 6347.43)\n",
      "in attempt 437 the loss was 6451.826896737415 (best loss: 6347.43)\n",
      "in attempt 438 the loss was 6423.016602030688 (best loss: 6347.43)\n",
      "in attempt 439 the loss was 6420.074444931237 (best loss: 6347.43)\n",
      "in attempt 440 the loss was 6355.073668175524 (best loss: 6347.43)\n",
      "in attempt 441 the loss was 6409.554919495634 (best loss: 6347.43)\n",
      "in attempt 442 the loss was 6457.1927978378535 (best loss: 6347.43)\n",
      "in attempt 443 the loss was 6385.999052026636 (best loss: 6347.43)\n",
      "in attempt 444 the loss was 6322.178185312842 (best loss: 6322.18)\n",
      "in attempt 445 the loss was 6435.4229149707135 (best loss: 6322.18)\n",
      "in attempt 446 the loss was 6345.672548294911 (best loss: 6322.18)\n",
      "in attempt 447 the loss was 6375.262760350132 (best loss: 6322.18)\n",
      "in attempt 448 the loss was 6353.128605399968 (best loss: 6322.18)\n",
      "in attempt 449 the loss was 6401.857776220671 (best loss: 6322.18)\n",
      "in attempt 450 the loss was 6403.508709715767 (best loss: 6322.18)\n",
      "in attempt 451 the loss was 6351.809497907785 (best loss: 6322.18)\n",
      "in attempt 452 the loss was 6407.594677717996 (best loss: 6322.18)\n",
      "in attempt 453 the loss was 6386.88502601485 (best loss: 6322.18)\n",
      "in attempt 454 the loss was 6383.152759077007 (best loss: 6322.18)\n",
      "in attempt 455 the loss was 6374.59759985703 (best loss: 6322.18)\n",
      "in attempt 456 the loss was 6363.747186085613 (best loss: 6322.18)\n",
      "in attempt 457 the loss was 6420.314661041526 (best loss: 6322.18)\n",
      "in attempt 458 the loss was 6332.214111050358 (best loss: 6322.18)\n",
      "in attempt 459 the loss was 6420.62611425596 (best loss: 6322.18)\n",
      "in attempt 460 the loss was 6427.137412235872 (best loss: 6322.18)\n",
      "in attempt 461 the loss was 6288.432290698666 (best loss: 6288.43)\n",
      "in attempt 462 the loss was 6330.148543913938 (best loss: 6288.43)\n",
      "in attempt 463 the loss was 6286.558868742281 (best loss: 6286.56)\n",
      "in attempt 464 the loss was 6298.782440497409 (best loss: 6286.56)\n",
      "in attempt 465 the loss was 6368.880311165676 (best loss: 6286.56)\n",
      "in attempt 466 the loss was 6327.257171850702 (best loss: 6286.56)\n",
      "in attempt 467 the loss was 6362.955667161725 (best loss: 6286.56)\n",
      "in attempt 468 the loss was 6268.427133881602 (best loss: 6268.43)\n",
      "in attempt 469 the loss was 6357.053707600402 (best loss: 6268.43)\n",
      "in attempt 470 the loss was 6295.977952223452 (best loss: 6268.43)\n",
      "in attempt 471 the loss was 6382.497996588764 (best loss: 6268.43)\n",
      "in attempt 472 the loss was 6303.310904854254 (best loss: 6268.43)\n",
      "in attempt 473 the loss was 6363.608869754411 (best loss: 6268.43)\n",
      "in attempt 474 the loss was 6397.9927630977 (best loss: 6268.43)\n",
      "in attempt 475 the loss was 6412.907406049507 (best loss: 6268.43)\n",
      "in attempt 476 the loss was 6303.3652936468325 (best loss: 6268.43)\n",
      "in attempt 477 the loss was 6247.297122560634 (best loss: 6247.30)\n",
      "in attempt 478 the loss was 6213.4378125067005 (best loss: 6213.44)\n",
      "in attempt 479 the loss was 6272.791839798867 (best loss: 6213.44)\n",
      "in attempt 480 the loss was 6260.005763092917 (best loss: 6213.44)\n",
      "in attempt 481 the loss was 6253.831252120809 (best loss: 6213.44)\n",
      "in attempt 482 the loss was 6229.6043830774 (best loss: 6213.44)\n",
      "in attempt 483 the loss was 6201.824737409808 (best loss: 6201.82)\n",
      "in attempt 484 the loss was 6254.001998590389 (best loss: 6201.82)\n",
      "in attempt 485 the loss was 6311.493809261008 (best loss: 6201.82)\n",
      "in attempt 486 the loss was 6271.795861934355 (best loss: 6201.82)\n",
      "in attempt 487 the loss was 6200.259511368722 (best loss: 6200.26)\n",
      "in attempt 488 the loss was 6213.36806436895 (best loss: 6200.26)\n",
      "in attempt 489 the loss was 6326.0325443955335 (best loss: 6200.26)\n",
      "in attempt 490 the loss was 6330.8229334163025 (best loss: 6200.26)\n",
      "in attempt 491 the loss was 6221.25790381417 (best loss: 6200.26)\n",
      "in attempt 492 the loss was 6281.190548857285 (best loss: 6200.26)\n",
      "in attempt 493 the loss was 6273.675425912782 (best loss: 6200.26)\n",
      "in attempt 494 the loss was 6286.782128076768 (best loss: 6200.26)\n",
      "in attempt 495 the loss was 6256.89220922114 (best loss: 6200.26)\n",
      "in attempt 496 the loss was 6272.252342030732 (best loss: 6200.26)\n",
      "in attempt 497 the loss was 6148.693152178934 (best loss: 6148.69)\n",
      "in attempt 498 the loss was 6220.546374735813 (best loss: 6148.69)\n",
      "in attempt 499 the loss was 6252.502098217462 (best loss: 6148.69)\n",
      "in attempt 500 the loss was 6194.337010547863 (best loss: 6148.69)\n",
      "in attempt 501 the loss was 6145.502363535595 (best loss: 6145.50)\n",
      "in attempt 502 the loss was 6175.3060495034515 (best loss: 6145.50)\n",
      "in attempt 503 the loss was 6185.063161737662 (best loss: 6145.50)\n",
      "in attempt 504 the loss was 6182.291647312168 (best loss: 6145.50)\n",
      "in attempt 505 the loss was 6247.9778978411005 (best loss: 6145.50)\n",
      "in attempt 506 the loss was 6140.916062783699 (best loss: 6140.92)\n",
      "in attempt 507 the loss was 6273.02499765598 (best loss: 6140.92)\n",
      "in attempt 508 the loss was 6192.719707115093 (best loss: 6140.92)\n",
      "in attempt 509 the loss was 6262.664011599763 (best loss: 6140.92)\n",
      "in attempt 510 the loss was 6208.90048645625 (best loss: 6140.92)\n",
      "in attempt 511 the loss was 6156.123022146165 (best loss: 6140.92)\n",
      "in attempt 512 the loss was 6122.986717681136 (best loss: 6122.99)\n",
      "in attempt 513 the loss was 6171.535602378845 (best loss: 6122.99)\n",
      "in attempt 514 the loss was 6191.633476471609 (best loss: 6122.99)\n",
      "in attempt 515 the loss was 6255.599380107809 (best loss: 6122.99)\n",
      "in attempt 516 the loss was 6090.819743557342 (best loss: 6090.82)\n",
      "in attempt 517 the loss was 6094.091174758126 (best loss: 6090.82)\n",
      "in attempt 518 the loss was 6176.721134476282 (best loss: 6090.82)\n",
      "in attempt 519 the loss was 6154.863598479017 (best loss: 6090.82)\n",
      "in attempt 520 the loss was 6111.615304499521 (best loss: 6090.82)\n",
      "in attempt 521 the loss was 6234.9477724042135 (best loss: 6090.82)\n",
      "in attempt 522 the loss was 6137.479951104467 (best loss: 6090.82)\n",
      "in attempt 523 the loss was 6086.465223655604 (best loss: 6086.47)\n",
      "in attempt 524 the loss was 6124.373388750457 (best loss: 6086.47)\n",
      "in attempt 525 the loss was 6177.495189582321 (best loss: 6086.47)\n",
      "in attempt 526 the loss was 6197.644494016538 (best loss: 6086.47)\n",
      "in attempt 527 the loss was 6118.231141983993 (best loss: 6086.47)\n",
      "in attempt 528 the loss was 6115.485313965134 (best loss: 6086.47)\n",
      "in attempt 529 the loss was 6061.1987666796795 (best loss: 6061.20)\n",
      "in attempt 530 the loss was 6116.663355334342 (best loss: 6061.20)\n",
      "in attempt 531 the loss was 6144.733025675845 (best loss: 6061.20)\n",
      "in attempt 532 the loss was 6175.301945797153 (best loss: 6061.20)\n",
      "in attempt 533 the loss was 6106.839192746119 (best loss: 6061.20)\n",
      "in attempt 534 the loss was 6047.607390381961 (best loss: 6047.61)\n",
      "in attempt 535 the loss was 6089.21609531138 (best loss: 6047.61)\n",
      "in attempt 536 the loss was 6056.261018392358 (best loss: 6047.61)\n",
      "in attempt 537 the loss was 6081.13918461014 (best loss: 6047.61)\n",
      "in attempt 538 the loss was 6035.504329654557 (best loss: 6035.50)\n",
      "in attempt 539 the loss was 6108.39833336722 (best loss: 6035.50)\n",
      "in attempt 540 the loss was 6116.219502806921 (best loss: 6035.50)\n",
      "in attempt 541 the loss was 6134.222571243142 (best loss: 6035.50)\n",
      "in attempt 542 the loss was 6080.47431879529 (best loss: 6035.50)\n",
      "in attempt 543 the loss was 6054.4122965548595 (best loss: 6035.50)\n",
      "in attempt 544 the loss was 6077.627217931973 (best loss: 6035.50)\n",
      "in attempt 545 the loss was 6065.9947465352025 (best loss: 6035.50)\n",
      "in attempt 546 the loss was 6028.241836914265 (best loss: 6028.24)\n",
      "in attempt 547 the loss was 6058.25036428919 (best loss: 6028.24)\n",
      "in attempt 548 the loss was 6139.83035901039 (best loss: 6028.24)\n",
      "in attempt 549 the loss was 6132.4376684155495 (best loss: 6028.24)\n",
      "in attempt 550 the loss was 6029.041523004186 (best loss: 6028.24)\n",
      "in attempt 551 the loss was 6048.159197415716 (best loss: 6028.24)\n",
      "in attempt 552 the loss was 5988.920928699868 (best loss: 5988.92)\n",
      "in attempt 553 the loss was 6037.056052145053 (best loss: 5988.92)\n",
      "in attempt 554 the loss was 6051.901175000905 (best loss: 5988.92)\n",
      "in attempt 555 the loss was 6055.324300180802 (best loss: 5988.92)\n",
      "in attempt 556 the loss was 6010.555594295445 (best loss: 5988.92)\n",
      "in attempt 557 the loss was 6062.598467097804 (best loss: 5988.92)\n",
      "in attempt 558 the loss was 6000.306106567808 (best loss: 5988.92)\n",
      "in attempt 559 the loss was 6040.602767084437 (best loss: 5988.92)\n",
      "in attempt 560 the loss was 5984.296311934817 (best loss: 5984.30)\n",
      "in attempt 561 the loss was 5946.264345773774 (best loss: 5946.26)\n",
      "in attempt 562 the loss was 5979.083186224501 (best loss: 5946.26)\n",
      "in attempt 563 the loss was 5959.948849183142 (best loss: 5946.26)\n",
      "in attempt 564 the loss was 6004.752574144457 (best loss: 5946.26)\n",
      "in attempt 565 the loss was 5943.57132986672 (best loss: 5943.57)\n",
      "in attempt 566 the loss was 6029.571538469202 (best loss: 5943.57)\n",
      "in attempt 567 the loss was 5983.077516883344 (best loss: 5943.57)\n",
      "in attempt 568 the loss was 5973.927322928911 (best loss: 5943.57)\n",
      "in attempt 569 the loss was 5937.479804316879 (best loss: 5937.48)\n",
      "in attempt 570 the loss was 5989.887409481522 (best loss: 5937.48)\n",
      "in attempt 571 the loss was 5980.824987886507 (best loss: 5937.48)\n",
      "in attempt 572 the loss was 5935.483678731294 (best loss: 5935.48)\n",
      "in attempt 573 the loss was 5996.117052489846 (best loss: 5935.48)\n",
      "in attempt 574 the loss was 5988.688649908763 (best loss: 5935.48)\n",
      "in attempt 575 the loss was 6007.583552030163 (best loss: 5935.48)\n",
      "in attempt 576 the loss was 6002.244097261885 (best loss: 5935.48)\n",
      "in attempt 577 the loss was 5988.977394491392 (best loss: 5935.48)\n",
      "in attempt 578 the loss was 5986.40107197859 (best loss: 5935.48)\n",
      "in attempt 579 the loss was 5966.111658875394 (best loss: 5935.48)\n",
      "in attempt 580 the loss was 6006.654896741515 (best loss: 5935.48)\n",
      "in attempt 581 the loss was 5971.188054241953 (best loss: 5935.48)\n",
      "in attempt 582 the loss was 5899.634179001001 (best loss: 5899.63)\n",
      "in attempt 583 the loss was 5936.693505553369 (best loss: 5899.63)\n",
      "in attempt 584 the loss was 5950.612730104522 (best loss: 5899.63)\n",
      "in attempt 585 the loss was 5976.143847723815 (best loss: 5899.63)\n",
      "in attempt 586 the loss was 5952.226445302983 (best loss: 5899.63)\n",
      "in attempt 587 the loss was 6002.795129182129 (best loss: 5899.63)\n",
      "in attempt 588 the loss was 5972.424709627746 (best loss: 5899.63)\n",
      "in attempt 589 the loss was 6006.473242142448 (best loss: 5899.63)\n",
      "in attempt 590 the loss was 5953.424117177603 (best loss: 5899.63)\n",
      "in attempt 591 the loss was 5896.680194182974 (best loss: 5896.68)\n",
      "in attempt 592 the loss was 5928.770844503148 (best loss: 5896.68)\n",
      "in attempt 593 the loss was 5941.740296962512 (best loss: 5896.68)\n",
      "in attempt 594 the loss was 5885.307809773664 (best loss: 5885.31)\n",
      "in attempt 595 the loss was 5945.269111925943 (best loss: 5885.31)\n",
      "in attempt 596 the loss was 5901.979543654704 (best loss: 5885.31)\n",
      "in attempt 597 the loss was 5904.816490791918 (best loss: 5885.31)\n",
      "in attempt 598 the loss was 5907.417181164839 (best loss: 5885.31)\n",
      "in attempt 599 the loss was 5985.72310940858 (best loss: 5885.31)\n",
      "in attempt 600 the loss was 5970.500334291612 (best loss: 5885.31)\n",
      "in attempt 601 the loss was 5913.382149181095 (best loss: 5885.31)\n",
      "in attempt 602 the loss was 5922.151152088343 (best loss: 5885.31)\n",
      "in attempt 603 the loss was 5875.965310877939 (best loss: 5875.97)\n",
      "in attempt 604 the loss was 5912.583418658445 (best loss: 5875.97)\n",
      "in attempt 605 the loss was 5932.331985883729 (best loss: 5875.97)\n",
      "in attempt 606 the loss was 5944.887090156188 (best loss: 5875.97)\n",
      "in attempt 607 the loss was 5948.823144276352 (best loss: 5875.97)\n",
      "in attempt 608 the loss was 5896.373624342397 (best loss: 5875.97)\n",
      "in attempt 609 the loss was 5893.818238589852 (best loss: 5875.97)\n",
      "in attempt 610 the loss was 5935.067127450424 (best loss: 5875.97)\n",
      "in attempt 611 the loss was 5899.170407807728 (best loss: 5875.97)\n",
      "in attempt 612 the loss was 5905.663412726965 (best loss: 5875.97)\n",
      "in attempt 613 the loss was 5851.211870000799 (best loss: 5851.21)\n",
      "in attempt 614 the loss was 5919.887521876677 (best loss: 5851.21)\n",
      "in attempt 615 the loss was 5867.457474555878 (best loss: 5851.21)\n",
      "in attempt 616 the loss was 5852.716753597047 (best loss: 5851.21)\n",
      "in attempt 617 the loss was 5945.123878122422 (best loss: 5851.21)\n",
      "in attempt 618 the loss was 5869.142665106854 (best loss: 5851.21)\n",
      "in attempt 619 the loss was 5921.205408380165 (best loss: 5851.21)\n",
      "in attempt 620 the loss was 5905.624090706784 (best loss: 5851.21)\n",
      "in attempt 621 the loss was 5964.143668755818 (best loss: 5851.21)\n",
      "in attempt 622 the loss was 5858.766621889403 (best loss: 5851.21)\n",
      "in attempt 623 the loss was 5870.153741557034 (best loss: 5851.21)\n",
      "in attempt 624 the loss was 5861.42146003208 (best loss: 5851.21)\n",
      "in attempt 625 the loss was 5846.2474438172185 (best loss: 5846.25)\n",
      "in attempt 626 the loss was 5917.379869742369 (best loss: 5846.25)\n",
      "in attempt 627 the loss was 5956.06960075357 (best loss: 5846.25)\n",
      "in attempt 628 the loss was 5844.498379017395 (best loss: 5844.50)\n",
      "in attempt 629 the loss was 5861.935728725384 (best loss: 5844.50)\n",
      "in attempt 630 the loss was 5960.396710934974 (best loss: 5844.50)\n",
      "in attempt 631 the loss was 5905.091921634066 (best loss: 5844.50)\n",
      "in attempt 632 the loss was 5883.89870259744 (best loss: 5844.50)\n",
      "in attempt 633 the loss was 5853.5833085609875 (best loss: 5844.50)\n",
      "in attempt 634 the loss was 5866.612302833344 (best loss: 5844.50)\n",
      "in attempt 635 the loss was 5854.5110631572215 (best loss: 5844.50)\n",
      "in attempt 636 the loss was 5965.012440208554 (best loss: 5844.50)\n",
      "in attempt 637 the loss was 5825.449352361489 (best loss: 5825.45)\n",
      "in attempt 638 the loss was 5836.7315248192535 (best loss: 5825.45)\n",
      "in attempt 639 the loss was 5877.446530425777 (best loss: 5825.45)\n",
      "in attempt 640 the loss was 5920.355631162332 (best loss: 5825.45)\n",
      "in attempt 641 the loss was 5830.321895593264 (best loss: 5825.45)\n",
      "in attempt 642 the loss was 5869.717633597789 (best loss: 5825.45)\n",
      "in attempt 643 the loss was 5896.796800031369 (best loss: 5825.45)\n",
      "in attempt 644 the loss was 5841.196920518301 (best loss: 5825.45)\n",
      "in attempt 645 the loss was 5843.745260572579 (best loss: 5825.45)\n",
      "in attempt 646 the loss was 5867.5314015048125 (best loss: 5825.45)\n",
      "in attempt 647 the loss was 5913.80596915367 (best loss: 5825.45)\n",
      "in attempt 648 the loss was 5870.98795147347 (best loss: 5825.45)\n",
      "in attempt 649 the loss was 5879.202279062147 (best loss: 5825.45)\n",
      "in attempt 650 the loss was 5917.955889662872 (best loss: 5825.45)\n",
      "in attempt 651 the loss was 5872.37450435843 (best loss: 5825.45)\n",
      "in attempt 652 the loss was 5852.417851387936 (best loss: 5825.45)\n",
      "in attempt 653 the loss was 5894.028989162751 (best loss: 5825.45)\n",
      "in attempt 654 the loss was 5838.754455025455 (best loss: 5825.45)\n",
      "in attempt 655 the loss was 5869.15949105876 (best loss: 5825.45)\n",
      "in attempt 656 the loss was 5823.9367792198855 (best loss: 5823.94)\n",
      "in attempt 657 the loss was 5822.995818932772 (best loss: 5823.00)\n",
      "in attempt 658 the loss was 5871.663792526264 (best loss: 5823.00)\n",
      "in attempt 659 the loss was 5869.4360385915115 (best loss: 5823.00)\n",
      "in attempt 660 the loss was 5818.321988277468 (best loss: 5818.32)\n",
      "in attempt 661 the loss was 5837.469421499772 (best loss: 5818.32)\n",
      "in attempt 662 the loss was 5884.703398554967 (best loss: 5818.32)\n",
      "in attempt 663 the loss was 5845.282298381282 (best loss: 5818.32)\n",
      "in attempt 664 the loss was 5845.593209314749 (best loss: 5818.32)\n",
      "in attempt 665 the loss was 5910.124507529514 (best loss: 5818.32)\n",
      "in attempt 666 the loss was 5809.961472470143 (best loss: 5809.96)\n",
      "in attempt 667 the loss was 5825.146495609163 (best loss: 5809.96)\n",
      "in attempt 668 the loss was 5854.913219329051 (best loss: 5809.96)\n",
      "in attempt 669 the loss was 5844.64287807073 (best loss: 5809.96)\n",
      "in attempt 670 the loss was 5889.119100365613 (best loss: 5809.96)\n",
      "in attempt 671 the loss was 5880.756772519331 (best loss: 5809.96)\n",
      "in attempt 672 the loss was 5852.878085517803 (best loss: 5809.96)\n",
      "in attempt 673 the loss was 5829.978182404275 (best loss: 5809.96)\n",
      "in attempt 674 the loss was 5872.699106312941 (best loss: 5809.96)\n",
      "in attempt 675 the loss was 5877.91047988613 (best loss: 5809.96)\n",
      "in attempt 676 the loss was 5856.885130894099 (best loss: 5809.96)\n",
      "in attempt 677 the loss was 5885.742937479654 (best loss: 5809.96)\n",
      "in attempt 678 the loss was 5829.728153287273 (best loss: 5809.96)\n",
      "in attempt 679 the loss was 5868.903008607066 (best loss: 5809.96)\n",
      "in attempt 680 the loss was 5878.498994775536 (best loss: 5809.96)\n",
      "in attempt 681 the loss was 5817.141546067503 (best loss: 5809.96)\n",
      "in attempt 682 the loss was 5873.724361175228 (best loss: 5809.96)\n",
      "in attempt 683 the loss was 5891.72342624289 (best loss: 5809.96)\n",
      "in attempt 684 the loss was 5894.684921780523 (best loss: 5809.96)\n",
      "in attempt 685 the loss was 5923.210735893919 (best loss: 5809.96)\n",
      "in attempt 686 the loss was 5881.145669610105 (best loss: 5809.96)\n",
      "in attempt 687 the loss was 5866.312516393564 (best loss: 5809.96)\n",
      "in attempt 688 the loss was 5848.272084813489 (best loss: 5809.96)\n",
      "in attempt 689 the loss was 5883.597320627654 (best loss: 5809.96)\n",
      "in attempt 690 the loss was 5849.795779846016 (best loss: 5809.96)\n",
      "in attempt 691 the loss was 5852.709588942571 (best loss: 5809.96)\n",
      "in attempt 692 the loss was 5769.989757636481 (best loss: 5769.99)\n",
      "in attempt 693 the loss was 5839.065991544078 (best loss: 5769.99)\n",
      "in attempt 694 the loss was 5842.059119897188 (best loss: 5769.99)\n",
      "in attempt 695 the loss was 5859.080990404838 (best loss: 5769.99)\n",
      "in attempt 696 the loss was 5803.341547612833 (best loss: 5769.99)\n",
      "in attempt 697 the loss was 5842.372274199324 (best loss: 5769.99)\n",
      "in attempt 698 the loss was 5834.7193780454345 (best loss: 5769.99)\n",
      "in attempt 699 the loss was 5780.727829837402 (best loss: 5769.99)\n",
      "in attempt 700 the loss was 5810.457769083776 (best loss: 5769.99)\n",
      "in attempt 701 the loss was 5778.161674994107 (best loss: 5769.99)\n",
      "in attempt 702 the loss was 5773.143122024618 (best loss: 5769.99)\n",
      "in attempt 703 the loss was 5790.342436278903 (best loss: 5769.99)\n",
      "in attempt 704 the loss was 5799.293331340168 (best loss: 5769.99)\n",
      "in attempt 705 the loss was 5856.104983723882 (best loss: 5769.99)\n",
      "in attempt 706 the loss was 5805.25750681013 (best loss: 5769.99)\n",
      "in attempt 707 the loss was 5810.632340049471 (best loss: 5769.99)\n",
      "in attempt 708 the loss was 5778.52038126808 (best loss: 5769.99)\n",
      "in attempt 709 the loss was 5779.337017310038 (best loss: 5769.99)\n",
      "in attempt 710 the loss was 5787.4388054511855 (best loss: 5769.99)\n",
      "in attempt 711 the loss was 5872.522033068657 (best loss: 5769.99)\n",
      "in attempt 712 the loss was 5793.684547559389 (best loss: 5769.99)\n",
      "in attempt 713 the loss was 5810.315357974181 (best loss: 5769.99)\n",
      "in attempt 714 the loss was 5889.0589708392745 (best loss: 5769.99)\n",
      "in attempt 715 the loss was 5831.147177391107 (best loss: 5769.99)\n",
      "in attempt 716 the loss was 5832.766059647079 (best loss: 5769.99)\n",
      "in attempt 717 the loss was 5787.14279484206 (best loss: 5769.99)\n",
      "in attempt 718 the loss was 5809.668841740995 (best loss: 5769.99)\n",
      "in attempt 719 the loss was 5814.594801431768 (best loss: 5769.99)\n",
      "in attempt 720 the loss was 5818.7659046801655 (best loss: 5769.99)\n",
      "in attempt 721 the loss was 5769.828708538107 (best loss: 5769.83)\n",
      "in attempt 722 the loss was 5792.898184082636 (best loss: 5769.83)\n",
      "in attempt 723 the loss was 5819.436589688091 (best loss: 5769.83)\n",
      "in attempt 724 the loss was 5805.312419536611 (best loss: 5769.83)\n",
      "in attempt 725 the loss was 5789.506109565146 (best loss: 5769.83)\n",
      "in attempt 726 the loss was 5793.41473987895 (best loss: 5769.83)\n",
      "in attempt 727 the loss was 5872.772889823418 (best loss: 5769.83)\n",
      "in attempt 728 the loss was 5764.710962488827 (best loss: 5764.71)\n",
      "in attempt 729 the loss was 5800.398186271659 (best loss: 5764.71)\n",
      "in attempt 730 the loss was 5783.033527206659 (best loss: 5764.71)\n",
      "in attempt 731 the loss was 5764.43801375773 (best loss: 5764.44)\n",
      "in attempt 732 the loss was 5861.117477691975 (best loss: 5764.44)\n",
      "in attempt 733 the loss was 5846.358821606964 (best loss: 5764.44)\n",
      "in attempt 734 the loss was 5842.023618621463 (best loss: 5764.44)\n",
      "in attempt 735 the loss was 5807.790892165236 (best loss: 5764.44)\n",
      "in attempt 736 the loss was 5804.831319332148 (best loss: 5764.44)\n",
      "in attempt 737 the loss was 5851.133129038033 (best loss: 5764.44)\n",
      "in attempt 738 the loss was 5773.908519513412 (best loss: 5764.44)\n",
      "in attempt 739 the loss was 5785.030117000342 (best loss: 5764.44)\n",
      "in attempt 740 the loss was 5780.387538821059 (best loss: 5764.44)\n",
      "in attempt 741 the loss was 5785.097527106027 (best loss: 5764.44)\n",
      "in attempt 742 the loss was 5803.970659708355 (best loss: 5764.44)\n",
      "in attempt 743 the loss was 5773.903506042881 (best loss: 5764.44)\n",
      "in attempt 744 the loss was 5795.534836999377 (best loss: 5764.44)\n",
      "in attempt 745 the loss was 5814.134557625628 (best loss: 5764.44)\n",
      "in attempt 746 the loss was 5771.523081915107 (best loss: 5764.44)\n",
      "in attempt 747 the loss was 5768.960779008737 (best loss: 5764.44)\n",
      "in attempt 748 the loss was 5791.624542799464 (best loss: 5764.44)\n",
      "in attempt 749 the loss was 5845.163787035537 (best loss: 5764.44)\n",
      "in attempt 750 the loss was 5868.499014991547 (best loss: 5764.44)\n",
      "in attempt 751 the loss was 5717.35050516712 (best loss: 5717.35)\n",
      "in attempt 752 the loss was 5810.553987829529 (best loss: 5717.35)\n",
      "in attempt 753 the loss was 5730.007788017148 (best loss: 5717.35)\n",
      "in attempt 754 the loss was 5797.562732003409 (best loss: 5717.35)\n",
      "in attempt 755 the loss was 5700.841475938612 (best loss: 5700.84)\n",
      "in attempt 756 the loss was 5768.187309742327 (best loss: 5700.84)\n",
      "in attempt 757 the loss was 5715.236347090246 (best loss: 5700.84)\n",
      "in attempt 758 the loss was 5711.74119174736 (best loss: 5700.84)\n",
      "in attempt 759 the loss was 5718.146131834217 (best loss: 5700.84)\n",
      "in attempt 760 the loss was 5689.645882106674 (best loss: 5689.65)\n",
      "in attempt 761 the loss was 5733.413056576014 (best loss: 5689.65)\n",
      "in attempt 762 the loss was 5740.482645310354 (best loss: 5689.65)\n",
      "in attempt 763 the loss was 5725.217961112921 (best loss: 5689.65)\n",
      "in attempt 764 the loss was 5720.177216028978 (best loss: 5689.65)\n",
      "in attempt 765 the loss was 5768.230784485255 (best loss: 5689.65)\n",
      "in attempt 766 the loss was 5792.165833725826 (best loss: 5689.65)\n",
      "in attempt 767 the loss was 5788.985156049742 (best loss: 5689.65)\n",
      "in attempt 768 the loss was 5715.79004560863 (best loss: 5689.65)\n",
      "in attempt 769 the loss was 5799.307910280239 (best loss: 5689.65)\n",
      "in attempt 770 the loss was 5710.756386645622 (best loss: 5689.65)\n",
      "in attempt 771 the loss was 5707.395932822856 (best loss: 5689.65)\n",
      "in attempt 772 the loss was 5733.851993563915 (best loss: 5689.65)\n",
      "in attempt 773 the loss was 5735.405714711129 (best loss: 5689.65)\n",
      "in attempt 774 the loss was 5690.762468511292 (best loss: 5689.65)\n",
      "in attempt 775 the loss was 5703.74217577299 (best loss: 5689.65)\n",
      "in attempt 776 the loss was 5679.235622370352 (best loss: 5679.24)\n",
      "in attempt 777 the loss was 5660.413923035763 (best loss: 5660.41)\n",
      "in attempt 778 the loss was 5722.816885021633 (best loss: 5660.41)\n",
      "in attempt 779 the loss was 5761.653770881386 (best loss: 5660.41)\n",
      "in attempt 780 the loss was 5722.430632560868 (best loss: 5660.41)\n",
      "in attempt 781 the loss was 5709.717767169235 (best loss: 5660.41)\n",
      "in attempt 782 the loss was 5717.126846331948 (best loss: 5660.41)\n",
      "in attempt 783 the loss was 5684.015471493238 (best loss: 5660.41)\n",
      "in attempt 784 the loss was 5642.554043119846 (best loss: 5642.55)\n",
      "in attempt 785 the loss was 5705.691046556807 (best loss: 5642.55)\n",
      "in attempt 786 the loss was 5641.68555935993 (best loss: 5641.69)\n",
      "in attempt 787 the loss was 5681.1189293576135 (best loss: 5641.69)\n",
      "in attempt 788 the loss was 5671.814816692446 (best loss: 5641.69)\n",
      "in attempt 789 the loss was 5732.225566146966 (best loss: 5641.69)\n",
      "in attempt 790 the loss was 5656.745901820258 (best loss: 5641.69)\n",
      "in attempt 791 the loss was 5728.187371549814 (best loss: 5641.69)\n",
      "in attempt 792 the loss was 5738.400180821565 (best loss: 5641.69)\n",
      "in attempt 793 the loss was 5634.827343680612 (best loss: 5634.83)\n",
      "in attempt 794 the loss was 5596.215758395707 (best loss: 5596.22)\n",
      "in attempt 795 the loss was 5629.026985275266 (best loss: 5596.22)\n",
      "in attempt 796 the loss was 5586.300436051963 (best loss: 5586.30)\n",
      "in attempt 797 the loss was 5643.120867664434 (best loss: 5586.30)\n",
      "in attempt 798 the loss was 5596.194637077021 (best loss: 5586.30)\n",
      "in attempt 799 the loss was 5593.536058457623 (best loss: 5586.30)\n",
      "in attempt 800 the loss was 5593.650989062373 (best loss: 5586.30)\n",
      "in attempt 801 the loss was 5550.273203633855 (best loss: 5550.27)\n",
      "in attempt 802 the loss was 5530.961017756643 (best loss: 5530.96)\n",
      "in attempt 803 the loss was 5587.270579772112 (best loss: 5530.96)\n",
      "in attempt 804 the loss was 5550.778718445067 (best loss: 5530.96)\n",
      "in attempt 805 the loss was 5608.9607024219695 (best loss: 5530.96)\n",
      "in attempt 806 the loss was 5666.752399378891 (best loss: 5530.96)\n",
      "in attempt 807 the loss was 5601.771471914872 (best loss: 5530.96)\n",
      "in attempt 808 the loss was 5538.223201340045 (best loss: 5530.96)\n",
      "in attempt 809 the loss was 5561.422713890959 (best loss: 5530.96)\n",
      "in attempt 810 the loss was 5534.001994901296 (best loss: 5530.96)\n",
      "in attempt 811 the loss was 5589.473404865308 (best loss: 5530.96)\n",
      "in attempt 812 the loss was 5549.584606701714 (best loss: 5530.96)\n",
      "in attempt 813 the loss was 5539.432116518123 (best loss: 5530.96)\n",
      "in attempt 814 the loss was 5621.445423890109 (best loss: 5530.96)\n",
      "in attempt 815 the loss was 5491.011856143829 (best loss: 5491.01)\n",
      "in attempt 816 the loss was 5498.4117380727 (best loss: 5491.01)\n",
      "in attempt 817 the loss was 5536.786992554414 (best loss: 5491.01)\n",
      "in attempt 818 the loss was 5471.340147947496 (best loss: 5471.34)\n",
      "in attempt 819 the loss was 5561.073721632602 (best loss: 5471.34)\n",
      "in attempt 820 the loss was 5422.178176644601 (best loss: 5422.18)\n",
      "in attempt 821 the loss was 5515.312120003866 (best loss: 5422.18)\n",
      "in attempt 822 the loss was 5460.343366050049 (best loss: 5422.18)\n",
      "in attempt 823 the loss was 5500.733970761921 (best loss: 5422.18)\n",
      "in attempt 824 the loss was 5390.486829956641 (best loss: 5390.49)\n",
      "in attempt 825 the loss was 5445.647347985835 (best loss: 5390.49)\n",
      "in attempt 826 the loss was 5419.4193750226095 (best loss: 5390.49)\n",
      "in attempt 827 the loss was 5459.097520914954 (best loss: 5390.49)\n",
      "in attempt 828 the loss was 5457.721209776034 (best loss: 5390.49)\n",
      "in attempt 829 the loss was 5392.58223258983 (best loss: 5390.49)\n",
      "in attempt 830 the loss was 5399.085811266469 (best loss: 5390.49)\n",
      "in attempt 831 the loss was 5419.592022041838 (best loss: 5390.49)\n",
      "in attempt 832 the loss was 5507.0365191551045 (best loss: 5390.49)\n",
      "in attempt 833 the loss was 5430.524168456423 (best loss: 5390.49)\n",
      "in attempt 834 the loss was 5426.85148614583 (best loss: 5390.49)\n",
      "in attempt 835 the loss was 5398.739537163858 (best loss: 5390.49)\n",
      "in attempt 836 the loss was 5363.195422705175 (best loss: 5363.20)\n",
      "in attempt 837 the loss was 5389.938144607972 (best loss: 5363.20)\n",
      "in attempt 838 the loss was 5414.81897720694 (best loss: 5363.20)\n",
      "in attempt 839 the loss was 5435.830422109519 (best loss: 5363.20)\n",
      "in attempt 840 the loss was 5423.127431712253 (best loss: 5363.20)\n",
      "in attempt 841 the loss was 5346.357981816607 (best loss: 5346.36)\n",
      "in attempt 842 the loss was 5391.38152932326 (best loss: 5346.36)\n",
      "in attempt 843 the loss was 5386.345872093214 (best loss: 5346.36)\n",
      "in attempt 844 the loss was 5316.482788325112 (best loss: 5316.48)\n",
      "in attempt 845 the loss was 5349.201086613186 (best loss: 5316.48)\n",
      "in attempt 846 the loss was 5391.420756673062 (best loss: 5316.48)\n",
      "in attempt 847 the loss was 5366.530043024724 (best loss: 5316.48)\n",
      "in attempt 848 the loss was 5388.8967619512005 (best loss: 5316.48)\n",
      "in attempt 849 the loss was 5416.754963965642 (best loss: 5316.48)\n",
      "in attempt 850 the loss was 5364.269083720162 (best loss: 5316.48)\n",
      "in attempt 851 the loss was 5424.713105047922 (best loss: 5316.48)\n",
      "in attempt 852 the loss was 5385.888317220122 (best loss: 5316.48)\n",
      "in attempt 853 the loss was 5421.300052150976 (best loss: 5316.48)\n",
      "in attempt 854 the loss was 5375.383655205206 (best loss: 5316.48)\n",
      "in attempt 855 the loss was 5387.917680275378 (best loss: 5316.48)\n",
      "in attempt 856 the loss was 5304.090423266805 (best loss: 5304.09)\n",
      "in attempt 857 the loss was 5375.378016352113 (best loss: 5304.09)\n",
      "in attempt 858 the loss was 5383.8038245524785 (best loss: 5304.09)\n",
      "in attempt 859 the loss was 5332.518220506665 (best loss: 5304.09)\n",
      "in attempt 860 the loss was 5294.76252959026 (best loss: 5294.76)\n",
      "in attempt 861 the loss was 5339.1419956814425 (best loss: 5294.76)\n",
      "in attempt 862 the loss was 5343.701808155487 (best loss: 5294.76)\n",
      "in attempt 863 the loss was 5349.01926651501 (best loss: 5294.76)\n",
      "in attempt 864 the loss was 5326.1452672786445 (best loss: 5294.76)\n",
      "in attempt 865 the loss was 5314.533939191151 (best loss: 5294.76)\n",
      "in attempt 866 the loss was 5297.999311576522 (best loss: 5294.76)\n",
      "in attempt 867 the loss was 5325.1641334894575 (best loss: 5294.76)\n",
      "in attempt 868 the loss was 5376.19479948207 (best loss: 5294.76)\n",
      "in attempt 869 the loss was 5313.406501199395 (best loss: 5294.76)\n",
      "in attempt 870 the loss was 5321.506642734756 (best loss: 5294.76)\n",
      "in attempt 871 the loss was 5319.646523665078 (best loss: 5294.76)\n",
      "in attempt 872 the loss was 5331.621731994539 (best loss: 5294.76)\n",
      "in attempt 873 the loss was 5322.226036476124 (best loss: 5294.76)\n",
      "in attempt 874 the loss was 5352.533328883448 (best loss: 5294.76)\n",
      "in attempt 875 the loss was 5319.397735797131 (best loss: 5294.76)\n",
      "in attempt 876 the loss was 5347.852063822789 (best loss: 5294.76)\n",
      "in attempt 877 the loss was 5306.513195316451 (best loss: 5294.76)\n",
      "in attempt 878 the loss was 5366.955367117441 (best loss: 5294.76)\n",
      "in attempt 879 the loss was 5357.331903257678 (best loss: 5294.76)\n",
      "in attempt 880 the loss was 5342.873232431846 (best loss: 5294.76)\n",
      "in attempt 881 the loss was 5322.210354547111 (best loss: 5294.76)\n",
      "in attempt 882 the loss was 5373.40776643744 (best loss: 5294.76)\n",
      "in attempt 883 the loss was 5285.972019320909 (best loss: 5285.97)\n",
      "in attempt 884 the loss was 5310.010534011111 (best loss: 5285.97)\n",
      "in attempt 885 the loss was 5299.391434013314 (best loss: 5285.97)\n",
      "in attempt 886 the loss was 5295.49954028194 (best loss: 5285.97)\n",
      "in attempt 887 the loss was 5359.132153362369 (best loss: 5285.97)\n",
      "in attempt 888 the loss was 5302.381906506721 (best loss: 5285.97)\n",
      "in attempt 889 the loss was 5324.012982140456 (best loss: 5285.97)\n",
      "in attempt 890 the loss was 5380.9952508337265 (best loss: 5285.97)\n",
      "in attempt 891 the loss was 5354.737889126759 (best loss: 5285.97)\n",
      "in attempt 892 the loss was 5350.742380585016 (best loss: 5285.97)\n",
      "in attempt 893 the loss was 5333.964933489242 (best loss: 5285.97)\n",
      "in attempt 894 the loss was 5309.762203192763 (best loss: 5285.97)\n",
      "in attempt 895 the loss was 5325.356638610419 (best loss: 5285.97)\n",
      "in attempt 896 the loss was 5402.733280578064 (best loss: 5285.97)\n",
      "in attempt 897 the loss was 5318.232109225322 (best loss: 5285.97)\n",
      "in attempt 898 the loss was 5335.781884085311 (best loss: 5285.97)\n",
      "in attempt 899 the loss was 5387.366190245124 (best loss: 5285.97)\n",
      "in attempt 900 the loss was 5295.374054088232 (best loss: 5285.97)\n",
      "in attempt 901 the loss was 5320.202939280958 (best loss: 5285.97)\n",
      "in attempt 902 the loss was 5374.905892174345 (best loss: 5285.97)\n",
      "in attempt 903 the loss was 5363.78504252116 (best loss: 5285.97)\n",
      "in attempt 904 the loss was 5312.941033882958 (best loss: 5285.97)\n",
      "in attempt 905 the loss was 5360.596611991943 (best loss: 5285.97)\n",
      "in attempt 906 the loss was 5325.79873712701 (best loss: 5285.97)\n",
      "in attempt 907 the loss was 5355.868150179876 (best loss: 5285.97)\n",
      "in attempt 908 the loss was 5326.609744087992 (best loss: 5285.97)\n",
      "in attempt 909 the loss was 5295.733067753639 (best loss: 5285.97)\n",
      "in attempt 910 the loss was 5366.637080017197 (best loss: 5285.97)\n",
      "in attempt 911 the loss was 5375.466704915164 (best loss: 5285.97)\n",
      "in attempt 912 the loss was 5368.853166616202 (best loss: 5285.97)\n",
      "in attempt 913 the loss was 5301.344162290164 (best loss: 5285.97)\n",
      "in attempt 914 the loss was 5318.355142854109 (best loss: 5285.97)\n",
      "in attempt 915 the loss was 5317.357049646675 (best loss: 5285.97)\n",
      "in attempt 916 the loss was 5387.687911698063 (best loss: 5285.97)\n",
      "in attempt 917 the loss was 5336.590483398619 (best loss: 5285.97)\n",
      "in attempt 918 the loss was 5287.170622801381 (best loss: 5285.97)\n",
      "in attempt 919 the loss was 5364.261005894505 (best loss: 5285.97)\n",
      "in attempt 920 the loss was 5360.378617570117 (best loss: 5285.97)\n",
      "in attempt 921 the loss was 5348.380994949392 (best loss: 5285.97)\n",
      "in attempt 922 the loss was 5401.0968771407815 (best loss: 5285.97)\n",
      "in attempt 923 the loss was 5318.809320044417 (best loss: 5285.97)\n",
      "in attempt 924 the loss was 5306.9775968887025 (best loss: 5285.97)\n",
      "in attempt 925 the loss was 5304.109793632708 (best loss: 5285.97)\n",
      "in attempt 926 the loss was 5291.422422011625 (best loss: 5285.97)\n",
      "in attempt 927 the loss was 5390.636302028159 (best loss: 5285.97)\n",
      "in attempt 928 the loss was 5421.54387394196 (best loss: 5285.97)\n",
      "in attempt 929 the loss was 5334.509638059544 (best loss: 5285.97)\n",
      "in attempt 930 the loss was 5354.976194510114 (best loss: 5285.97)\n",
      "in attempt 931 the loss was 5354.295978738373 (best loss: 5285.97)\n",
      "in attempt 932 the loss was 5257.229217680344 (best loss: 5257.23)\n",
      "in attempt 933 the loss was 5316.5684504148785 (best loss: 5257.23)\n",
      "in attempt 934 the loss was 5310.126224094001 (best loss: 5257.23)\n",
      "in attempt 935 the loss was 5294.82379827701 (best loss: 5257.23)\n",
      "in attempt 936 the loss was 5313.45428647771 (best loss: 5257.23)\n",
      "in attempt 937 the loss was 5272.750375232421 (best loss: 5257.23)\n",
      "in attempt 938 the loss was 5231.9970447349515 (best loss: 5232.00)\n",
      "in attempt 939 the loss was 5281.23171408478 (best loss: 5232.00)\n",
      "in attempt 940 the loss was 5235.163087616495 (best loss: 5232.00)\n",
      "in attempt 941 the loss was 5252.159955662427 (best loss: 5232.00)\n",
      "in attempt 942 the loss was 5300.400571421424 (best loss: 5232.00)\n",
      "in attempt 943 the loss was 5309.3101576044355 (best loss: 5232.00)\n",
      "in attempt 944 the loss was 5274.541233064651 (best loss: 5232.00)\n",
      "in attempt 945 the loss was 5267.52176589102 (best loss: 5232.00)\n",
      "in attempt 946 the loss was 5256.965544023733 (best loss: 5232.00)\n",
      "in attempt 947 the loss was 5332.028701078607 (best loss: 5232.00)\n",
      "in attempt 948 the loss was 5281.050079751731 (best loss: 5232.00)\n",
      "in attempt 949 the loss was 5221.9429592938795 (best loss: 5221.94)\n",
      "in attempt 950 the loss was 5216.487881332799 (best loss: 5216.49)\n",
      "in attempt 951 the loss was 5244.607217429482 (best loss: 5216.49)\n",
      "in attempt 952 the loss was 5231.551216277612 (best loss: 5216.49)\n",
      "in attempt 953 the loss was 5238.130532413496 (best loss: 5216.49)\n",
      "in attempt 954 the loss was 5217.987491665244 (best loss: 5216.49)\n",
      "in attempt 955 the loss was 5268.988540065115 (best loss: 5216.49)\n",
      "in attempt 956 the loss was 5282.219810452699 (best loss: 5216.49)\n",
      "in attempt 957 the loss was 5257.289000004075 (best loss: 5216.49)\n",
      "in attempt 958 the loss was 5261.113577327089 (best loss: 5216.49)\n",
      "in attempt 959 the loss was 5330.781488472563 (best loss: 5216.49)\n",
      "in attempt 960 the loss was 5289.371456090194 (best loss: 5216.49)\n",
      "in attempt 961 the loss was 5340.648083176473 (best loss: 5216.49)\n",
      "in attempt 962 the loss was 5262.49732499331 (best loss: 5216.49)\n",
      "in attempt 963 the loss was 5285.3041659549235 (best loss: 5216.49)\n",
      "in attempt 964 the loss was 5251.350138031446 (best loss: 5216.49)\n",
      "in attempt 965 the loss was 5262.540956418692 (best loss: 5216.49)\n",
      "in attempt 966 the loss was 5288.975471766918 (best loss: 5216.49)\n",
      "in attempt 967 the loss was 5275.4324535601645 (best loss: 5216.49)\n",
      "in attempt 968 the loss was 5218.202397587813 (best loss: 5216.49)\n",
      "in attempt 969 the loss was 5219.3954218177005 (best loss: 5216.49)\n",
      "in attempt 970 the loss was 5321.255530430481 (best loss: 5216.49)\n",
      "in attempt 971 the loss was 5262.095198591442 (best loss: 5216.49)\n",
      "in attempt 972 the loss was 5269.723395663105 (best loss: 5216.49)\n",
      "in attempt 973 the loss was 5284.991517354087 (best loss: 5216.49)\n",
      "in attempt 974 the loss was 5339.769776718136 (best loss: 5216.49)\n",
      "in attempt 975 the loss was 5293.044429605723 (best loss: 5216.49)\n",
      "in attempt 976 the loss was 5313.719783146386 (best loss: 5216.49)\n",
      "in attempt 977 the loss was 5287.737439215885 (best loss: 5216.49)\n",
      "in attempt 978 the loss was 5223.558532503259 (best loss: 5216.49)\n",
      "in attempt 979 the loss was 5264.65714014744 (best loss: 5216.49)\n",
      "in attempt 980 the loss was 5250.008794156493 (best loss: 5216.49)\n",
      "in attempt 981 the loss was 5224.788479819752 (best loss: 5216.49)\n",
      "in attempt 982 the loss was 5245.138974948175 (best loss: 5216.49)\n",
      "in attempt 983 the loss was 5261.119269118328 (best loss: 5216.49)\n",
      "in attempt 984 the loss was 5258.537239951016 (best loss: 5216.49)\n",
      "in attempt 985 the loss was 5292.5495157759615 (best loss: 5216.49)\n",
      "in attempt 986 the loss was 5266.770314797853 (best loss: 5216.49)\n",
      "in attempt 987 the loss was 5303.863281667494 (best loss: 5216.49)\n",
      "in attempt 988 the loss was 5263.246943950786 (best loss: 5216.49)\n",
      "in attempt 989 the loss was 5258.078136296703 (best loss: 5216.49)\n",
      "in attempt 990 the loss was 5260.975722972507 (best loss: 5216.49)\n",
      "in attempt 991 the loss was 5243.112582608682 (best loss: 5216.49)\n",
      "in attempt 992 the loss was 5224.089115881449 (best loss: 5216.49)\n",
      "in attempt 993 the loss was 5314.644462536665 (best loss: 5216.49)\n",
      "in attempt 994 the loss was 5224.362813629026 (best loss: 5216.49)\n",
      "in attempt 995 the loss was 5276.683871370895 (best loss: 5216.49)\n",
      "in attempt 996 the loss was 5272.334232599221 (best loss: 5216.49)\n",
      "in attempt 997 the loss was 5245.669236674603 (best loss: 5216.49)\n",
      "in attempt 998 the loss was 5312.373995373808 (best loss: 5216.49)\n",
      "in attempt 999 the loss was 5207.770365051898 (best loss: 5207.77)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Random Local Search\n",
    "\n",
    "np.random.seed(0)\n",
    "# seed 0 best loss = 4440.61\n",
    "# seed 3 best loss = 5204.05\n",
    "# seed 20 best loss = 4453.73\n",
    "# seed 42 best loss = 4948.15\n",
    "W = np.random.randn(10, 3073) * 0.001 # generate random starting W\n",
    "bestloss = float(\"inf\")\n",
    "for num in range(1000):\n",
    "  step_size = 0.0001\n",
    "  Wtry = W + np.random.randn(10, 3073) * step_size\n",
    "  _, loss = L(X_dev, y_dev, Wtry)\n",
    "  if loss < bestloss:\n",
    "    W = Wtry\n",
    "    bestloss = loss\n",
    "  print(f'in attempt {num} the loss was {loss} (best loss: {bestloss:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "876a35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random local search accuracy on test set: 8.80%\n"
     ]
    }
   ],
   "source": [
    "# To see how well it works on the test set\n",
    "\n",
    "score = scores(X_test, bestW)\n",
    "Y_pred = np.argmax(score, axis=0)\n",
    "accuracy = np.mean(Y_pred == y_test)\n",
    "print(f'Random local search accuracy on test set: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b445f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Following the Gradient\n",
    "\n",
    "def eval_numerical_gradient(f, x):\n",
    "  \"\"\"\n",
    "  a naive implementation of numerical gradient of f at x\n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\"\n",
    "\n",
    "  fx = f(x) # evaluate function value at original point\n",
    "  grad = np.zeros(x.shape)\n",
    "  h = 0.00001\n",
    "\n",
    "  # iterate over all indexes in x\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "\n",
    "    # evaluate function at x+h\n",
    "    ix = it.multi_index\n",
    "    old_value = x[ix]\n",
    "    x[ix] = old_value + h # increment by h\n",
    "    fxh = f(x) # evalute f(x + h)\n",
    "    x[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "    # compute the partial derivative\n",
    "    grad[ix] = (fxh - fx) / h # the slope\n",
    "    it.iternext() # step to next dimension\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d5ff7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loss: 6076.083363580405\n",
      "for step size 1e-10 new loss: 5826.747403526153\n",
      "for step size 1e-09 new loss: 4309.112501280722\n",
      "for step size 1e-08 new loss: 10494.14342263991\n",
      "for step size 1e-07 new loss: 97494.52349454709\n",
      "for step size 1e-06 new loss: 970226.1082724626\n",
      "for step size 1e-05 new loss: 9697731.118655175\n",
      "for step size 0.0001 new loss: 96972781.22248226\n",
      "for step size 0.001 new loss: 969723282.2607533\n",
      "for step size 0.01 new loss: 9697228292.643463\n",
      "for step size 0.1 new loss: 96972278396.47057\n"
     ]
    }
   ],
   "source": [
    "# compute the numerical gradient for the CIFAR-10 loss function at some random point in the weight space\n",
    "\n",
    "def CIFAR10_loss_fun(W):\n",
    "  # L returns (scores, loss); numerical gradient function expects a scalar loss\n",
    "  _, loss = L(X_dev, y_dev, W)\n",
    "  return loss\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "W = np.random.rand(10, 3073) * 0.001 # random weight vector\n",
    "df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient\n",
    "\n",
    "loss_original = CIFAR10_loss_fun(W) # the original loss\n",
    "print(f'original loss: {loss_original}')\n",
    "\n",
    "# lets see the effect of multiple step sizes\n",
    "for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n",
    "  step_size = 10 ** step_size_log\n",
    "  W_new = W - step_size * df # new position in the weight space\n",
    "  loss_new = CIFAR10_loss_fun(W_new)\n",
    "  print(f'for step size {step_size} new loss: {loss_new}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7540aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic Gradient\n",
    "\n",
    "def svm_loss_and_gradient(W, x_i, y_i, delta=1.0):\n",
    "\n",
    "    score = scores(x_i, W)               # shape (C,)\n",
    "    correct_class_score = score[y_i]\n",
    "\n",
    "    # Compute margins for all classes\n",
    "    margins = np.maximum(0, score - correct_class_score + delta)\n",
    "    margins[y_i] = 0  # The correct class doesnt contribute to the loss\n",
    "\n",
    "    loss_i = np.sum(margins)\n",
    "\n",
    "    # Compute gradient\n",
    "    dW_i = np.zeros_like(W)\n",
    "\n",
    "    # Indicator: which classes contributed to the loss\n",
    "    positive_margins = margins > 0\n",
    "\n",
    "    # For incorrect classes: add x_i to rows that violated margin\n",
    "    dW_i[positive_margins] += np.outer(np.ones(np.sum(positive_margins)), x_i)\n",
    "\n",
    "    # For correct class: subtract (# of violations) * x_i\n",
    "    num_violations = np.sum(positive_margins)\n",
    "    dW_i[y_i] -= num_violations * x_i\n",
    "\n",
    "    return loss_i, dW_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f6f130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.332964274642322\n",
      "Gradient:\n",
      " [[ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-0.66734865 -2.98815815  0.41031653 -0.6261354   1.70819148]\n",
      " [ 0.33367433  1.49407907 -0.20515826  0.3130677  -0.85409574]]\n"
     ]
    }
   ],
   "source": [
    "# Example: \n",
    "np.random.seed(0)\n",
    "W = np.random.randn(3, 5)\n",
    "x_i = np.random.randn(5)\n",
    "y_i = 1\n",
    "\n",
    "loss_i, grad_i = svm_loss_and_gradient(W, x_i, y_i)\n",
    "\n",
    "print(\"Loss:\", loss_i)\n",
    "print(\"Gradient:\\n\", grad_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "115bcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Following the gradient\n",
    "\n",
    "# Taking a loss function\n",
    "def loss_function(X, y, W):\n",
    "    _, loss = L(X, y, W)\n",
    "    return loss\n",
    "\n",
    "def gradient_descent(X, y, W, loss_func, step_size=1e-7, num_iters=100):\n",
    "    \"\"\"\n",
    "    Perform vanilla gradient descent to optimize weights W.\n",
    "    \"\"\"\n",
    "    for _ in range(num_iters):\n",
    "        weights_grad = eval_numerical_gradient(X, y, W, loss_func)\n",
    "        W += - step_size * weights_grad  # update weights\n",
    "    return W\n",
    "\n",
    "def sgd(X, y, W, loss_func, step_size=1e-7, num_iters=100, batch_size=200):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent to optimize weights W.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        batch_indices = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        weights_grad = eval_numerical_gradient(X_batch, y_batch, W, loss_func)\n",
    "        W += - step_size * weights_grad  # update weights\n",
    "    return W\n",
    "\n",
    "def sgd_simple(X_batch, y_batch, w, learning_rate, loss_func):\n",
    "    while True:\n",
    "        dx = eval_numerical_gradient(X_batch, y_batch, w, loss_func)\n",
    "        w -= learning_rate * dx\n",
    "        if np.linalg.norm(dx) < 1e-5:  # convergence criterion\n",
    "            break\n",
    "    return w\n",
    "\n",
    "def sgd_momentum(X, y, W, loss_func, step_size=1e-7, num_iters=100, batch_size=200, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent with momentum to optimize weights W.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    v = np.zeros_like(W)  # initialize velocity\n",
    "    for _ in range(num_iters):\n",
    "        batch_indices = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        weights_grad = eval_numerical_gradient(X_batch, y_batch, W, loss_func)\n",
    "        v = momentum * v - step_size * weights_grad  # update velocity\n",
    "        W += v  # update weights\n",
    "    return W\n",
    "\n",
    "def rmsprop(X, y, W, loss_func, step_size=1e-7, num_iters=100, batch_size=200, decay_rate=0.99, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Perform RMSProp to optimize weights W.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    cache = np.zeros_like(W)  # initialize cache\n",
    "    for _ in range(num_iters):\n",
    "        batch_indices = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        weights_grad = eval_numerical_gradient(X_batch, y_batch, W, loss_func)\n",
    "        cache = decay_rate * cache + (1 - decay_rate) * (weights_grad ** 2)  # update cache\n",
    "        W += - step_size * weights_grad / (np.sqrt(cache) + epsilon)  # update weights\n",
    "    return W\n",
    "\n",
    "def adam(X, y, W, loss_func, step_size=1e-7, num_iters=100, batch_size=200, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Perform Adam optimization to optimize weights W.\n",
    "    \"\"\"\n",
    "    num_train = X.shape[0]\n",
    "    m = np.zeros_like(W)  # initialize first moment\n",
    "    v = np.zeros_like(W)  # initialize second moment\n",
    "    for t in range(1, num_iters + 1):\n",
    "        batch_indices = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        weights_grad = eval_numerical_gradient(X_batch, y_batch, W, loss_func)\n",
    "        m = beta1 * m + (1 - beta1) * weights_grad  # update first moment\n",
    "        v = beta2 * v + (1 - beta2) * (weights_grad ** 2)  # update second moment\n",
    "        m_hat = m / (1 - beta1 ** t)  # bias-corrected first moment\n",
    "        v_hat = v / (1 - beta2 ** t)  # bias-corrected second moment\n",
    "        W += - step_size * m_hat / (np.sqrt(v_hat) + epsilon)  # update weights\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea7c34c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval_numerical_gradient() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m weights_over_time = []\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m101\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     W = \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m weight_intervals:\n\u001b[32m     10\u001b[39m         weights_over_time.append(W.copy())  \u001b[38;5;66;03m# store a copy of the weights\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36msgd\u001b[39m\u001b[34m(X, y, W, loss_func, step_size, num_iters, batch_size)\u001b[39m\n\u001b[32m     24\u001b[39m     X_batch = X[batch_indices]\n\u001b[32m     25\u001b[39m     y_batch = y[batch_indices]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     weights_grad = \u001b[43meval_numerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     W += - step_size * weights_grad  \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m W\n",
      "\u001b[31mTypeError\u001b[39m: eval_numerical_gradient() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# displaying the generation of weights\n",
    "\n",
    "weight_intervals = [0, 1, 2, 3, 5, 10, 20, 50, 100] # iterations at which weights are stored\n",
    "\n",
    "weights_over_time = []\n",
    "\n",
    "for iter in range(101):\n",
    "    W = sgd(X_train, y_train, W, loss_function, step_size=1e-7, num_iters=1, batch_size=200)\n",
    "    if iter in weight_intervals:\n",
    "        weights_over_time.append(W.copy())  # store a copy of the weights\n",
    "# Now weights_over_time contains the weights at specified iterations    \n",
    "\n",
    "# Visualizing the weight evolution\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, W in enumerate(weights_over_time):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(W[:, :-1].reshape(10, 32, 32, 3).mean(axis=0).astype('uint8'))  # visualize weights as images\n",
    "    plt.title(f'Weights at iteration {weight_intervals[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
