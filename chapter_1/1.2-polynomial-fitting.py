# Setup
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(999)


# 1.2.1) Synthetic data

# We use a synthetic data set generated by sampling from a sinusoidal function.

N=10 # number of data points
x_n = np.linspace(0, 1, N) # input values
print(x_n)
t_n = np.sin(2 * np.pi * x_n) # target values computed by function sin(2πx)
t_n += np.random.normal(0, 0.1, N) # adding gaussian noise
print(t_n)
# plotting the data
plt.figure(figsize=(10, 6))
plt.scatter(x_n, t_n, label='Data', color='red')
plt.plot(np.linspace(0,1,200), np.sin(2 * np.pi * np.linspace(0,1,200)), label='True function', color='green')
plt.xlabel('$x_n$')
plt.ylabel('$t_n$')
plt.title('Synthetic Data (1.2.1)')
plt.legend()
print('1.2.1 plotted')
plt.show()


# 1.2.2) Linear model

# Fitting polynomials of different degrees to the synthetic data. 
# Using inbuilt libraries first and then writing custom functions.

M_range = [0, 1, 3, 9] # different polynomial orders to fit

# Using inbuilt libraries to fit and evaluate polynomials of different degrees

plt.figure(figsize=(10, 6))
for i, M in enumerate(M_range):
    print(f'Fitting polynomial of degree {M}')
    weights = np.polyfit(x_n, t_n, M) # fit polynomial of degree M
    x_grid = np.linspace(0, 1, 200)
    y_grid = np.polyval(weights, x_grid) # evaluate fitted polynomial
    print(f'Polynomial weights for degree {M}: {weights}')
    # plotting the data 
    plt.subplot(2, 2, i + 1)
    plt.scatter(x_n, t_n, color='blue', label='Data')
    plt.plot(x_grid, y_grid, color='red', label=f'Poly degree {M}')
    plt.plot(x_grid, np.sin(2 * np.pi * x_grid), color='green', label='True function')
    plt.legend()
plt.suptitle('Polynomial Fits (1.2.2) using inbuilt libraries')
print('1.2.2 plotted using inbuilt libraries')
plt.show()

# Using custom function to evaluate polynomials

def y(x,w):
    """Evaluate polynomial at x with coefficients w."""
    return sum(w[j] * x**j for j in range(len(w)))

plt.figure(figsize=(10, 6))
for i, M in enumerate(M_range):
    print(f'Fitting polynomial of degree {M} using custom function')
    # weights = np.polyfit(x_n, t_n, M) # fit polynomial using inbuilt function
    # weights = weights[::-1]  # reverse to ascending order
    weights = np.zeros(M + 1) # we take dummy weights to begin with
    x_grid = np.linspace(0, 1, 200)
    y_grid = y(x_grid, weights) # evaluate polynomial using custom function
    print(f'Polynomial weights for degree {M}: {weights}')
    # plotting the data 
    plt.subplot(2, 2, i + 1)
    plt.scatter(x_n, t_n, color='blue', label='Data')
    plt.plot(x_grid, y_grid, color='red', label=f'Poly degree {M}')
    plt.plot(x_grid, np.sin(2 * np.pi * x_grid), color='green', label='True function')
    plt.legend()
plt.suptitle('Polynomial Fits (1.2.2) using custom evaluation function with dummy weights')
print('1.2.2 plotted using custom function with dummy weights')
plt.show()


# 1.2.3) Error function

# Previously used inbuilt function np.polyfit to fit polynomials.
# Now defining an error function to quantify the fit quality, so we can find the best weights ourselves.

def E(w):
    """Compute sum of squares of displacements of each data point from the function y."""
    return 0.5 * np.sum((y(x_n,w) - t_n) ** 2)

# Now we have to choose the value of w that minimizes E(w).

# Because the error function is a quadratic function of the coefficients w, its derivatives with respect to the coefficients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w*. The resulting polynomial is given by the function y(x, w*).

# finding w*
from scipy.optimize import minimize
plt.figure(figsize=(10, 6)) 
for i, M in enumerate(M_range):
    print(f'Finding optimal weights for polynomial of degree {M}')
    w_0 = np.zeros(M + 1)  # initial guess for weights, matrix of zeros
    result = minimize(E, w_0)  # minimize error function
    w_star = result.x
    print(f'Optimal weights for degree {M}: {w_star}')
    # plotting the fitted polynomial
    x_grid = np.linspace(0, 1, 200)
    y_grid = y(x_grid, w_star)
    plt.subplot(2, 2, i + 1)
    plt.scatter(x_n, t_n, color='blue', label='Data')
    plt.plot(x_grid, y_grid, color='red', label=f'Poly degree {M}')
    plt.plot(x_grid, np.sin(2 * np.pi * x_grid), color='green', label='True function')
    plt.legend()
plt.suptitle('Polynomial Fits (1.2.3) using custom error minimization')
print('1.2.3 plotted with custom error minimization')
plt.show()  


# 1.2.4) Model complexity 

# Choosing the order M of the polynomial: We can see that as we increase the polynomial degree, the fit improves on the training data, but may lead to overfitting.
# We can obtain some quantitative insight into the dependence of the generalization performance on M by considering a separate set of data known as a test set, comprising 100 data points generated using the same procedure as used to generate the training set points. For each value of M, we can evaluate the residual value of E(w*) given by (1.2) for the training data, and we can also evaluate E(w*) for the test data set.

# Generating test data
N_test = 200
x_n_test = np.linspace(0, 1, N_test)
t_n_test = np.sin(2 * np.pi * x_n_test)
t_n_test += np.random.normal(0, 0.1, N_test)
E_train = []
E_test = []

# Instead of the error function E, we use RMS error which allows us to compare different sizes of data sets on an equal footing.
def E_rms(w, x, t):
    """Compute root mean square error."""
    return np.sqrt(np.mean((y(x,w) - t) ** 2))

M_test_range = range(0, 10)  # polynomial degrees from 0 to 9

# calculating RMS errors for training and test data
for M in M_test_range:
    print(f'Calculating RMS error for polynomial of degree {M}')
    w_0 = np.zeros(M + 1)  # initial guess for weights
    result = minimize(E, w_0)  # minimize error function
    w_star = result.x
    E_train.append(E_rms(w_star, x_n, t_n))
    E_test.append(E_rms(w_star, x_n_test, t_n_test))
# plotting RMS errors
plt.figure(figsize=(8, 5))
plt.plot(M_test_range, E_train, marker='o', color='red', label='Training')
plt.plot(M_test_range, E_test, marker='o', color='blue', label='Test')
plt.xlabel('Polynomial Degree M')
plt.ylabel('RMS Error')
plt.legend()
plt.title('Training and Test RMS Errors vs Polynomial Degree')
print('1.2.4 plotted training and test RMS errors vs polynomial degree')
plt.show()

# showing how overfitting decreases with larger datasets at M=9
N_values = [100, 15]
plt.figure(figsize=(10, 5))
for j, N in enumerate(N_values):
    print(f'Generating data and fitting polynomial of degree 9 for N={N}')
    x_n = np.linspace(0, 1, N)
    t_n = np.sin(2 * np.pi * x_n) + np.random.normal(0, 0.1, N)
    w_0 = np.zeros(10)  # initial guess for weights for M=9
    result = minimize(E, w_0, method='BFGS')  # minimize error function
    w_star = result.x
    x_grid = np.linspace(0, 1, 200)
    y_grid = y(x_grid, w_star)
    plt.subplot(1, 2, j + 1)
    plt.scatter(x_n, t_n, color='blue', label='Data')
    plt.plot(x_grid, y_grid, color='red', label='Fitted polynomial')
    plt.plot(x_grid, np.sin(2 * np.pi * x_grid), color='green', label='True function')
    plt.legend()
    plt.title(f'Polynomial Degree 9 with N={N}')
plt.suptitle('Effect of Dataset Size on Overfitting (1.2.4)')
print('1.2.4 plotted effect of dataset size on overfitting')
plt.show()


# 1.2.5) Regularization

# printing the weights at each order without regularization
print("Weights without regularization:")
for M in M_range:
    w_0 = np.zeros(M + 1)  # initial guess for weights
    result = minimize(E, w_0)  # minimize error function
    w_star = result.x
    print(f'Degree {M}: {w_star}')

# Regularization adds a penalty term to the error function to discourage large weights, aka shrinkage methods/weight decay.

def E_reg(w, lam):
    """Compute regularized error function."""
    return E(w) + (lam / 2) * np.sum(w ** 2)

# Plotting M=9 polynomial with different lambda values for regularization
lam_values = [-18, -10, -5, 0]  # log lambda values
plt.figure(figsize=(10, 6))
print(f"x_n: {x_n}")
for i, lam in enumerate(lam_values):
    print(f'Fitting polynomial (M=9) using regularized error with λ={lam} and lambda={lam}')
    w_0 = np.zeros(10)  # initial guess for weights for M=9
    result = minimize(E_reg, w_0, args=(lam,))  # minimize regularized error function
    w_star = result.x
    print(f'Optimal weights for λ={lam}: {w_star}')
    x_grid = np.linspace(0, 1, 200)
    y_grid = y(x_grid, w_star)
    plt.subplot(2, 2, i + 1)
    plt.scatter(x_n, t_n, color='blue', label='Data')
    plt.plot(x_grid, y_grid, color='red', label='Fitted polynomial')
    plt.plot(x_grid, np.sin(2 * np.pi * x_grid), color='green', label='True function')
    plt.title(f' λ={lam}')
    plt.legend()
plt.suptitle('M=9 Polynomials with Regularized Error Function (1.2.5)')
print('1.2.5 plotted regularized polynomial fits')
plt.show()

# Graph of the root-meansquare error versus ln λ for the M = 9 polynomial for training and test sets.

E_train_reg = []
E_test_reg = []
ln_lam_range = np.linspace(-30, 0, 50)  # range of ln lambda values
for ln_lam in ln_lam_range:
    lam = np.exp(ln_lam)
    print(f'Calculating RMS error for ln λ = {ln_lam}')
    w_0 = np.zeros(10)  # initial guess for weights for M=9
    result = minimize(E_reg, w_0, args=(lam,))  # minimize regularized error function
    w_star = result.x
    E_train_reg.append(E_rms(w_star, x_n, t_n))
    E_test_reg.append(E_rms(w_star, x_n_test, t_n_test))
# plotting RMS errors
plt.figure(figsize=(8, 5))
plt.plot(ln_lam_range, E_train_reg, color='red', label='Training')
plt.plot(ln_lam_range, E_test_reg, color='blue', label='Test')
plt.xlabel('ln λ')
plt.ylabel('RMS Error')
plt.legend()
plt.title('Training and Test RMS Errors vs ln λ for M=9 Polynomial')
print('1.2.5 plotted training and test RMS errors vs ln λ for M=9 polynomial')
plt.show()

# using inbuilt libraries for the above instead of the custom functions

import sklearn
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
E_train_sklearn = []
E_test_sklearn = []
for ln_lam in ln_lam_range:
    lam = np.exp(ln_lam)
    print(f'Calculating RMS error using sklearn for ln λ = {ln_lam}')
    poly = PolynomialFeatures(degree=9)
    X_n = poly.fit_transform(x_n.reshape(-1, 1))
    X_n_test = poly.transform(x_n_test.reshape(-1, 1))
    model = Ridge(alpha=lam, fit_intercept=False)
    model.fit(X_n, t_n)
    t_n_pred = model.predict(X_n)
    t_n_test_pred = model.predict(X_n_test)
    E_train_sklearn.append(np.sqrt(mean_squared_error(t_n, t_n_pred)))
    E_test_sklearn.append(np.sqrt(mean_squared_error(t_n_test, t_n_test_pred)))
# plotting RMS errors
plt.figure(figsize=(8, 5))  
plt.plot(ln_lam_range, E_train_sklearn, color='red', label='Training (sklearn)')
plt.plot(ln_lam_range, E_test_sklearn, color='blue', label='Test (sklearn)')
plt.xlabel('ln λ')
plt.ylabel('RMS Error')
plt.legend()
plt.title('Training and Test RMS Errors vs ln λ for M=9 Polynomial (sklearn)')
print('1.2.5 plotted training and test RMS errors vs ln λ for M=9 polynomial using sklearn')
plt.show()


# 1.2.6) Model selection

# λ is a hyperparameter and cannot be determined through minimization of the regularized error function jointly with w on the training data, as this would always lead to λ = 0 and an overfitting model. Similarly, M is also a hyperparameter and minimizing the training error using M will lead to large values of M and overfitting.
# We also cannot use the test data to choose hyperparameters, as this would lead to overfitting on the test data. The test data should only be used once at the very end to provide an unbiased estimate of the generalization error.
# Instead, we can use a validation data set (aka hold-out set or development set), separate from both the training and test data.

# The technique of S-fold cross-validation, illustrated here for the case of S = 4, involves taking the available data and partitioning it into S groups of equal size. Then S −1 of the groups are used to train a set of models that are then evaluated on the remaining group. This procedure is then repeated for all S possible choices for the held-out group, indicated here by the red blocks, and the performance scores from the S runs are then averaged to give an overall performance estimate.

# We will generate a validation data set and use it to choose M and λ based on the RMS error on the validation data.
N_val = 100
x_n_val = np.linspace(0, 1, N_val)
t_n_val = np.sin(2 * np.pi * x_n_val)
t_n_val += np.random.normal(0, 0.1, N_val)
E_val = []
# calculating RMS errors for validation data
for M in M_test_range:
    for ln_lam in ln_lam_range:
        lam = np.exp(ln_lam)
        w_0 = np.zeros(M + 1)  # initial guess for weights
        result = minimize(E_reg, w_0, args=(lam,))  # minimize regularized error function
        w_star = result.x
        E_val.append(E_rms(w_star, x_n_val, t_n_val))
        print(f'RMS error for degree {M} with ln λ = {ln_lam} on validation set: {E_val[-1]}')
# plotting RMS errors
E_val = np.array(E_val).reshape(len(M_test_range), len(ln_lam_range))
plt.figure(figsize=(10, 6))
for i, M in enumerate(M_test_range):
    plt.plot(ln_lam_range, E_val[i, :], label=f'M={M}')
plt.xlabel('ln λ')
plt.ylabel('RMS Error on Validation Set')
plt.legend()
plt.title('Validation RMS Errors vs ln λ for Different Polynomial Degrees')
print('1.2.6 plotted validation RMS errors vs ln λ for different polynomial degrees')
plt.show()